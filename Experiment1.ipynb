{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4e8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4a355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "from model.baseline_model import *\n",
    "from trainer import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import robustbench\n",
    "#from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model, clean_accuracy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d31627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/CIFAR-10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc48776d812469a9b7acb1e8c65dead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/CIFAR-10/cifar-10-python.tar.gz to ./datasets/CIFAR-10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, train_loader, val_loader = CIFAR10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e2f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "device = \"cuda\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0dee4e",
   "metadata": {},
   "source": [
    "# Train shallow CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adbeaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "net_ori = BaseNet().to(device)\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_ori.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = './cifar_base.pth'\n",
    "device = \"cuda\"\n",
    "writer = SummaryWriter(\"NormalBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "865ff15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100]\t\n",
      "Epoch [1/100]\t Training Loss: 2.3029514401770004\t lr: 0.001\n",
      "Epoch [1/100]\t Validation Loss: 2.3015229158763644\t lr: 0.001\n",
      "Epoch [1/100]\t\n",
      "Epoch [2/100]\t Training Loss: 2.3000576453440633\t lr: 0.001\n",
      "Epoch [2/100]\t Validation Loss: 2.297634154935426\t lr: 0.001\n",
      "Epoch [2/100]\t\n",
      "Epoch [3/100]\t Training Loss: 2.2934116225718233\t lr: 0.001\n",
      "Epoch [3/100]\t Validation Loss: 2.2857805233967454\t lr: 0.001\n",
      "Epoch [3/100]\t\n",
      "Epoch [4/100]\t Training Loss: 2.270483707223097\t lr: 0.001\n",
      "Epoch [4/100]\t Validation Loss: 2.244015521641019\t lr: 0.001\n",
      "Epoch [4/100]\t\n",
      "Epoch [5/100]\t Training Loss: 2.1838065199839796\t lr: 0.001\n",
      "Epoch [5/100]\t Validation Loss: 2.0896584202971638\t lr: 0.001\n",
      "Epoch [5/100]\t\n",
      "Epoch [6/100]\t Training Loss: 2.0403649858806445\t lr: 0.001\n",
      "Epoch [6/100]\t Validation Loss: 2.0033185255678396\t lr: 0.001\n",
      "Epoch [6/100]\t\n",
      "Epoch [7/100]\t Training Loss: 1.9974482367410684\t lr: 0.001\n",
      "Epoch [7/100]\t Validation Loss: 1.9824071051199226\t lr: 0.001\n",
      "Epoch [7/100]\t\n",
      "Epoch [8/100]\t Training Loss: 1.9708023665811094\t lr: 0.001\n",
      "Epoch [8/100]\t Validation Loss: 1.9412562334084813\t lr: 0.001\n",
      "Epoch [8/100]\t\n",
      "Epoch [9/100]\t Training Loss: 1.941410747330512\t lr: 0.001\n",
      "Epoch [9/100]\t Validation Loss: 1.916893971117237\t lr: 0.001\n",
      "Epoch [9/100]\t\n",
      "Epoch [10/100]\t Training Loss: 1.9082897023471725\t lr: 0.001\n",
      "Epoch [10/100]\t Validation Loss: 1.8848276243934148\t lr: 0.001\n",
      "Epoch [10/100]\t\n",
      "Epoch [11/100]\t Training Loss: 1.8674087853687804\t lr: 0.001\n",
      "Epoch [11/100]\t Validation Loss: 1.8378563138503063\t lr: 0.001\n",
      "Epoch [11/100]\t\n",
      "Epoch [12/100]\t Training Loss: 1.819679164520615\t lr: 0.001\n",
      "Epoch [12/100]\t Validation Loss: 1.7876402773434603\t lr: 0.001\n",
      "Epoch [12/100]\t\n",
      "Epoch [13/100]\t Training Loss: 1.7591100284815444\t lr: 0.001\n",
      "Epoch [13/100]\t Validation Loss: 1.724698781967163\t lr: 0.001\n",
      "Epoch [13/100]\t\n",
      "Epoch [14/100]\t Training Loss: 1.6952373478418725\t lr: 0.001\n",
      "Epoch [14/100]\t Validation Loss: 1.6877829862546316\t lr: 0.001\n",
      "Epoch [14/100]\t\n",
      "Epoch [15/100]\t Training Loss: 1.6384550677540968\t lr: 0.001\n",
      "Epoch [15/100]\t Validation Loss: 1.5991970934445345\t lr: 0.001\n",
      "Epoch [15/100]\t\n",
      "Epoch [16/100]\t Training Loss: 1.592891923606853\t lr: 0.001\n",
      "Epoch [16/100]\t Validation Loss: 1.5734258678895008\t lr: 0.001\n",
      "Epoch [16/100]\t\n",
      "Epoch [17/100]\t Training Loss: 1.5566717337464433\t lr: 0.001\n",
      "Epoch [17/100]\t Validation Loss: 1.5369293885894968\t lr: 0.001\n",
      "Epoch [17/100]\t\n",
      "Epoch [18/100]\t Training Loss: 1.5257837074187102\t lr: 0.001\n",
      "Epoch [18/100]\t Validation Loss: 1.5009011965763719\t lr: 0.001\n",
      "Epoch [18/100]\t\n",
      "Epoch [19/100]\t Training Loss: 1.498397757025326\t lr: 0.001\n",
      "Epoch [19/100]\t Validation Loss: 1.4761234386057793\t lr: 0.001\n",
      "Epoch [19/100]\t\n",
      "Epoch [20/100]\t Training Loss: 1.477764524157395\t lr: 0.001\n",
      "Epoch [20/100]\t Validation Loss: 1.4797994124738476\t lr: 0.001\n",
      "Epoch [20/100]\t\n",
      "Epoch [21/100]\t Training Loss: 1.4552925858656158\t lr: 0.001\n",
      "Epoch [21/100]\t Validation Loss: 1.4403081649466405\t lr: 0.001\n",
      "Epoch [21/100]\t\n",
      "Epoch [22/100]\t Training Loss: 1.435464865411334\t lr: 0.001\n",
      "Epoch [22/100]\t Validation Loss: 1.5054597009586383\t lr: 0.001\n",
      "Epoch [22/100]\t\n",
      "Epoch [23/100]\t Training Loss: 1.4164993464184539\t lr: 0.001\n",
      "Epoch [23/100]\t Validation Loss: 1.4073442944997474\t lr: 0.001\n",
      "Epoch [23/100]\t\n",
      "Epoch [24/100]\t Training Loss: 1.4008452066070283\t lr: 0.001\n",
      "Epoch [24/100]\t Validation Loss: 1.3987274335909494\t lr: 0.001\n",
      "Epoch [24/100]\t\n",
      "Epoch [25/100]\t Training Loss: 1.382021994541978\t lr: 0.001\n",
      "Epoch [25/100]\t Validation Loss: 1.4161225828943373\t lr: 0.001\n",
      "Epoch [25/100]\t\n",
      "Epoch [26/100]\t Training Loss: 1.3677555151912562\t lr: 0.001\n",
      "Epoch [26/100]\t Validation Loss: 1.3716739612289621\t lr: 0.001\n",
      "Epoch [26/100]\t\n",
      "Epoch [27/100]\t Training Loss: 1.3521673279954953\t lr: 0.001\n",
      "Epoch [27/100]\t Validation Loss: 1.3692757525021517\t lr: 0.001\n",
      "Epoch [27/100]\t\n",
      "Epoch [28/100]\t Training Loss: 1.3368254043257144\t lr: 0.001\n",
      "Epoch [28/100]\t Validation Loss: 1.3524655722364594\t lr: 0.001\n",
      "Epoch [28/100]\t\n",
      "Epoch [29/100]\t Training Loss: 1.322686879531197\t lr: 0.001\n",
      "Epoch [29/100]\t Validation Loss: 1.3256339664700665\t lr: 0.001\n",
      "Epoch [29/100]\t\n",
      "Epoch [30/100]\t Training Loss: 1.3103530138654782\t lr: 0.001\n",
      "Epoch [30/100]\t Validation Loss: 1.3270396054545535\t lr: 0.001\n",
      "Epoch [30/100]\t\n",
      "Epoch [31/100]\t Training Loss: 1.3010507599472085\t lr: 0.001\n",
      "Epoch [31/100]\t Validation Loss: 1.3124659167060369\t lr: 0.001\n",
      "Epoch [31/100]\t\n",
      "Epoch [32/100]\t Training Loss: 1.286932387620287\t lr: 0.001\n",
      "Epoch [32/100]\t Validation Loss: 1.2995989835714992\t lr: 0.001\n",
      "Epoch [32/100]\t\n",
      "Epoch [33/100]\t Training Loss: 1.271666070994209\t lr: 0.001\n",
      "Epoch [33/100]\t Validation Loss: 1.2948257455342933\t lr: 0.001\n",
      "Epoch [33/100]\t\n",
      "Epoch [34/100]\t Training Loss: 1.2642734895276901\t lr: 0.001\n",
      "Epoch [34/100]\t Validation Loss: 1.298402086088929\t lr: 0.001\n",
      "Epoch [34/100]\t\n",
      "Epoch [35/100]\t Training Loss: 1.2495707727759087\t lr: 0.001\n",
      "Epoch [35/100]\t Validation Loss: 1.2655798332600654\t lr: 0.001\n",
      "Epoch [35/100]\t\n",
      "Epoch [36/100]\t Training Loss: 1.2393022466193684\t lr: 0.001\n",
      "Epoch [36/100]\t Validation Loss: 1.2670331204993814\t lr: 0.001\n",
      "Epoch [36/100]\t\n",
      "Epoch [37/100]\t Training Loss: 1.2294280990920103\t lr: 0.001\n",
      "Epoch [37/100]\t Validation Loss: 1.261136735541911\t lr: 0.001\n",
      "Epoch [37/100]\t\n",
      "Epoch [38/100]\t Training Loss: 1.2174031007320374\t lr: 0.001\n",
      "Epoch [38/100]\t Validation Loss: 1.2759786663176138\t lr: 0.001\n",
      "Epoch [38/100]\t\n",
      "Epoch [39/100]\t Training Loss: 1.206451859620526\t lr: 0.001\n",
      "Epoch [39/100]\t Validation Loss: 1.2576642308054091\t lr: 0.001\n",
      "Epoch [39/100]\t\n",
      "Epoch [40/100]\t Training Loss: 1.19577141826415\t lr: 0.001\n",
      "Epoch [40/100]\t Validation Loss: 1.2659917632235755\t lr: 0.001\n",
      "Epoch [40/100]\t\n",
      "Epoch [41/100]\t Training Loss: 1.1882012829451305\t lr: 0.001\n",
      "Epoch [41/100]\t Validation Loss: 1.2736758348308033\t lr: 0.001\n",
      "Epoch [41/100]\t\n",
      "Epoch [42/100]\t Training Loss: 1.1770282851155762\t lr: 0.001\n",
      "Epoch [42/100]\t Validation Loss: 1.237490886374365\t lr: 0.001\n",
      "Epoch [42/100]\t\n",
      "Epoch [43/100]\t Training Loss: 1.1719567489136211\t lr: 0.001\n",
      "Epoch [43/100]\t Validation Loss: 1.2987334607522698\t lr: 0.001\n",
      "Epoch [43/100]\t\n",
      "Epoch [44/100]\t Training Loss: 1.1602525804048913\t lr: 0.001\n",
      "Epoch [44/100]\t Validation Loss: 1.2417191040666797\t lr: 0.001\n",
      "Epoch [44/100]\t\n",
      "Epoch [45/100]\t Training Loss: 1.1487648054157071\t lr: 0.001\n",
      "Epoch [45/100]\t Validation Loss: 1.2282621996312202\t lr: 0.001\n",
      "Epoch [45/100]\t\n",
      "Epoch [46/100]\t Training Loss: 1.1389838638512984\t lr: 0.001\n",
      "Epoch [46/100]\t Validation Loss: 1.233940277673021\t lr: 0.001\n",
      "Epoch [46/100]\t\n",
      "Epoch [47/100]\t Training Loss: 1.1291444504352481\t lr: 0.001\n",
      "Epoch [47/100]\t Validation Loss: 1.2181057235862636\t lr: 0.001\n",
      "Epoch [47/100]\t\n",
      "Epoch [48/100]\t Training Loss: 1.1246803081249033\t lr: 0.001\n",
      "Epoch [48/100]\t Validation Loss: 1.2248604984223088\t lr: 0.001\n",
      "Epoch [48/100]\t\n",
      "Epoch [49/100]\t Training Loss: 1.1193375549352993\t lr: 0.001\n",
      "Epoch [49/100]\t Validation Loss: 1.2338983997513977\t lr: 0.001\n",
      "Epoch [49/100]\t\n",
      "Epoch [50/100]\t Training Loss: 1.1093887628801644\t lr: 0.001\n",
      "Epoch [50/100]\t Validation Loss: 1.202911835682543\t lr: 0.001\n",
      "Epoch [50/100]\t\n",
      "Epoch [51/100]\t Training Loss: 1.102749909891192\t lr: 0.001\n",
      "Epoch [51/100]\t Validation Loss: 1.2195973471750188\t lr: 0.001\n",
      "Epoch [51/100]\t\n",
      "Epoch [52/100]\t Training Loss: 1.0916311027448806\t lr: 0.001\n",
      "Epoch [52/100]\t Validation Loss: 1.2075901921791365\t lr: 0.001\n",
      "Epoch [52/100]\t\n",
      "Epoch [53/100]\t Training Loss: 1.0818259863902235\t lr: 0.001\n",
      "Epoch [53/100]\t Validation Loss: 1.2005773005606253\t lr: 0.001\n",
      "Epoch [53/100]\t\n",
      "Epoch [54/100]\t Training Loss: 1.0722773023273633\t lr: 0.001\n",
      "Epoch [54/100]\t Validation Loss: 1.1966545710080787\t lr: 0.001\n",
      "Epoch [54/100]\t\n",
      "Epoch [55/100]\t Training Loss: 1.0674182463180073\t lr: 0.001\n",
      "Epoch [55/100]\t Validation Loss: 1.1970489613617523\t lr: 0.001\n",
      "Epoch [55/100]\t\n",
      "Epoch [56/100]\t Training Loss: 1.0605497120896263\t lr: 0.001\n",
      "Epoch [56/100]\t Validation Loss: 1.1876832307139529\t lr: 0.001\n",
      "Epoch [56/100]\t\n",
      "Epoch [57/100]\t Training Loss: 1.05364165906711\t lr: 0.001\n",
      "Epoch [57/100]\t Validation Loss: 1.227691213541393\t lr: 0.001\n",
      "Epoch [57/100]\t\n",
      "Epoch [58/100]\t Training Loss: 1.0447506177455872\t lr: 0.001\n",
      "Epoch [58/100]\t Validation Loss: 1.1895705725573287\t lr: 0.001\n",
      "Epoch [58/100]\t\n",
      "Epoch [59/100]\t Training Loss: 1.0376696489046298\t lr: 0.001\n",
      "Epoch [59/100]\t Validation Loss: 1.1934683918952942\t lr: 0.001\n",
      "Epoch [59/100]\t\n",
      "Epoch [60/100]\t Training Loss: 1.0331155087636865\t lr: 0.001\n",
      "Epoch [60/100]\t Validation Loss: 1.2079860198346875\t lr: 0.001\n",
      "Epoch [60/100]\t\n",
      "Epoch [61/100]\t Training Loss: 1.0292625747373343\t lr: 0.001\n",
      "Epoch [61/100]\t Validation Loss: 1.179662486420402\t lr: 0.001\n",
      "Epoch [61/100]\t\n",
      "Epoch [62/100]\t Training Loss: 1.0145440597058561\t lr: 0.001\n",
      "Epoch [62/100]\t Validation Loss: 1.1935317267345478\t lr: 0.001\n",
      "Epoch [62/100]\t\n",
      "Epoch [63/100]\t Training Loss: 1.0093712414926885\t lr: 0.001\n",
      "Epoch [63/100]\t Validation Loss: 1.1931957119627843\t lr: 0.001\n",
      "Epoch [63/100]\t\n",
      "Epoch [64/100]\t Training Loss: 1.0038631110240126\t lr: 0.001\n",
      "Epoch [64/100]\t Validation Loss: 1.1764911360378507\t lr: 0.001\n",
      "Epoch [64/100]\t\n",
      "Epoch [65/100]\t Training Loss: 0.9954003216055654\t lr: 0.001\n",
      "Epoch [65/100]\t Validation Loss: 1.2027152814442599\t lr: 0.001\n",
      "Epoch [65/100]\t\n",
      "Epoch [66/100]\t Training Loss: 0.990129352530555\t lr: 0.001\n",
      "Epoch [66/100]\t Validation Loss: 1.2064516831047927\t lr: 0.001\n",
      "Epoch [66/100]\t\n",
      "Epoch [67/100]\t Training Loss: 0.9846008262975746\t lr: 0.001\n",
      "Epoch [67/100]\t Validation Loss: 1.1895002564297448\t lr: 0.001\n",
      "Epoch [67/100]\t\n",
      "Epoch [68/100]\t Training Loss: 0.9757405992054269\t lr: 0.001\n",
      "Epoch [68/100]\t Validation Loss: 1.180897667438169\t lr: 0.001\n",
      "Epoch [68/100]\t\n",
      "Epoch [69/100]\t Training Loss: 0.9649260676730319\t lr: 0.001\n",
      "Epoch [69/100]\t Validation Loss: 1.1968415794493277\t lr: 0.001\n",
      "Epoch [69/100]\t\n",
      "Epoch [70/100]\t Training Loss: 0.9627511504361087\t lr: 0.001\n",
      "Epoch [70/100]\t Validation Loss: 1.1870004911965961\t lr: 0.001\n",
      "Epoch [70/100]\t\n",
      "Epoch [71/100]\t Training Loss: 0.9561550691914376\t lr: 0.001\n",
      "Epoch [71/100]\t Validation Loss: 1.18967751762535\t lr: 0.001\n",
      "Epoch [71/100]\t\n",
      "Epoch [72/100]\t Training Loss: 0.9492009925415449\t lr: 0.001\n",
      "Epoch [72/100]\t Validation Loss: 1.168776359739183\t lr: 0.001\n",
      "Epoch [72/100]\t\n",
      "Epoch [73/100]\t Training Loss: 0.9463910760781954\t lr: 0.001\n",
      "Epoch [73/100]\t Validation Loss: 1.1573427930662903\t lr: 0.001\n",
      "Epoch [73/100]\t\n",
      "Epoch [74/100]\t Training Loss: 0.9350023789478995\t lr: 0.001\n",
      "Epoch [74/100]\t Validation Loss: 1.1603164242792734\t lr: 0.001\n",
      "Epoch [74/100]\t\n",
      "Epoch [75/100]\t Training Loss: 0.9307331668446436\t lr: 0.001\n",
      "Epoch [75/100]\t Validation Loss: 1.203941151311126\t lr: 0.001\n",
      "Epoch [75/100]\t\n",
      "Epoch [76/100]\t Training Loss: 0.9274445985589186\t lr: 0.001\n",
      "Epoch [76/100]\t Validation Loss: 1.183736823027647\t lr: 0.001\n",
      "Epoch [76/100]\t\n",
      "Epoch [77/100]\t Training Loss: 0.9164762391763575\t lr: 0.001\n",
      "Epoch [77/100]\t Validation Loss: 1.178137628338005\t lr: 0.001\n",
      "Epoch [77/100]\t\n",
      "Epoch [78/100]\t Training Loss: 0.9105620306471119\t lr: 0.001\n",
      "Epoch [78/100]\t Validation Loss: 1.1716988116880007\t lr: 0.001\n",
      "Epoch [78/100]\t\n",
      "Epoch [79/100]\t Training Loss: 0.9052822317003899\t lr: 0.001\n",
      "Epoch [79/100]\t Validation Loss: 1.1953339946420887\t lr: 0.001\n",
      "Epoch [79/100]\t\n",
      "Epoch [80/100]\t Training Loss: 0.9012671306614986\t lr: 0.001\n",
      "Epoch [80/100]\t Validation Loss: 1.1767015019549598\t lr: 0.001\n",
      "Epoch [80/100]\t\n",
      "Epoch [81/100]\t Training Loss: 0.8957358116993819\t lr: 0.001\n",
      "Epoch [81/100]\t Validation Loss: 1.1779103015042558\t lr: 0.001\n",
      "Epoch [81/100]\t\n",
      "Epoch [82/100]\t Training Loss: 0.893250250298044\t lr: 0.001\n",
      "Epoch [82/100]\t Validation Loss: 1.172968484178374\t lr: 0.001\n",
      "Epoch [82/100]\t\n",
      "Epoch [83/100]\t Training Loss: 0.8852970476650521\t lr: 0.001\n",
      "Epoch [83/100]\t Validation Loss: 1.170515697968157\t lr: 0.001\n",
      "Epoch [83/100]\t\n",
      "Epoch [84/100]\t Training Loss: 0.8834642621562304\t lr: 0.001\n",
      "Epoch [84/100]\t Validation Loss: 1.164961546282225\t lr: 0.001\n",
      "Epoch [84/100]\t\n",
      "Epoch [85/100]\t Training Loss: 0.8746925801267404\t lr: 0.001\n",
      "Epoch [85/100]\t Validation Loss: 1.1760388024245636\t lr: 0.001\n",
      "Epoch [85/100]\t\n",
      "Epoch [86/100]\t Training Loss: 0.8686443770023258\t lr: 0.001\n",
      "Epoch [86/100]\t Validation Loss: 1.171621146081369\t lr: 0.001\n",
      "Epoch [86/100]\t\n",
      "Epoch [87/100]\t Training Loss: 0.8641703415404806\t lr: 0.001\n",
      "Epoch [87/100]\t Validation Loss: 1.1933958998209313\t lr: 0.001\n",
      "Epoch [87/100]\t\n",
      "Epoch [88/100]\t Training Loss: 0.8561232722628757\t lr: 0.001\n",
      "Epoch [88/100]\t Validation Loss: 1.281855432293083\t lr: 0.001\n",
      "Epoch [88/100]\t\n",
      "Epoch [89/100]\t Training Loss: 0.8519018321390956\t lr: 0.001\n",
      "Epoch [89/100]\t Validation Loss: 1.1783306908003892\t lr: 0.001\n",
      "Epoch [89/100]\t\n",
      "Epoch [90/100]\t Training Loss: 0.8447719333726732\t lr: 0.001\n",
      "Epoch [90/100]\t Validation Loss: 1.173874302000939\t lr: 0.001\n",
      "Epoch [90/100]\t\n",
      "Epoch [91/100]\t Training Loss: 0.8402201478438609\t lr: 0.001\n",
      "Epoch [91/100]\t Validation Loss: 1.1809882747976086\t lr: 0.001\n",
      "Epoch [91/100]\t\n",
      "Epoch [92/100]\t Training Loss: 0.8393004774437536\t lr: 0.001\n",
      "Epoch [92/100]\t Validation Loss: 1.1835912367965602\t lr: 0.001\n",
      "Epoch [92/100]\t\n",
      "Epoch [93/100]\t Training Loss: 0.8310653370664552\t lr: 0.001\n",
      "Epoch [93/100]\t Validation Loss: 1.1818565567837487\t lr: 0.001\n",
      "Epoch [93/100]\t\n",
      "Epoch [94/100]\t Training Loss: 0.8275952677592597\t lr: 0.001\n",
      "Epoch [94/100]\t Validation Loss: 1.1876856601690944\t lr: 0.001\n",
      "Epoch [94/100]\t\n",
      "Epoch [95/100]\t Training Loss: 0.8195811075627651\t lr: 0.001\n",
      "Epoch [95/100]\t Validation Loss: 1.1925967939292328\t lr: 0.001\n",
      "Epoch [95/100]\t\n",
      "Epoch [96/100]\t Training Loss: 0.8146284320165434\t lr: 0.001\n",
      "Epoch [96/100]\t Validation Loss: 1.1739405710485917\t lr: 0.001\n",
      "Epoch [96/100]\t\n",
      "Epoch [97/100]\t Training Loss: 0.8095161623662085\t lr: 0.001\n",
      "Epoch [97/100]\t Validation Loss: 1.2030472204655032\t lr: 0.001\n",
      "Epoch [97/100]\t\n",
      "Epoch [98/100]\t Training Loss: 0.8075589202249142\t lr: 0.001\n",
      "Epoch [98/100]\t Validation Loss: 1.1960513659670382\t lr: 0.001\n",
      "Epoch [98/100]\t\n",
      "Epoch [99/100]\t Training Loss: 0.7989282707119232\t lr: 0.001\n",
      "Epoch [99/100]\t Validation Loss: 1.1897142763379254\t lr: 0.001\n",
      "Epoch [99/100]\t\n",
      "Epoch [100/100]\t Training Loss: 0.791234406211492\t lr: 0.001\n",
      "Epoch [100/100]\t Validation Loss: 1.1876680571821672\t lr: 0.001\n",
      "Epoch [99/100]\t Time Taken: 13.087552817662557 minutes\n"
     ]
    }
   ],
   "source": [
    "train(net_ori, train_loader, train_criterion,val_loader, val_criterion, \n",
    "              optimizer, 100, writer, \n",
    "                 PATH, False, None, device=\"cuda\",\n",
    "                  n_steps_show=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5deda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 60 %\n"
     ]
    }
   ],
   "source": [
    "#test clean accuray\n",
    "test(val_loader, net_ori)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2950a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = BaseNet().to(device)\n",
    "#test attacks \n",
    "pgd_attack = generate_adv(net_ori, \"pgd\")\n",
    "fgsm_attack = generate_adv(net_ori, \"fgsm\")\n",
    "nifgsm_attack = generate_adv(net_ori, \"nifgsm\")\n",
    "vmifgsm_attack = generate_adv(net_ori, \"vmifgsm\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51fbab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_dataset(10000, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf27f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 1.02 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "adv_images_pgd_adv = pgd_attack(images, labels)\n",
    "acc = clean_accuracy(net_ori, adv_images_pgd_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"adv model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24cb7e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 3.65 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "adv_images_fgsm_adv = fgsm_attack(images, labels)\n",
    "acc = clean_accuracy(net_ori, adv_images_fgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"adv model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1292bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 1.88 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "#nifgsm_attack = generate_adv(net, \"nifgsm\")\n",
    "\n",
    "adv_images_nifgsm_adv = nifgsm_attack(images, labels)\n",
    "acc = clean_accuracy(net_ori, adv_images_nifgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"adv model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f16e483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 1.09 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "#vmifgsm_attack = generate_adv(net, \"vmifgsm\")\n",
    "adv_images_vmifgsm_adv =vmifgsm_attack(images, labels)\n",
    "acc = clean_accuracy(net_ori, adv_images_vmifgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"adv model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb31099",
   "metadata": {},
   "source": [
    "# Adversarial Training on shallow CNN model by PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeafbdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 8.5394e-02, -1.1512e-01, -1.0861e-01, -1.9637e-02, -1.8678e-02],\n",
      "          [ 3.7605e-02, -6.2260e-02, -5.1802e-02,  5.4722e-02, -5.2045e-02],\n",
      "          [-3.0720e-02,  1.0825e-01, -7.0859e-02, -6.5885e-02,  4.8625e-02],\n",
      "          [ 8.8407e-02, -3.9397e-02, -4.9881e-02,  1.5771e-02, -4.9440e-02],\n",
      "          [-1.0586e-01, -6.3314e-02,  5.0533e-02,  1.1038e-01, -6.0548e-02]],\n",
      "\n",
      "         [[-9.8391e-02, -3.9039e-02, -3.9520e-02, -7.5328e-02, -6.3877e-02],\n",
      "          [-6.8654e-02,  3.4659e-02, -8.7734e-02, -7.3791e-02,  5.3084e-02],\n",
      "          [-8.5928e-02, -1.0200e-01, -9.4525e-02,  9.3352e-02,  9.8604e-03],\n",
      "          [ 3.7303e-03, -9.6609e-02, -4.9321e-02, -1.0189e-01, -7.0317e-02],\n",
      "          [-8.2237e-02, -5.6334e-02,  3.2203e-02,  3.9847e-02,  1.0035e-01]],\n",
      "\n",
      "         [[ 2.1195e-02, -1.9120e-03,  5.2093e-02, -6.7285e-02,  1.0167e-01],\n",
      "          [-6.7320e-02, -4.1070e-02,  7.0719e-02,  9.2750e-02, -4.3728e-02],\n",
      "          [-6.5170e-02, -1.5343e-02,  6.9010e-02, -2.1152e-02, -2.5351e-02],\n",
      "          [ 7.9767e-02,  4.0638e-02,  1.9075e-02, -3.5609e-02, -3.0423e-02],\n",
      "          [-7.0083e-02, -5.2983e-02, -1.1362e-01, -2.1252e-03,  2.6328e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6651e-01,  2.7588e-01,  2.9060e-01,  2.4264e-01,  1.2931e-01],\n",
      "          [ 1.7005e-01,  3.4774e-01,  3.5728e-01,  4.6115e-01,  1.4335e-01],\n",
      "          [ 2.0589e-01,  5.2396e-01,  4.4666e-01,  4.5583e-01,  4.2255e-01],\n",
      "          [ 1.8188e-01,  3.1558e-01,  4.1267e-01,  5.6933e-01,  3.8408e-01],\n",
      "          [ 2.7285e-01,  3.3539e-01,  4.5967e-01,  4.3128e-01,  2.6640e-01]],\n",
      "\n",
      "         [[-1.4895e-01, -4.7775e-02,  5.0587e-03, -7.9390e-03, -1.0506e-01],\n",
      "          [-2.2977e-01, -5.5671e-02,  1.7230e-01,  1.4821e-01, -8.3430e-02],\n",
      "          [-2.0921e-01,  5.0326e-02, -1.0324e-01, -2.4468e-02, -1.8976e-01],\n",
      "          [-2.8090e-01, -1.3532e-01, -1.2532e-01, -2.6032e-01, -2.7066e-01],\n",
      "          [-3.9955e-01, -1.9051e-01, -1.2339e-01, -2.1174e-01, -3.4291e-01]],\n",
      "\n",
      "         [[-4.1282e-02,  2.4250e-01,  2.9881e-01,  9.5045e-02,  2.0494e-02],\n",
      "          [-1.0751e-02,  1.1251e-01,  1.1280e-01,  1.0900e-01,  6.0855e-02],\n",
      "          [-2.5312e-01,  2.1439e-02,  1.5961e-01, -3.5063e-02, -2.4030e-01],\n",
      "          [-1.6668e-01,  4.7567e-03, -5.9373e-02, -6.9627e-02, -2.6183e-01],\n",
      "          [-1.4550e-01, -1.7466e-01, -1.3711e-01, -1.1780e-01, -1.5401e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.9036e-02, -9.5487e-02,  1.4967e-01, -8.9374e-02, -1.9983e-01],\n",
      "          [-1.3661e-01, -2.7430e-01, -2.8327e-02,  5.4807e-01,  4.9040e-01],\n",
      "          [ 1.3396e-01, -2.7429e-01, -6.8461e-01, -2.8981e-01,  1.7945e-02],\n",
      "          [ 1.8051e-01,  2.8651e-01, -6.3232e-02, -6.0318e-01, -3.7623e-01],\n",
      "          [-1.2409e-01,  1.9307e-01,  1.9335e-01,  3.3818e-01,  1.4058e-01]],\n",
      "\n",
      "         [[ 1.1860e-02,  5.3790e-02,  2.7830e-01,  1.8782e-01, -2.1210e-01],\n",
      "          [-5.8278e-02, -1.1449e-01,  4.9230e-02,  7.2735e-01,  6.5480e-01],\n",
      "          [ 2.1282e-01, -1.6283e-01, -5.5178e-01, -5.4920e-02,  5.0765e-02],\n",
      "          [ 3.1295e-01,  3.3241e-01, -4.4441e-02, -5.4949e-01, -3.3393e-01],\n",
      "          [-2.1526e-01,  2.1098e-01,  3.1634e-01,  3.0783e-01,  7.5104e-02]],\n",
      "\n",
      "         [[-1.5166e-01,  4.6955e-02,  1.1587e-01, -1.4881e-02, -3.9537e-01],\n",
      "          [-1.0708e-01, -2.2613e-01, -8.2343e-02,  5.4797e-01,  4.3614e-01],\n",
      "          [ 2.6907e-01, -3.4676e-01, -5.9784e-01, -8.5600e-02, -1.6014e-02],\n",
      "          [ 2.6686e-01,  2.1286e-01, -1.9370e-01, -5.6527e-01, -5.0506e-01],\n",
      "          [-2.5253e-01,  1.9195e-01,  2.1567e-01,  2.1995e-01,  1.8271e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3548e-01,  1.0228e-01,  2.2277e-01,  1.4536e-01,  4.7862e-02],\n",
      "          [ 3.5870e-01,  5.5821e-01,  4.5305e-01,  4.3432e-01,  3.3211e-01],\n",
      "          [ 2.9323e-01,  4.2375e-01,  4.9067e-01,  2.8165e-01,  1.6605e-01],\n",
      "          [-1.0603e-01, -3.2233e-01, -4.3843e-01, -4.1459e-01, -3.6669e-01],\n",
      "          [-2.4858e-01, -5.3355e-01, -5.0289e-01, -3.9548e-01, -1.1631e-01]],\n",
      "\n",
      "         [[-2.4572e-01, -2.8415e-01, -9.2746e-02,  6.1198e-02,  9.3976e-02],\n",
      "          [ 2.7279e-01,  2.8690e-01,  3.0825e-01,  3.0001e-01,  2.0407e-01],\n",
      "          [ 2.8217e-01,  4.7251e-01,  3.4095e-01,  2.3005e-01, -1.2388e-01],\n",
      "          [-5.4507e-02, -3.9380e-01, -3.0207e-01, -2.8207e-01, -4.4506e-01],\n",
      "          [-2.0340e-01, -4.5228e-01, -4.5818e-01, -4.3081e-01, -8.6316e-02]],\n",
      "\n",
      "         [[-2.0800e-01, -1.9029e-01, -1.7049e-01,  3.1606e-03,  6.6797e-02],\n",
      "          [ 2.3341e-01,  2.7416e-01,  3.6641e-01,  1.5812e-01,  5.4155e-02],\n",
      "          [ 4.1870e-01,  4.8393e-01,  2.9955e-01,  1.9563e-01,  6.0280e-02],\n",
      "          [ 1.1755e-01, -2.8170e-02, -2.5053e-01, -2.4439e-01, -2.6721e-01],\n",
      "          [-1.4822e-01, -3.2782e-01, -2.9720e-01, -2.1787e-01,  9.4821e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8732e-01, -1.8406e-01, -1.9954e-01, -2.8900e-01, -2.4194e-01],\n",
      "          [-2.8541e-01, -1.7118e-01, -8.2273e-02, -4.4151e-02, -1.1416e-01],\n",
      "          [-2.7400e-01, -1.9965e-01, -1.7918e-01, -2.3855e-02, -1.7724e-01],\n",
      "          [-3.1868e-01, -1.0225e-01, -1.5402e-01, -3.0676e-02, -1.5454e-01],\n",
      "          [-2.4707e-01, -1.6979e-01, -1.6424e-01, -9.2284e-02, -1.1031e-01]],\n",
      "\n",
      "         [[ 2.1693e-02,  2.5450e-01,  2.0947e-01,  3.4578e-01,  9.0561e-02],\n",
      "          [ 2.0725e-01,  2.2186e-01,  2.5896e-01,  2.0985e-01,  1.7686e-01],\n",
      "          [ 9.5516e-02,  2.7355e-01,  2.5916e-01,  1.8245e-01,  2.5667e-01],\n",
      "          [-5.8418e-02,  1.0261e-01,  1.9420e-01,  2.5380e-01,  2.2457e-01],\n",
      "          [ 3.3529e-02,  7.3789e-02,  1.1785e-01,  1.2959e-01,  1.1559e-01]],\n",
      "\n",
      "         [[ 2.8453e-01,  4.6081e-01,  5.7211e-01,  3.6852e-01,  3.1580e-01],\n",
      "          [ 2.7902e-01,  5.0036e-01,  6.0667e-01,  4.8231e-01,  2.2476e-01],\n",
      "          [ 2.8337e-01,  4.0563e-01,  5.6403e-01,  4.5753e-01,  3.2737e-01],\n",
      "          [ 1.7288e-01,  1.9763e-01,  3.1251e-01,  1.8709e-01,  1.8547e-01],\n",
      "          [ 4.7624e-03,  7.0985e-02,  2.9028e-01,  1.3886e-01,  1.7339e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.1138e-01, -4.2967e-01, -6.8376e-02,  7.4063e-02, -2.5199e-02],\n",
      "          [-2.2734e-01, -2.2392e-02,  2.8737e-01,  4.0508e-02, -9.6763e-02],\n",
      "          [-3.3053e-02,  7.1067e-02,  2.7648e-01,  6.6388e-02,  1.6958e-01],\n",
      "          [ 3.7441e-02,  1.8488e-01,  2.5885e-01,  1.8733e-01,  9.1747e-02],\n",
      "          [ 1.6381e-01,  3.1192e-01,  7.3209e-02,  1.5690e-01,  4.6161e-02]],\n",
      "\n",
      "         [[-1.2826e-01, -1.3557e-01,  2.6199e-01,  2.8733e-01,  1.3404e-01],\n",
      "          [-1.0534e-01,  1.8400e-01,  3.9759e-01,  2.6478e-01,  1.3735e-01],\n",
      "          [ 2.3564e-01,  3.5783e-01,  4.3168e-01,  3.2866e-01,  2.2156e-01],\n",
      "          [ 1.6002e-01,  4.3105e-01,  2.8348e-01,  2.2965e-01,  1.1877e-01],\n",
      "          [ 2.2534e-01,  3.1219e-01,  2.4026e-01,  3.5472e-01,  2.1064e-01]],\n",
      "\n",
      "         [[-4.1656e-01, -4.7301e-01,  1.5570e-03, -3.6342e-02, -1.3108e-01],\n",
      "          [-5.1688e-01, -7.7945e-02,  2.8089e-01,  1.0489e-01, -9.0168e-02],\n",
      "          [-3.5957e-01, -8.0217e-02, -2.1452e-02, -2.6029e-03,  7.3567e-02],\n",
      "          [-2.8959e-01, -5.5582e-02, -4.6306e-02, -1.5631e-01, -3.0549e-02],\n",
      "          [-6.9897e-02, -1.1458e-01, -4.9922e-02, -6.9747e-04, -2.3850e-01]]]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0722, -0.4548, -0.0179, -0.0590, -0.4793,  0.1897], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-1.7991e-02,  1.6506e-02, -5.7457e-02, -8.1159e-02,  1.7128e-02],\n",
      "          [-2.4759e-02,  7.1722e-02, -7.8490e-02, -1.6711e-02, -4.7275e-02],\n",
      "          [ 1.8966e-02,  7.3581e-02,  6.4056e-02,  8.0125e-02, -6.1783e-02],\n",
      "          [-4.9870e-02, -6.7771e-02, -1.5870e-02,  5.5299e-02, -1.4187e-02],\n",
      "          [-1.1354e-02,  7.1354e-02, -5.7442e-02, -4.6765e-02,  3.0323e-02]],\n",
      "\n",
      "         [[-7.4767e-02, -1.1738e-01,  4.1221e-01,  7.6025e-02,  3.8291e-03],\n",
      "          [-1.3663e-01, -3.0159e-02,  5.0326e-01, -6.9634e-02, -1.0285e-01],\n",
      "          [-1.2798e-01,  1.2407e-01,  2.3748e-01, -1.8956e-01,  2.8175e-02],\n",
      "          [ 2.5101e-02,  1.1921e-01,  4.3753e-02, -1.2842e-01, -1.3921e-02],\n",
      "          [ 2.0570e-01, -6.9329e-03, -6.7187e-02,  6.0407e-02,  1.1882e-02]],\n",
      "\n",
      "         [[-4.1217e-02, -6.2464e-02, -1.4822e-01, -7.0674e-02, -3.1306e-02],\n",
      "          [-6.9041e-02, -3.7867e-02, -1.6177e-01, -1.8027e-01, -6.1425e-02],\n",
      "          [-6.5261e-02, -8.6295e-02, -2.0798e-01, -1.1311e-01,  4.8139e-02],\n",
      "          [-8.5800e-02, -1.3132e-01, -1.6493e-01, -1.1030e-01, -2.8086e-02],\n",
      "          [-9.0668e-02, -1.4844e-01, -1.0255e-01, -3.6207e-02,  4.3650e-02]],\n",
      "\n",
      "         [[ 6.3093e-02, -2.0710e-01, -1.3058e-01,  1.3201e-01,  7.7109e-03],\n",
      "          [-5.9888e-02, -1.3541e-01, -4.1790e-03,  7.5303e-02, -2.2013e-01],\n",
      "          [-2.1327e-01,  1.4528e-02,  9.9442e-02, -2.1627e-01, -2.1612e-01],\n",
      "          [-2.6789e-01,  1.2654e-01,  5.9180e-02, -1.3407e-01, -5.5627e-02],\n",
      "          [-1.8173e-01, -7.9221e-02,  1.9468e-02, -2.8725e-02, -4.0454e-02]],\n",
      "\n",
      "         [[-6.2192e-02,  1.3625e-01,  2.7789e-01, -6.0275e-02, -1.5902e-01],\n",
      "          [-9.0145e-02,  1.4304e-01,  5.0393e-01, -2.5086e-01, -2.3794e-01],\n",
      "          [-4.6464e-02,  2.1608e-01,  2.6146e-01, -3.5269e-01, -2.1987e-01],\n",
      "          [-1.0005e-02,  8.1041e-02,  1.0549e-01, -1.9310e-01, -6.8131e-02],\n",
      "          [-1.5323e-02, -2.4680e-02,  3.1525e-02, -6.2216e-02, -2.6219e-01]],\n",
      "\n",
      "         [[ 3.3332e-02,  9.1546e-02,  4.6411e-01, -2.0212e-01, -1.6568e-01],\n",
      "          [-2.2730e-01,  2.1409e-01,  4.8530e-01, -4.8810e-01,  1.2999e-02],\n",
      "          [-8.1246e-02,  2.3392e-01,  1.1063e-01, -3.6430e-01,  1.9775e-01],\n",
      "          [ 5.4651e-02,  1.0141e-01, -1.3056e-01, -7.5520e-02,  1.0817e-01],\n",
      "          [ 1.9981e-01,  1.2414e-01, -1.6668e-02,  1.0738e-01,  2.4816e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.3994e-03,  7.0965e-02, -6.1516e-02,  6.3090e-02,  2.9076e-02],\n",
      "          [-3.3268e-02, -5.5786e-02,  3.6459e-02, -1.6865e-02,  3.7953e-02],\n",
      "          [ 1.9259e-02,  7.9094e-02,  1.0913e-02,  3.2353e-02, -6.1413e-02],\n",
      "          [-2.1461e-02, -5.5838e-02,  7.9611e-02,  3.5263e-02, -5.0946e-02],\n",
      "          [-4.1106e-02,  1.7908e-02,  3.0883e-03,  5.1570e-02,  4.0920e-02]],\n",
      "\n",
      "         [[-7.6008e-02, -1.4424e-01, -2.3427e-01, -9.6975e-02, -1.2433e-02],\n",
      "          [-5.8823e-02, -7.9943e-02, -2.4518e-01, -1.8666e-01, -1.0201e-01],\n",
      "          [-4.0657e-02, -1.0401e-01, -2.1271e-01, -1.9450e-01,  3.9159e-02],\n",
      "          [-7.3971e-02, -1.1292e-01, -3.8711e-02, -9.4065e-02,  8.8888e-02],\n",
      "          [ 1.9211e-02, -8.7043e-02,  4.4641e-02,  1.7923e-01,  1.9357e-01]],\n",
      "\n",
      "         [[-6.1044e-02, -8.6069e-02, -3.4815e-02, -1.2715e-02, -5.9175e-02],\n",
      "          [ 1.1843e-01,  4.0705e-02, -5.4826e-02,  3.8973e-02,  4.5557e-02],\n",
      "          [ 1.9061e-01,  2.8425e-02,  8.4256e-02,  7.4348e-02,  9.5384e-02],\n",
      "          [ 3.9832e-02,  1.4540e-01,  7.9092e-02,  1.2041e-01,  1.2238e-01],\n",
      "          [ 1.5865e-01,  5.9456e-02,  1.4696e-01,  8.4188e-02,  1.1152e-01]],\n",
      "\n",
      "         [[ 6.1048e-02, -1.2567e-01, -5.9413e-02, -5.3547e-03,  4.3336e-02],\n",
      "          [ 6.6576e-02,  1.0310e-01,  2.5150e-02, -5.3634e-02, -1.6178e-01],\n",
      "          [-4.0542e-02,  1.6398e-01,  8.7687e-02, -1.0414e-01, -1.3215e-01],\n",
      "          [ 7.0280e-02,  1.0212e-01, -1.1578e-02,  1.0110e-01,  8.5631e-02],\n",
      "          [ 1.6901e-02,  7.7270e-02,  1.2029e-01,  3.2315e-02,  3.8201e-02]],\n",
      "\n",
      "         [[ 3.3773e-01,  3.1323e-01,  2.0955e-01,  2.7609e-02,  1.1774e-01],\n",
      "          [ 2.4861e-01,  4.6264e-01,  3.1702e-01,  1.5307e-01,  5.9261e-02],\n",
      "          [ 1.0072e-01,  3.5825e-01,  2.4568e-01, -6.8137e-02, -4.7003e-02],\n",
      "          [-1.3819e-01, -7.2015e-02, -8.3436e-02, -7.1988e-02,  1.1975e-01],\n",
      "          [-4.0427e-01, -4.0131e-01, -2.8791e-01, -8.1796e-02,  9.1987e-02]],\n",
      "\n",
      "         [[-1.4332e-01, -1.6573e-01, -9.1748e-02, -4.6123e-02,  1.0188e-02],\n",
      "          [-3.5476e-02, -6.9464e-02, -9.6778e-02, -1.4800e-01, -1.1035e-01],\n",
      "          [-3.9809e-02, -1.1014e-01, -1.4368e-01, -2.0371e-01,  1.3618e-01],\n",
      "          [-1.3742e-01, -1.3072e-01, -1.8423e-01, -1.0170e-01,  1.9554e-01],\n",
      "          [ 1.2282e-03,  7.5435e-03,  2.5142e-02,  8.6827e-02,  2.2790e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.5931e-03, -7.2750e-03,  3.2928e-02,  1.1184e-02,  3.8623e-02],\n",
      "          [-9.4753e-03,  1.0527e-02, -4.7871e-02,  5.3712e-02,  4.3728e-02],\n",
      "          [-2.0505e-02,  8.0371e-02,  1.3720e-02, -7.2105e-02,  4.4230e-02],\n",
      "          [-2.8151e-02,  9.2058e-03,  3.3467e-02, -7.6887e-02,  6.8940e-02],\n",
      "          [-2.9696e-02, -2.1814e-02, -2.2108e-02,  9.0897e-03,  3.4445e-02]],\n",
      "\n",
      "         [[-8.6719e-02,  1.6390e-02, -1.6408e-02, -3.0295e-02, -6.7543e-02],\n",
      "          [-3.8235e-02, -4.9798e-02,  3.5772e-02, -2.3826e-02,  3.4369e-03],\n",
      "          [-2.5254e-02,  7.0190e-02, -6.4864e-02, -1.8728e-02,  1.7155e-02],\n",
      "          [ 2.6977e-04, -7.3846e-02,  3.9875e-02, -5.4117e-03,  7.8929e-02],\n",
      "          [ 6.4581e-02,  1.8868e-02,  3.7242e-02, -4.5402e-02, -5.4168e-02]],\n",
      "\n",
      "         [[-3.9464e-02,  5.4324e-02,  3.0210e-02,  8.2009e-03,  2.1167e-02],\n",
      "          [ 3.1812e-02, -7.5474e-02, -6.8578e-02,  7.7851e-02,  6.4268e-02],\n",
      "          [-7.1483e-03, -6.6790e-02, -2.8761e-02,  7.1563e-02, -3.3259e-02],\n",
      "          [-3.5545e-02,  6.2099e-02,  7.4560e-02, -1.3816e-02, -9.9449e-03],\n",
      "          [ 6.8277e-02,  3.0458e-03, -3.6393e-02,  7.0853e-02, -6.2482e-02]],\n",
      "\n",
      "         [[ 4.4394e-02, -2.8176e-02,  1.2422e-02,  2.7992e-02, -2.3015e-03],\n",
      "          [ 5.6945e-02, -4.2451e-02, -4.8942e-02, -1.7303e-02,  2.1631e-02],\n",
      "          [-6.2239e-02, -5.4019e-02,  7.1957e-02,  5.2195e-02,  4.7636e-02],\n",
      "          [-7.9698e-02, -1.2547e-03, -5.7533e-02,  7.3628e-02, -7.5171e-02],\n",
      "          [ 6.4144e-02,  4.4767e-02, -5.6779e-02,  2.5434e-02,  6.7621e-03]],\n",
      "\n",
      "         [[ 2.4283e-02,  2.3965e-02, -7.7109e-02,  1.2645e-02, -2.2389e-02],\n",
      "          [ 6.0072e-02,  3.7432e-02, -7.1813e-02, -6.8411e-02, -4.7589e-02],\n",
      "          [ 3.7608e-02, -5.1223e-02, -5.0059e-02, -7.4514e-02,  2.2965e-02],\n",
      "          [-7.8912e-02, -3.2011e-02,  1.0961e-02, -5.2319e-02, -3.6632e-02],\n",
      "          [-5.3182e-02,  1.4291e-03,  3.2802e-02, -4.6811e-03, -7.6540e-02]],\n",
      "\n",
      "         [[-4.3276e-02,  2.1277e-02, -4.5837e-02, -3.9742e-02,  7.2429e-02],\n",
      "          [-4.2051e-02,  7.6027e-02, -1.2479e-02, -2.9402e-02, -8.2839e-02],\n",
      "          [ 4.5621e-02,  7.1056e-02, -6.8306e-02, -6.1454e-02,  2.5700e-02],\n",
      "          [-7.2644e-02, -3.7407e-02, -2.4265e-02, -6.3686e-02, -2.5762e-02],\n",
      "          [ 2.9157e-02,  6.0170e-02, -1.7979e-02,  4.8454e-02,  2.2459e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.6641e-02, -6.3496e-02,  8.3025e-03, -7.9843e-02, -4.2489e-02],\n",
      "          [-4.3078e-03, -4.8779e-02,  8.1609e-02, -2.5421e-02,  7.0777e-02],\n",
      "          [-5.9592e-02,  4.5506e-02,  5.9733e-02,  1.2308e-02, -3.2070e-02],\n",
      "          [-7.6054e-02, -4.2443e-02, -1.6875e-02, -2.1729e-02,  2.5488e-02],\n",
      "          [ 3.6852e-02, -1.1245e-02,  7.8172e-02, -7.8167e-02,  7.2982e-02]],\n",
      "\n",
      "         [[ 4.9133e-02,  6.6670e-02,  7.1663e-02,  1.5075e-01,  1.0371e-01],\n",
      "          [-4.3242e-02,  1.2083e-01,  1.3457e-01,  1.2775e-01,  8.5083e-02],\n",
      "          [-5.5590e-02, -3.6727e-02, -7.3916e-02, -5.9262e-02,  6.7849e-02],\n",
      "          [-4.0789e-02, -9.7621e-02, -9.6331e-02, -9.1521e-03, -6.1256e-02],\n",
      "          [-1.9292e-02, -4.6174e-02,  3.7530e-02,  4.0688e-02,  7.3407e-02]],\n",
      "\n",
      "         [[-5.2517e-02, -1.2575e-01, -5.8170e-02,  3.1972e-02,  4.3261e-02],\n",
      "          [-5.0753e-02,  2.9035e-03,  1.4643e-02,  1.6474e-02,  1.3224e-02],\n",
      "          [-9.2759e-02,  8.3795e-03,  2.0928e-02, -3.5618e-02,  5.5207e-02],\n",
      "          [-1.3884e-01, -1.6769e-01, -8.0139e-02, -3.0271e-02, -1.1038e-01],\n",
      "          [-1.6799e-01, -5.4328e-02, -4.2942e-02, -6.0321e-02, -1.5705e-01]],\n",
      "\n",
      "         [[ 4.4278e-02,  5.6192e-03, -5.5013e-02, -1.1505e-01, -3.6630e-02],\n",
      "          [ 3.0948e-01,  2.2548e-01,  3.5660e-01,  3.9824e-01,  2.4356e-01],\n",
      "          [ 8.7722e-02, -1.0177e-02,  8.0121e-02,  6.5738e-02,  2.1926e-02],\n",
      "          [-2.2236e-01, -2.9637e-01, -2.6920e-01, -2.0660e-01, -1.3921e-01],\n",
      "          [-1.4026e-01, -1.5442e-01, -2.9334e-01, -3.1195e-01, -1.9282e-01]],\n",
      "\n",
      "         [[ 7.0530e-02,  9.6821e-02,  1.0873e-01, -4.0478e-02,  4.6123e-02],\n",
      "          [ 5.3673e-02,  1.3137e-01,  1.1081e-01,  1.7791e-01,  1.0487e-01],\n",
      "          [-1.7468e-01, -1.2890e-01, -6.5499e-02, -5.6753e-02, -1.4587e-02],\n",
      "          [-1.0570e-01, -2.3589e-01, -2.2873e-01, -1.3069e-01, -1.7339e-01],\n",
      "          [-1.8038e-01, -1.2338e-01, -2.0538e-01, -8.7096e-02, -4.7436e-02]],\n",
      "\n",
      "         [[-2.2759e-02,  1.3298e-01,  6.6956e-02,  7.7004e-02,  1.2636e-01],\n",
      "          [-2.5967e-02,  7.2047e-02,  5.9233e-02, -9.1827e-03,  8.3660e-02],\n",
      "          [-1.4415e-01, -2.2433e-02, -1.1835e-01, -7.8290e-02, -1.0627e-01],\n",
      "          [-6.3782e-02,  4.4102e-02, -6.4836e-02, -2.8717e-02,  2.6086e-02],\n",
      "          [-1.2415e-02,  5.0108e-04, -7.3341e-02, -4.5722e-02,  7.9407e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.9558e-02, -1.2394e-02,  8.1160e-02, -3.2398e-02, -7.3752e-02],\n",
      "          [ 3.4489e-02,  1.9836e-02,  6.4589e-02, -1.8269e-03, -2.2696e-02],\n",
      "          [-2.2022e-02,  4.3722e-02,  5.2979e-02, -7.7728e-02,  1.7828e-02],\n",
      "          [ 5.0664e-02, -2.1919e-02, -7.8805e-02, -2.5426e-02,  2.4977e-02],\n",
      "          [-3.1019e-03, -1.3083e-02,  2.3912e-02, -8.0753e-02,  4.6624e-02]],\n",
      "\n",
      "         [[-6.5857e-02, -1.0133e-01, -9.7980e-03,  1.3530e-02,  1.5226e-01],\n",
      "          [ 6.8579e-02, -2.6976e-02,  2.5654e-03, -1.0816e-02,  9.0590e-02],\n",
      "          [ 9.5681e-02,  1.3491e-01,  1.9035e-01,  2.7003e-02,  1.5727e-01],\n",
      "          [-1.0435e-01,  1.7967e-01,  1.4832e-01,  1.6453e-01,  6.0564e-02],\n",
      "          [-1.6415e-01,  8.5340e-02,  1.6555e-01,  8.3116e-02,  9.1681e-03]],\n",
      "\n",
      "         [[ 1.2728e-01,  1.4148e-01,  9.5863e-02,  9.4394e-02, -1.1012e-02],\n",
      "          [ 6.6515e-02,  5.2788e-02,  1.3822e-01,  1.0425e-02,  3.7695e-02],\n",
      "          [ 1.1120e-01, -3.2616e-02, -1.2303e-01, -5.4072e-04, -7.4345e-02],\n",
      "          [-1.3863e-02,  1.9529e-02,  2.5469e-02, -1.0361e-01, -1.5409e-02],\n",
      "          [ 4.5023e-02,  7.6581e-02,  3.7438e-02,  2.4980e-02, -9.5141e-02]],\n",
      "\n",
      "         [[-7.8907e-02, -2.6248e-02,  6.7230e-02, -6.5307e-02, -1.9127e-02],\n",
      "          [-9.5313e-02, -9.6391e-02, -1.8736e-01, -5.5775e-02, -6.0457e-02],\n",
      "          [-1.5905e-01, -8.7537e-02, -1.1724e-01, -1.2060e-01,  2.8881e-02],\n",
      "          [-1.3895e-01, -1.4415e-01, -8.9619e-02, -1.2631e-01, -5.3221e-02],\n",
      "          [-1.7259e-01, -1.8798e-01,  3.8102e-02,  4.1228e-02, -6.1273e-02]],\n",
      "\n",
      "         [[ 3.5280e-01,  1.9583e-01,  1.3919e-01,  1.4409e-01,  2.3876e-01],\n",
      "          [ 1.5863e-01, -8.5458e-02, -4.4657e-02, -7.7147e-02,  1.1552e-02],\n",
      "          [ 1.9798e-01,  1.2215e-01, -5.5355e-02, -7.7655e-02, -1.0725e-01],\n",
      "          [ 9.0122e-02,  1.7108e-01,  1.8432e-01,  2.9007e-02, -1.8090e-01],\n",
      "          [ 1.4709e-01,  2.4560e-01,  9.8455e-02,  1.1676e-01,  6.6525e-02]],\n",
      "\n",
      "         [[-2.6501e-01, -1.3731e-01, -1.2281e-01, -1.6442e-01,  2.0792e-02],\n",
      "          [-8.1978e-02, -5.6513e-02, -7.7858e-02,  3.6269e-02,  1.1943e-01],\n",
      "          [-1.5106e-01, -2.1501e-01, -1.6089e-01, -6.0911e-02, -8.2374e-02],\n",
      "          [-2.2548e-01, -1.4042e-01, -4.8080e-02, -1.0930e-01, -1.9533e-01],\n",
      "          [-2.4507e-02, -1.0049e-01, -2.9513e-02, -1.0421e-01,  4.4913e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.7142e-02,  3.6264e-02, -4.0926e-02, -7.4388e-02, -6.1942e-02],\n",
      "          [ 7.6679e-02, -7.6850e-02,  6.4233e-02,  2.5348e-02, -3.6448e-02],\n",
      "          [-1.1678e-02, -5.4603e-02, -2.0892e-02, -7.6859e-02, -4.0652e-02],\n",
      "          [-4.7502e-02,  1.5997e-02,  3.8165e-02, -4.6233e-02,  2.4422e-02],\n",
      "          [ 7.5707e-02,  9.1108e-03,  6.6697e-03,  3.6734e-02, -7.0240e-02]],\n",
      "\n",
      "         [[-6.2053e-02, -2.4780e-02,  4.6063e-02,  4.3457e-02, -7.4215e-03],\n",
      "          [-3.1827e-01, -1.7678e-01,  3.6568e-01,  2.3277e-01,  1.0780e-01],\n",
      "          [-3.2636e-01, -3.0822e-01,  3.0456e-01,  3.5328e-01, -4.7918e-02],\n",
      "          [-1.3336e-01, -1.0992e-01,  1.6668e-01,  1.3437e-01, -8.8624e-02],\n",
      "          [ 8.6397e-03,  7.8931e-02, -5.7293e-03,  2.9058e-02,  6.1678e-03]],\n",
      "\n",
      "         [[ 2.8209e-02, -9.6763e-02, -9.5512e-03,  5.8043e-02,  8.4442e-02],\n",
      "          [-4.8046e-02, -5.1866e-02, -4.5094e-02, -4.7556e-02,  1.7152e-02],\n",
      "          [ 6.4256e-03,  1.1812e-01,  2.2227e-02,  2.0320e-02, -1.7300e-02],\n",
      "          [-3.3172e-02,  7.3041e-02,  8.4419e-04, -2.0975e-02,  1.5039e-01],\n",
      "          [-2.7511e-02, -2.5720e-02,  2.0226e-02,  9.0448e-02,  1.0899e-01]],\n",
      "\n",
      "         [[ 2.6103e-01,  2.5202e-01, -1.2410e-01, -1.4911e-01,  5.1779e-02],\n",
      "          [ 1.2458e-01,  2.8568e-02, -1.6502e-01, -9.4631e-02, -2.1894e-02],\n",
      "          [ 4.4126e-02, -9.6189e-02, -6.0614e-02, -1.1796e-02, -1.3892e-01],\n",
      "          [-1.5581e-01, -1.8686e-01,  4.1465e-02, -5.4476e-03, -1.0272e-01],\n",
      "          [-1.5945e-02, -8.2176e-02, -3.2728e-02, -3.7331e-02, -4.6986e-02]],\n",
      "\n",
      "         [[-3.8509e-01, -3.0311e-01,  1.2997e-01,  2.1641e-01,  8.5604e-02],\n",
      "          [-4.8584e-01, -3.5777e-01,  3.0914e-01,  3.6281e-01,  2.9098e-01],\n",
      "          [-4.2933e-01, -4.0079e-01,  2.2289e-01,  4.5107e-01,  5.1953e-02],\n",
      "          [-1.5009e-01, -3.3101e-01,  1.8283e-01,  3.3821e-01,  3.1315e-02],\n",
      "          [-9.8536e-02, -7.0742e-02,  1.0336e-01,  2.5969e-02,  6.4292e-02]],\n",
      "\n",
      "         [[-2.1301e-01,  7.2205e-03,  1.6206e-01,  2.6783e-01,  1.4300e-01],\n",
      "          [-3.7945e-01, -2.1691e-01,  3.5337e-01,  2.6211e-01,  4.2959e-03],\n",
      "          [-4.4211e-01, -1.9529e-01,  3.4261e-01,  1.6921e-01, -1.3968e-01],\n",
      "          [-2.3069e-01, -3.9729e-03,  2.3991e-01, -5.2279e-02, -1.1290e-01],\n",
      "          [-1.2908e-01,  6.9565e-02,  9.6390e-02, -2.0533e-01, -2.3880e-02]]]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1744, -0.0656, -0.0574, -0.0379, -0.0125, -0.1800, -0.0984,  0.0371,\n",
      "         0.2238, -0.0701,  0.0760,  0.2177,  0.0310, -0.0793, -0.2194,  0.2002],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0291,  0.0047,  0.0060,  ...,  0.0005,  0.0449, -0.0120],\n",
      "        [-0.0204,  0.0404, -0.0421,  ...,  0.0204,  0.0111,  0.0926],\n",
      "        [ 0.0432,  0.0834, -0.0478,  ..., -0.2411, -0.0343,  0.0174],\n",
      "        ...,\n",
      "        [-0.0508, -0.0904, -0.0199,  ..., -0.0728, -0.0326, -0.0177],\n",
      "        [ 0.0450, -0.0446, -0.0312,  ...,  0.0761,  0.0456,  0.0079],\n",
      "        [ 0.0031, -0.0074, -0.0267,  ...,  0.0122, -0.0626, -0.2073]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 9.6675e-02,  6.2850e-02, -1.3971e-02,  4.1043e-02, -5.3302e-02,\n",
      "         6.4392e-03,  4.1524e-02,  1.3845e-02, -4.0126e-02, -4.0562e-02,\n",
      "        -3.6324e-02,  9.7119e-03, -2.3365e-02,  4.2315e-02, -9.4112e-03,\n",
      "        -9.7324e-02,  1.0049e-01,  1.4566e-01, -3.9631e-02,  2.8139e-02,\n",
      "        -7.1978e-02, -5.6305e-02, -2.2588e-04,  7.2210e-02, -2.0015e-02,\n",
      "         1.1666e-02,  8.1072e-02,  1.3594e-01,  3.8169e-02,  8.3205e-02,\n",
      "         7.8640e-02,  7.5871e-02,  1.5516e-01, -9.6871e-02, -9.2303e-02,\n",
      "        -4.6376e-02,  3.9941e-05,  8.6208e-04, -3.9645e-02, -5.1879e-02,\n",
      "         3.2688e-02,  4.0649e-02,  1.8820e-02,  6.3032e-02,  2.4399e-01,\n",
      "        -4.2947e-03,  4.1745e-03, -3.9948e-02,  1.5350e-03, -2.8814e-02,\n",
      "         9.1309e-02,  1.4757e-02, -7.3304e-03, -1.0247e-02, -4.3895e-03,\n",
      "        -2.0969e-02,  3.7151e-02,  1.5839e-01, -3.3969e-02,  1.1960e-01,\n",
      "        -1.3585e-02, -4.3430e-02,  7.1769e-02, -9.0174e-03,  9.1062e-03,\n",
      "        -1.0979e-01, -7.9617e-02,  3.3154e-02,  6.0108e-02,  4.3544e-02,\n",
      "         9.0709e-03,  3.0509e-02, -4.2115e-02, -1.1618e-03,  1.7175e-02,\n",
      "         5.8562e-02,  6.2747e-03,  1.1404e-02, -3.7544e-02, -2.6123e-02,\n",
      "        -1.7512e-01,  2.4046e-02, -9.6594e-03, -2.7941e-02, -5.8913e-02,\n",
      "         1.1246e-01, -1.9570e-02, -1.3552e-01, -4.5702e-02,  6.4925e-02,\n",
      "         5.8143e-02, -1.8660e-02, -2.2780e-03,  8.5796e-02, -2.9954e-02,\n",
      "        -1.8614e-03, -1.4886e-01,  3.2842e-02,  2.2046e-02, -3.5308e-02,\n",
      "        -1.0545e-01,  2.4452e-02,  1.2314e-01, -9.9721e-02,  4.4854e-02,\n",
      "        -8.3748e-02,  2.3112e-02,  2.9482e-02, -3.0053e-02,  4.2636e-02,\n",
      "        -2.9523e-02,  1.2573e-01,  3.2751e-02, -3.7846e-02, -4.1684e-02,\n",
      "        -5.3966e-03, -4.3841e-02,  5.2382e-02,  8.7418e-02,  2.9700e-02],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0485,  0.0300, -0.0445,  ..., -0.0154,  0.1084,  0.1432],\n",
      "        [-0.1387,  0.0249,  0.0209,  ..., -0.0756,  0.1312,  0.0939],\n",
      "        [-0.0909,  0.0208,  0.0700,  ...,  0.1636, -0.1035,  0.0203],\n",
      "        ...,\n",
      "        [-0.0621,  0.0429,  0.0317,  ...,  0.0724, -0.0721,  0.0972],\n",
      "        [-0.0644, -0.0243, -0.1208,  ..., -0.0575,  0.1816,  0.0036],\n",
      "        [ 0.1177,  0.0576, -0.0747,  ..., -0.0280, -0.0388, -0.0204]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0576, -0.2078, -0.0455, -0.0713,  0.0577, -0.0453, -0.1544,  0.0511,\n",
      "         0.0962, -0.0137,  0.0061, -0.0118,  0.0236,  0.1100,  0.0269, -0.0900,\n",
      "        -0.0367,  0.0463, -0.0093, -0.0464, -0.0562, -0.0305, -0.1097, -0.0689,\n",
      "        -0.0522, -0.0911,  0.0226, -0.0442, -0.0288,  0.0490,  0.0977, -0.0377,\n",
      "        -0.0078,  0.0567,  0.0485,  0.0041,  0.0193, -0.2041, -0.1000,  0.0428,\n",
      "        -0.0788,  0.1091,  0.0110,  0.1348, -0.0716,  0.0372,  0.3274,  0.2990,\n",
      "         0.1282,  0.2248, -0.0571,  0.0362, -0.0346,  0.0016,  0.3861, -0.0274,\n",
      "         0.1290,  0.2448, -0.0452,  0.0906,  0.1904, -0.0496,  0.0356, -0.0107,\n",
      "        -0.0384, -0.0805,  0.2138, -0.0609, -0.1674, -0.0167, -0.1374,  0.1724,\n",
      "        -0.0546, -0.0329,  0.0336,  0.1077, -0.0282, -0.0771, -0.1703,  0.0819,\n",
      "         0.0601, -0.0228,  0.1267,  0.1794], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.0142e-01,  1.7931e-01,  2.4070e-01, -1.7884e-01,  3.2557e-03,\n",
      "         -1.0275e-01,  7.8051e-02, -4.0267e-01, -1.7454e-01, -2.6513e-01,\n",
      "          1.2021e-01,  1.0767e-01,  8.4903e-02, -9.7506e-02,  2.8343e-01,\n",
      "          3.3084e-02, -9.9435e-02, -2.3404e-03,  1.9951e-01, -1.8238e-01,\n",
      "         -1.9782e-03,  1.3216e-02,  3.1305e-02,  2.0167e-01,  2.4051e-01,\n",
      "         -1.0053e-02,  3.3984e-01, -3.9907e-02,  3.9402e-02, -5.9012e-02,\n",
      "         -9.7568e-02,  1.7984e-01, -8.5400e-02,  2.1603e-01, -8.4614e-02,\n",
      "          2.0889e-01,  1.6923e-01, -1.2879e-01,  9.3384e-02, -2.5544e-03,\n",
      "          3.4156e-01,  2.3115e-01, -5.4163e-02, -2.9249e-01,  2.0998e-01,\n",
      "          8.8897e-02, -2.6955e-01, -1.0558e-01, -9.7532e-02,  1.1277e-01,\n",
      "         -8.5425e-02,  1.8055e-02, -4.6122e-02,  1.1487e-01, -3.9297e-01,\n",
      "          1.8756e-01, -2.1431e-01,  9.2348e-02,  1.3422e-02,  6.3380e-02,\n",
      "         -2.7654e-01,  1.8481e-01, -4.9778e-02,  2.0255e-01, -2.4939e-01,\n",
      "         -1.5718e-01, -1.7323e-01,  8.1060e-02,  4.1864e-01,  2.3862e-01,\n",
      "         -5.8213e-02,  2.0343e-01, -1.1851e-01,  2.7647e-01, -2.9710e-01,\n",
      "          5.3783e-02,  2.4011e-01, -2.6515e-02, -1.4066e-02,  1.4945e-02,\n",
      "         -5.5714e-02,  5.8222e-02,  3.6757e-01, -2.1084e-01],\n",
      "        [-4.1557e-01,  3.3021e-01,  2.1593e-01, -5.3798e-02,  1.9918e-01,\n",
      "         -1.2310e-01, -8.6960e-02,  9.4246e-02,  2.1596e-01, -4.3517e-02,\n",
      "         -2.2613e-01,  2.3957e-01, -5.5364e-02, -4.7397e-01,  7.9467e-02,\n",
      "          6.0653e-02,  2.1043e-02,  1.1513e-01, -8.0808e-02,  2.7286e-01,\n",
      "          1.9513e-01,  1.8509e-02, -3.6642e-02,  1.6438e-01,  9.7735e-02,\n",
      "          1.1985e-02, -2.0959e-01,  2.2688e-01, -6.3656e-02, -1.3507e-01,\n",
      "         -3.2950e-02,  3.5510e-02,  1.8374e-01, -1.9183e-01, -9.9058e-02,\n",
      "         -3.3381e-02, -2.0039e-01,  1.8110e-01,  3.4134e-01,  3.6077e-02,\n",
      "          2.6535e-01, -9.9912e-02, -3.8197e-01, -7.1095e-02,  4.1488e-02,\n",
      "          5.5899e-02, -1.8062e-01,  9.4457e-02,  1.0589e-01, -2.8619e-01,\n",
      "         -6.7971e-03, -8.8246e-02, -3.2362e-01, -2.6616e-01, -5.9411e-01,\n",
      "         -3.2937e-01,  2.8548e-01, -1.0876e-01,  2.5491e-01,  8.0044e-02,\n",
      "          3.1368e-03,  1.6935e-01,  1.1282e-01, -1.3035e-01,  2.7462e-01,\n",
      "         -7.6068e-03,  2.7310e-02, -3.8872e-02,  2.9902e-02, -3.2360e-01,\n",
      "         -2.7386e-01, -3.8281e-02,  2.4851e-01,  3.5450e-02,  1.8770e-01,\n",
      "          1.5066e-01, -1.7022e-01, -7.9282e-02,  1.9441e-01, -1.7816e-02,\n",
      "          5.8798e-02, -1.4296e-01,  7.8867e-02,  3.6834e-01],\n",
      "        [-8.2004e-02, -3.1454e-01,  1.7876e-02, -1.2313e-01, -7.2936e-02,\n",
      "          1.5445e-01,  1.7916e-01,  6.8740e-02, -1.0554e-01,  2.1078e-01,\n",
      "         -2.6909e-01,  1.5734e-01,  6.7650e-02,  2.2052e-01,  6.2114e-02,\n",
      "          8.8136e-02,  1.1078e-01, -1.6165e-01,  3.5855e-01,  7.0034e-02,\n",
      "          1.1616e-02,  6.4197e-02, -1.8592e-01, -4.2655e-01,  2.0693e-01,\n",
      "          4.6652e-02,  8.5117e-02,  1.2710e-01,  1.7710e-01, -1.2704e-01,\n",
      "         -2.5394e-01,  9.3106e-02, -2.8452e-01,  1.6457e-01, -2.7966e-02,\n",
      "         -1.8156e-03,  1.6951e-01, -1.2778e-01, -2.6449e-01,  2.7822e-01,\n",
      "         -1.2167e-01,  2.3795e-01, -1.5812e-01,  5.4358e-02,  2.8621e-01,\n",
      "          1.1184e-02,  1.3873e-01, -2.7551e-02,  1.3679e-01,  1.4419e-01,\n",
      "         -9.8615e-02,  3.8716e-02,  2.4596e-01,  2.2058e-01,  1.3936e-01,\n",
      "          7.2765e-03,  3.3108e-01,  1.4437e-01,  3.3476e-01,  4.5554e-02,\n",
      "         -2.7032e-01,  7.6051e-02, -2.2709e-01,  8.4899e-02, -5.4443e-03,\n",
      "         -2.6217e-01,  2.9637e-01,  2.1007e-01, -1.0428e-01,  3.9313e-01,\n",
      "          8.2703e-03, -3.3016e-01,  1.1628e-01,  1.8284e-01,  2.9515e-01,\n",
      "          3.8868e-01,  4.4490e-01,  1.2896e-01,  2.3319e-01, -1.2022e-01,\n",
      "          2.7336e-01, -1.0483e-01, -1.7238e-02,  3.9940e-02],\n",
      "        [ 1.7216e-01,  1.0332e-01,  3.0653e-02, -6.3803e-02, -3.4331e-02,\n",
      "          8.9088e-02, -2.2802e-01, -9.5181e-02, -1.1101e-01,  1.8327e-01,\n",
      "         -4.1013e-02, -1.4170e-02,  3.0112e-02, -1.2298e-01, -2.5054e-01,\n",
      "         -6.6088e-02, -2.9270e-02, -8.7797e-02, -2.2172e-01, -1.0494e-01,\n",
      "         -1.4974e-01, -2.0019e-01, -1.0968e-01,  1.4084e-01, -1.7587e-01,\n",
      "          6.2546e-02, -9.0528e-02,  6.1275e-02, -2.2269e-01,  1.0342e-01,\n",
      "          1.2271e-01, -5.2563e-02,  2.8303e-01,  4.8859e-02,  2.0237e-01,\n",
      "         -7.2789e-02,  5.7209e-02, -4.7787e-02, -3.3001e-01,  3.9545e-02,\n",
      "         -9.4099e-02,  1.6055e-01,  9.2089e-03,  1.2437e-01, -1.4337e-01,\n",
      "         -2.7319e-01,  2.8973e-01, -4.3745e-02, -2.0466e-01,  2.3948e-01,\n",
      "         -5.9710e-02,  2.4938e-01,  1.3614e-01,  1.2215e-01,  2.5227e-01,\n",
      "         -9.3524e-02,  1.4541e-01, -8.6727e-02, -1.5395e-01,  1.7543e-01,\n",
      "          9.1062e-04,  4.6316e-02,  1.7773e-01, -1.5707e-01, -7.6189e-03,\n",
      "          2.1081e-02,  1.2815e-01, -4.2019e-01, -1.4382e-01,  2.5792e-01,\n",
      "         -8.1622e-02, -2.4731e-01,  7.8014e-02,  1.4067e-01,  1.4784e-01,\n",
      "         -1.6184e-01, -1.2225e-02, -1.9228e-01, -8.3590e-03,  2.4983e-02,\n",
      "         -1.0707e-01, -1.0930e-01,  1.6630e-01, -1.4045e-01],\n",
      "        [ 2.9013e-01, -2.0387e-01, -2.0007e-01,  2.6207e-01, -6.1469e-02,\n",
      "          3.2083e-02,  5.0900e-02,  1.5335e-01,  2.8180e-02, -3.6706e-01,\n",
      "         -2.4156e-02, -2.7481e-02, -2.4596e-02,  8.3905e-02,  4.7898e-02,\n",
      "          7.3301e-02, -4.4464e-02,  2.1950e-01, -2.7836e-01, -2.4165e-01,\n",
      "         -9.8020e-03, -9.2705e-02, -2.4160e-01, -8.2655e-02, -1.5165e-01,\n",
      "         -8.3328e-02,  2.1019e-01,  1.7583e-01,  1.0008e-02,  2.4047e-01,\n",
      "         -2.8079e-01,  7.2385e-02, -1.7751e-01, -2.2637e-01, -8.8655e-02,\n",
      "         -1.9953e-01,  2.9942e-02, -1.5117e-01,  1.5969e-01,  7.5407e-02,\n",
      "          3.0651e-01,  4.8234e-02, -7.6905e-02,  2.2324e-02, -5.6369e-02,\n",
      "         -1.4528e-01,  2.9272e-01,  2.4303e-01, -6.5332e-02,  2.7963e-01,\n",
      "         -5.1865e-02, -4.1169e-02, -2.9746e-02,  3.2737e-01,  2.7904e-01,\n",
      "          5.1909e-03, -2.4305e-01,  4.0202e-01, -1.3712e-01, -1.4856e-01,\n",
      "          1.3047e-01, -3.0637e-01, -2.6749e-02, -2.5069e-01,  9.9861e-02,\n",
      "         -3.4150e-01,  9.1885e-02, -9.8017e-02,  1.4708e-01, -2.0984e-01,\n",
      "         -2.5000e-01,  1.6546e-01, -9.4169e-02, -3.4965e-02, -3.9731e-02,\n",
      "         -1.7039e-01, -2.4999e-02, -1.0554e-01, -2.1301e-01, -1.9652e-02,\n",
      "          1.9024e-01, -2.4173e-01,  2.4023e-02,  1.6546e-01],\n",
      "        [ 1.6605e-01,  2.8211e-01, -1.2173e-01,  1.4315e-01,  1.1881e-01,\n",
      "          3.4432e-02,  2.4812e-01, -1.0544e-01, -2.4472e-01,  5.7533e-03,\n",
      "         -9.4832e-02, -3.1498e-01,  6.2485e-02, -1.4541e-01, -1.9675e-01,\n",
      "          1.0899e-01, -1.1726e-01,  8.8990e-02,  8.7047e-02,  5.6530e-02,\n",
      "          7.5208e-02, -5.0750e-02,  1.3722e-01,  1.0911e-02,  3.1830e-02,\n",
      "         -4.9810e-02, -1.9987e-01, -2.2526e-01,  8.8992e-03,  1.8010e-01,\n",
      "         -1.1472e-01,  1.0704e-01,  6.4342e-02, -2.3995e-01, -5.5203e-02,\n",
      "          2.4534e-01, -2.9322e-01,  6.5003e-02, -1.1651e-01, -1.6198e-01,\n",
      "         -2.2004e-01,  1.3916e-01,  1.4370e-01,  1.0307e-01,  1.6237e-01,\n",
      "         -1.8477e-01, -5.9520e-02, -2.1756e-01, -3.0205e-01,  6.2422e-02,\n",
      "         -3.3794e-02, -1.4461e-01, -9.1343e-02,  1.5130e-01,  2.3291e-01,\n",
      "         -2.9775e-01, -7.8681e-02, -7.3218e-02, -1.9200e-01, -1.4920e-01,\n",
      "         -6.1958e-02,  1.4284e-01,  1.5436e-01,  6.3744e-02,  1.6224e-01,\n",
      "          2.1279e-01,  1.8113e-01, -8.4911e-02, -2.2025e-01,  1.4260e-02,\n",
      "          3.7157e-01, -1.3028e-01,  3.7733e-03, -1.7197e-01,  1.2521e-01,\n",
      "         -1.2874e-02, -2.5838e-02,  7.8458e-02,  5.0663e-02, -3.2696e-01,\n",
      "         -4.9216e-02, -3.0988e-01,  2.3362e-01, -2.8927e-01],\n",
      "        [ 9.8884e-02, -1.0354e-01, -2.7210e-01, -4.7006e-01,  2.0937e-02,\n",
      "          4.9976e-02, -2.3208e-01, -3.3051e-01, -3.0351e-02,  1.1048e-01,\n",
      "         -1.5064e-02,  9.0300e-02,  6.3333e-02, -1.3495e-01,  1.5213e-01,\n",
      "          1.3562e-01, -3.3121e-02, -1.6602e-01,  1.8510e-01, -1.8748e-01,\n",
      "          6.4450e-02,  9.6846e-03,  1.6481e-01, -5.3129e-02,  3.4143e-01,\n",
      "          1.1972e-02, -1.0478e-01,  1.3175e-01, -4.8331e-02, -2.3486e-01,\n",
      "         -8.4866e-02, -6.2893e-02,  2.1176e-02,  5.9265e-02, -7.7185e-02,\n",
      "          1.3644e-01, -1.5183e-01, -4.1910e-01,  1.3849e-01, -1.0762e-01,\n",
      "         -3.4798e-01, -7.9217e-02,  3.6512e-01,  1.8921e-02, -2.3128e-01,\n",
      "          1.9911e-01,  3.1937e-01,  3.9926e-01,  1.6393e-01,  3.0400e-01,\n",
      "          3.0109e-02, -1.2914e-01, -1.2425e-01, -3.8942e-01,  2.3060e-01,\n",
      "          1.0912e-01,  3.4272e-01,  3.3021e-01,  1.3436e-01,  5.4275e-02,\n",
      "          3.2451e-01,  1.6659e-01, -3.4627e-02,  1.5291e-01, -1.9243e-02,\n",
      "         -1.5767e-01, -6.1914e-02,  1.4001e-02, -4.6195e-01, -2.2589e-02,\n",
      "         -1.5828e-01,  1.3806e-01,  2.8893e-02, -2.0948e-01, -2.0222e-01,\n",
      "          9.8791e-02, -1.5038e-02, -1.5482e-02, -3.8233e-01,  4.4224e-02,\n",
      "         -1.2323e-01,  6.5716e-02,  1.3490e-01,  1.9720e-01],\n",
      "        [ 4.1982e-02,  2.8303e-02,  1.6071e-02,  2.2790e-01,  1.8284e-01,\n",
      "         -3.8943e-02,  1.1254e-01,  3.2432e-01, -7.8021e-02,  3.0094e-03,\n",
      "          2.3086e-01, -1.9191e-01,  3.6174e-02,  1.3077e-01, -1.6683e-01,\n",
      "          6.6384e-02, -6.7785e-02, -1.6838e-01, -2.2100e-01, -1.8553e-01,\n",
      "          1.7462e-01,  4.5613e-01,  7.0779e-02, -1.3720e-01, -4.9293e-02,\n",
      "          5.6081e-02, -2.8426e-01, -9.3587e-03,  9.7994e-02,  1.0195e-01,\n",
      "          1.7837e-01,  6.2747e-05, -2.7294e-01, -1.6857e-02,  5.7070e-02,\n",
      "         -3.4516e-02, -1.6006e-01,  1.6511e-01, -1.9893e-01, -2.1094e-01,\n",
      "         -2.5887e-01, -1.9276e-01, -1.2295e-01,  3.2738e-01,  1.2767e-01,\n",
      "         -6.9247e-02, -2.9464e-01, -2.7355e-01,  4.9937e-02,  2.2466e-01,\n",
      "         -2.3925e-02, -1.8627e-01, -1.2854e-01, -1.0418e-01, -1.0999e-02,\n",
      "          1.5393e-01, -2.7261e-01, -1.7424e-01, -1.2671e-01, -5.5907e-02,\n",
      "          2.2743e-01, -2.7461e-01,  3.9615e-01, -3.5652e-01,  4.7882e-02,\n",
      "         -7.5143e-02, -3.3436e-01, -1.2361e-01,  1.5267e-01, -2.3631e-01,\n",
      "         -2.5440e-01,  4.0699e-01,  1.0193e-02, -8.6071e-02,  2.3432e-01,\n",
      "         -8.9525e-03, -2.0258e-02, -1.2603e-01,  4.0313e-02,  1.8619e-02,\n",
      "         -2.5946e-01,  2.9230e-01,  1.0066e-01, -1.1323e-01],\n",
      "        [ 3.2861e-02, -2.2638e-01, -4.5033e-02,  1.8175e-02, -1.6084e-01,\n",
      "          1.3122e-01, -3.9673e-01,  2.4463e-01,  3.1156e-01,  9.8281e-02,\n",
      "          3.3054e-02,  1.3038e-01,  7.0091e-02,  3.9862e-01, -8.5200e-02,\n",
      "         -1.3088e-01,  9.4065e-02,  3.1805e-01,  2.4276e-01,  2.3472e-01,\n",
      "         -6.9840e-02, -3.9605e-01,  2.5903e-01,  1.1571e-01, -3.4175e-01,\n",
      "         -1.6843e-02,  1.9998e-01, -2.3266e-01, -1.8096e-01, -1.7054e-02,\n",
      "          2.2135e-01,  9.3312e-02,  3.3349e-01, -1.5574e-01,  1.6277e-01,\n",
      "          1.9367e-01,  6.3339e-02, -2.5030e-01, -6.4068e-02,  1.1683e-01,\n",
      "          2.0956e-01, -4.8529e-01, -2.5629e-01, -7.9456e-02, -4.2163e-03,\n",
      "          1.7283e-01, -8.2753e-02, -1.1503e-01,  3.7477e-02, -5.0356e-01,\n",
      "         -2.5103e-02, -7.4238e-02, -1.3534e-01,  3.8338e-02, -1.0657e-01,\n",
      "          1.2776e-01,  1.8284e-02, -1.5753e-01, -1.4306e-01,  6.6173e-02,\n",
      "         -7.2767e-02, -1.4334e-03, -3.0685e-01,  3.0136e-01, -5.4928e-02,\n",
      "          1.2738e-01, -2.7731e-01, -1.4574e-02, -1.4206e-02,  7.0835e-03,\n",
      "          2.1632e-01,  5.4027e-02, -3.1950e-01, -1.5952e-02, -3.2787e-01,\n",
      "          5.9627e-02, -3.0779e-01,  1.0728e-01,  1.3377e-01,  6.1795e-02,\n",
      "          1.3094e-01,  1.2901e-01, -4.0926e-01, -1.6777e-01],\n",
      "        [-2.8029e-01,  4.0117e-02,  2.3680e-01,  2.1427e-01, -1.5957e-01,\n",
      "          1.2109e-01,  4.1513e-02, -8.6721e-02, -4.1208e-01,  3.6959e-01,\n",
      "          1.3083e-01,  6.6927e-02,  1.0058e-01, -5.7724e-04,  1.9456e-02,\n",
      "          7.3664e-02, -4.9059e-02, -1.6799e-01, -1.6008e-01, -2.5610e-03,\n",
      "          1.1556e-01,  1.1479e-01, -1.5196e-01,  1.4215e-01, -5.6768e-02,\n",
      "         -1.5085e-02, -2.1778e-01, -1.9284e-01,  2.8035e-01,  1.2307e-01,\n",
      "         -9.6302e-02, -6.6611e-02,  1.3655e-01, -1.0901e-01,  6.5077e-02,\n",
      "         -3.2014e-01,  2.2027e-01,  3.6069e-01,  2.5588e-01, -1.0185e-01,\n",
      "         -7.3538e-02,  4.0532e-02,  1.7071e-01, -4.2105e-01, -2.4719e-01,\n",
      "          1.6351e-01,  6.7511e-02, -9.8232e-02,  5.0175e-03, -2.0487e-01,\n",
      "          8.6555e-02, -2.1007e-03,  4.2108e-01, -5.1112e-02, -1.4213e-01,\n",
      "          2.1301e-01, -2.0699e-01, -1.6744e-01,  1.0123e-01,  3.4264e-02,\n",
      "         -1.8304e-01, -1.2631e-01, -5.8497e-02, -9.2483e-03, -1.4179e-01,\n",
      "          4.2043e-01, -8.7912e-02,  3.0595e-01, -3.7987e-02, -2.3993e-02,\n",
      "         -3.8710e-02, -2.1908e-01, -2.6205e-02, -1.5881e-01, -5.8313e-02,\n",
      "         -3.1911e-01,  5.1658e-02, -7.0539e-03,  1.0154e-01,  5.5293e-02,\n",
      "          1.7081e-02,  9.7068e-02, -4.8202e-01,  1.9742e-01]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4896, -0.0265,  0.0869,  0.2370,  0.3998, -0.2909,  0.3222, -0.0837,\n",
      "        -0.0959, -0.1429], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "net_adv = copy.deepcopy(net_ori) #am I doing correct? \n",
    "for param in net_adv.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5635cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_adv.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = './cifar_base_adv_pgd.pth'\n",
    "device = \"cuda\"\n",
    "writer = SummaryWriter(\"Adv_Base_PGD\")\n",
    "pgd_attack = generate_adv(net_adv, \"pgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08ee62d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100]\t\n",
      "Epoch [1/100]\t Training Loss: 2.606929569293166\t lr: 0.001\n",
      "Epoch [1/100]\t Validation Loss: 1.7744153255148778\t lr: 0.001\n",
      "Epoch [1/100]\t\n",
      "Epoch [2/100]\t Training Loss: 2.206723759241421\t lr: 0.001\n",
      "Epoch [2/100]\t Validation Loss: 1.7443217552160915\t lr: 0.001\n",
      "Epoch [2/100]\t\n",
      "Epoch [3/100]\t Training Loss: 2.1409123800599668\t lr: 0.001\n",
      "Epoch [3/100]\t Validation Loss: 1.7191773788838447\t lr: 0.001\n",
      "Epoch [3/100]\t\n",
      "Epoch [4/100]\t Training Loss: 2.1064195931720002\t lr: 0.001\n",
      "Epoch [4/100]\t Validation Loss: 1.7154925005345405\t lr: 0.001\n",
      "Epoch [4/100]\t\n",
      "Epoch [5/100]\t Training Loss: 2.0841019842630764\t lr: 0.001\n",
      "Epoch [5/100]\t Validation Loss: 1.6828070652635791\t lr: 0.001\n",
      "Epoch [5/100]\t\n",
      "Epoch [6/100]\t Training Loss: 2.064460202556132\t lr: 0.001\n",
      "Epoch [6/100]\t Validation Loss: 1.6758251235454897\t lr: 0.001\n",
      "Epoch [6/100]\t\n",
      "Epoch [7/100]\t Training Loss: 2.051415484579628\t lr: 0.001\n",
      "Epoch [7/100]\t Validation Loss: 1.6651591288892529\t lr: 0.001\n",
      "Epoch [7/100]\t\n",
      "Epoch [8/100]\t Training Loss: 2.0405058561993377\t lr: 0.001\n",
      "Epoch [8/100]\t Validation Loss: 1.65670728381676\t lr: 0.001\n",
      "Epoch [8/100]\t\n",
      "Epoch [9/100]\t Training Loss: 2.032004397848378\t lr: 0.001\n",
      "Epoch [9/100]\t Validation Loss: 1.652435826349862\t lr: 0.001\n",
      "Epoch [9/100]\t\n",
      "Epoch [10/100]\t Training Loss: 2.023149032726922\t lr: 0.001\n",
      "Epoch [10/100]\t Validation Loss: 1.6362305215642423\t lr: 0.001\n",
      "Epoch [10/100]\t\n",
      "Epoch [11/100]\t Training Loss: 2.015605376809454\t lr: 0.001\n",
      "Epoch [11/100]\t Validation Loss: 1.6360998455482194\t lr: 0.001\n",
      "Epoch [11/100]\t\n",
      "Epoch [12/100]\t Training Loss: 2.0089469437708942\t lr: 0.001\n",
      "Epoch [12/100]\t Validation Loss: 1.626774964453299\t lr: 0.001\n",
      "Epoch [12/100]\t\n",
      "Epoch [13/100]\t Training Loss: 2.0047918189212184\t lr: 0.001\n",
      "Epoch [13/100]\t Validation Loss: 1.6194028205509428\t lr: 0.001\n",
      "Epoch [13/100]\t\n",
      "Epoch [14/100]\t Training Loss: 2.000294752742933\t lr: 0.001\n",
      "Epoch [14/100]\t Validation Loss: 1.6208517159087747\t lr: 0.001\n",
      "Epoch [14/100]\t\n",
      "Epoch [15/100]\t Training Loss: 1.9948724639385254\t lr: 0.001\n",
      "Epoch [15/100]\t Validation Loss: 1.6172476674936995\t lr: 0.001\n",
      "Epoch [15/100]\t\n",
      "Epoch [16/100]\t Training Loss: 1.9917841323501313\t lr: 0.001\n",
      "Epoch [16/100]\t Validation Loss: 1.6107451402688329\t lr: 0.001\n",
      "Epoch [16/100]\t\n",
      "Epoch [17/100]\t Training Loss: 1.9870729708610593\t lr: 0.001\n",
      "Epoch [17/100]\t Validation Loss: 1.6145037895516505\t lr: 0.001\n",
      "Epoch [17/100]\t\n",
      "Epoch [18/100]\t Training Loss: 1.9828084999947901\t lr: 0.001\n",
      "Epoch [18/100]\t Validation Loss: 1.6094565135014207\t lr: 0.001\n",
      "Epoch [18/100]\t\n",
      "Epoch [19/100]\t Training Loss: 1.9803142373824059\t lr: 0.001\n",
      "Epoch [19/100]\t Validation Loss: 1.595685800419578\t lr: 0.001\n",
      "Epoch [19/100]\t\n",
      "Epoch [20/100]\t Training Loss: 1.9775092117012005\t lr: 0.001\n",
      "Epoch [20/100]\t Validation Loss: 1.5969913413253012\t lr: 0.001\n",
      "Epoch [20/100]\t\n",
      "Epoch [21/100]\t Training Loss: 1.974331073748791\t lr: 0.001\n",
      "Epoch [21/100]\t Validation Loss: 1.592625977117804\t lr: 0.001\n",
      "Epoch [21/100]\t\n",
      "Epoch [22/100]\t Training Loss: 1.9717688151942494\t lr: 0.001\n",
      "Epoch [22/100]\t Validation Loss: 1.5912582074539572\t lr: 0.001\n",
      "Epoch [22/100]\t\n",
      "Epoch [23/100]\t Training Loss: 1.9678959431855574\t lr: 0.001\n",
      "Epoch [23/100]\t Validation Loss: 1.599737265441991\t lr: 0.001\n",
      "Epoch [23/100]\t\n",
      "Epoch [24/100]\t Training Loss: 1.966620647998722\t lr: 0.001\n",
      "Epoch [24/100]\t Validation Loss: 1.5924412992936146\t lr: 0.001\n",
      "Epoch [24/100]\t\n",
      "Epoch [25/100]\t Training Loss: 1.9632516748764937\t lr: 0.001\n",
      "Epoch [25/100]\t Validation Loss: 1.5823977431164513\t lr: 0.001\n",
      "Epoch [25/100]\t\n",
      "Epoch [26/100]\t Training Loss: 1.9607912805074317\t lr: 0.001\n",
      "Epoch [26/100]\t Validation Loss: 1.5818234214299842\t lr: 0.001\n",
      "Epoch [26/100]\t\n",
      "Epoch [27/100]\t Training Loss: 1.959172949766564\t lr: 0.001\n",
      "Epoch [27/100]\t Validation Loss: 1.5862742224826087\t lr: 0.001\n",
      "Epoch [27/100]\t\n",
      "Epoch [28/100]\t Training Loss: 1.9581904307655666\t lr: 0.001\n",
      "Epoch [28/100]\t Validation Loss: 1.5688488573967656\t lr: 0.001\n",
      "Epoch [28/100]\t\n",
      "Epoch [29/100]\t Training Loss: 1.954648684357743\t lr: 0.001\n",
      "Epoch [29/100]\t Validation Loss: 1.5771539618697348\t lr: 0.001\n",
      "Epoch [29/100]\t\n",
      "Epoch [30/100]\t Training Loss: 1.9535684798989454\t lr: 0.001\n",
      "Epoch [30/100]\t Validation Loss: 1.5849783737448198\t lr: 0.001\n",
      "Epoch [30/100]\t\n",
      "Epoch [31/100]\t Training Loss: 1.951906432885953\t lr: 0.001\n",
      "Epoch [31/100]\t Validation Loss: 1.575441063204898\t lr: 0.001\n",
      "Epoch [31/100]\t\n",
      "Epoch [32/100]\t Training Loss: 1.9490829547652808\t lr: 0.001\n",
      "Epoch [32/100]\t Validation Loss: 1.5845197967336149\t lr: 0.001\n",
      "Epoch [32/100]\t\n",
      "Epoch [33/100]\t Training Loss: 1.949330937527025\t lr: 0.001\n",
      "Epoch [33/100]\t Validation Loss: 1.564876299870165\t lr: 0.001\n",
      "Epoch [33/100]\t\n",
      "Epoch [34/100]\t Training Loss: 1.945995843319027\t lr: 0.001\n",
      "Epoch [34/100]\t Validation Loss: 1.555452494681636\t lr: 0.001\n",
      "Epoch [34/100]\t\n",
      "Epoch [35/100]\t Training Loss: 1.9445544155052557\t lr: 0.001\n",
      "Epoch [35/100]\t Validation Loss: 1.5717481736895405\t lr: 0.001\n",
      "Epoch [35/100]\t\n",
      "Epoch [36/100]\t Training Loss: 1.9445874806865098\t lr: 0.001\n",
      "Epoch [36/100]\t Validation Loss: 1.5765041384515883\t lr: 0.001\n",
      "Epoch [36/100]\t\n",
      "Epoch [37/100]\t Training Loss: 1.9415747848008296\t lr: 0.001\n",
      "Epoch [37/100]\t Validation Loss: 1.5673554652853856\t lr: 0.001\n",
      "Epoch [37/100]\t\n",
      "Epoch [38/100]\t Training Loss: 1.9410479132781553\t lr: 0.001\n",
      "Epoch [38/100]\t Validation Loss: 1.568156556238102\t lr: 0.001\n",
      "Epoch [38/100]\t\n",
      "Epoch [39/100]\t Training Loss: 1.9386727645269135\t lr: 0.001\n",
      "Epoch [39/100]\t Validation Loss: 1.5622603259509122\t lr: 0.001\n",
      "Epoch [39/100]\t\n",
      "Epoch [40/100]\t Training Loss: 1.93656103293914\t lr: 0.001\n",
      "Epoch [40/100]\t Validation Loss: 1.568108783492559\t lr: 0.001\n",
      "Epoch [40/100]\t\n",
      "Epoch [41/100]\t Training Loss: 1.9350712835941168\t lr: 0.001\n",
      "Epoch [41/100]\t Validation Loss: 1.5623248317573644\t lr: 0.001\n",
      "Epoch [41/100]\t\n",
      "Epoch [42/100]\t Training Loss: 1.933446155179797\t lr: 0.001\n",
      "Epoch [42/100]\t Validation Loss: 1.564915652516522\t lr: 0.001\n",
      "Epoch [42/100]\t\n",
      "Epoch [43/100]\t Training Loss: 1.933275071861189\t lr: 0.001\n",
      "Epoch [43/100]\t Validation Loss: 1.5663872127291523\t lr: 0.001\n",
      "Epoch [43/100]\t\n",
      "Epoch [44/100]\t Training Loss: 1.9310270749089662\t lr: 0.001\n",
      "Epoch [44/100]\t Validation Loss: 1.5570010142990305\t lr: 0.001\n",
      "Epoch [44/100]\t\n",
      "Epoch [45/100]\t Training Loss: 1.9307560896324685\t lr: 0.001\n",
      "Epoch [45/100]\t Validation Loss: 1.553032887132862\t lr: 0.001\n",
      "Epoch [45/100]\t\n",
      "Epoch [46/100]\t Training Loss: 1.9293462641708685\t lr: 0.001\n",
      "Epoch [46/100]\t Validation Loss: 1.5499740310862093\t lr: 0.001\n",
      "Epoch [46/100]\t\n",
      "Epoch [47/100]\t Training Loss: 1.9281726752400703\t lr: 0.001\n",
      "Epoch [47/100]\t Validation Loss: 1.558097852936274\t lr: 0.001\n",
      "Epoch [47/100]\t\n",
      "Epoch [48/100]\t Training Loss: 1.9271558709156789\t lr: 0.001\n",
      "Epoch [48/100]\t Validation Loss: 1.550718518752086\t lr: 0.001\n",
      "Epoch [48/100]\t\n",
      "Epoch [49/100]\t Training Loss: 1.9252390885901878\t lr: 0.001\n",
      "Epoch [49/100]\t Validation Loss: 1.549111671085599\t lr: 0.001\n",
      "Epoch [49/100]\t\n",
      "Epoch [50/100]\t Training Loss: 1.924641143940294\t lr: 0.001\n",
      "Epoch [50/100]\t Validation Loss: 1.554615630379206\t lr: 0.001\n",
      "Epoch [50/100]\t\n",
      "Epoch [51/100]\t Training Loss: 1.9239326236802903\t lr: 0.001\n",
      "Epoch [51/100]\t Validation Loss: 1.5446667429767078\t lr: 0.001\n",
      "Epoch [51/100]\t\n",
      "Epoch [52/100]\t Training Loss: 1.9223462163334917\t lr: 0.001\n",
      "Epoch [52/100]\t Validation Loss: 1.5461593956886968\t lr: 0.001\n",
      "Epoch [52/100]\t\n",
      "Epoch [53/100]\t Training Loss: 1.920254796057406\t lr: 0.001\n",
      "Epoch [53/100]\t Validation Loss: 1.5451641475098044\t lr: 0.001\n",
      "Epoch [53/100]\t\n",
      "Epoch [54/100]\t Training Loss: 1.9198372714659746\t lr: 0.001\n",
      "Epoch [54/100]\t Validation Loss: 1.5464577599416804\t lr: 0.001\n",
      "Epoch [54/100]\t\n",
      "Epoch [55/100]\t Training Loss: 1.9190857273233517\t lr: 0.001\n",
      "Epoch [55/100]\t Validation Loss: 1.5457717859292333\t lr: 0.001\n",
      "Epoch [55/100]\t\n",
      "Epoch [56/100]\t Training Loss: 1.9182002367570883\t lr: 0.001\n",
      "Epoch [56/100]\t Validation Loss: 1.5472587679005876\t lr: 0.001\n",
      "Epoch [56/100]\t\n",
      "Epoch [57/100]\t Training Loss: 1.9166548325277655\t lr: 0.001\n",
      "Epoch [57/100]\t Validation Loss: 1.547961072076725\t lr: 0.001\n",
      "Epoch [57/100]\t\n",
      "Epoch [58/100]\t Training Loss: 1.9167143130851219\t lr: 0.001\n",
      "Epoch [58/100]\t Validation Loss: 1.5363086507290225\t lr: 0.001\n",
      "Epoch [58/100]\t\n",
      "Epoch [59/100]\t Training Loss: 1.915216574278634\t lr: 0.001\n",
      "Epoch [59/100]\t Validation Loss: 1.5467811460736431\t lr: 0.001\n",
      "Epoch [59/100]\t\n",
      "Epoch [60/100]\t Training Loss: 1.9157319370742953\t lr: 0.001\n",
      "Epoch [60/100]\t Validation Loss: 1.5540730485433265\t lr: 0.001\n",
      "Epoch [60/100]\t\n",
      "Epoch [61/100]\t Training Loss: 1.9133579852940785\t lr: 0.001\n",
      "Epoch [61/100]\t Validation Loss: 1.5430533025838151\t lr: 0.001\n",
      "Epoch [61/100]\t\n",
      "Epoch [62/100]\t Training Loss: 1.913272477476798\t lr: 0.001\n",
      "Epoch [62/100]\t Validation Loss: 1.5452463506143304\t lr: 0.001\n",
      "Epoch [62/100]\t\n",
      "Epoch [63/100]\t Training Loss: 1.9126495156446686\t lr: 0.001\n",
      "Epoch [63/100]\t Validation Loss: 1.5415887727013118\t lr: 0.001\n",
      "Epoch [63/100]\t\n",
      "Epoch [64/100]\t Training Loss: 1.9103331315852796\t lr: 0.001\n",
      "Epoch [64/100]\t Validation Loss: 1.5377379701107363\t lr: 0.001\n",
      "Epoch [64/100]\t\n",
      "Epoch [65/100]\t Training Loss: 1.909568135695689\t lr: 0.001\n",
      "Epoch [65/100]\t Validation Loss: 1.5332369095162501\t lr: 0.001\n",
      "Epoch [65/100]\t\n",
      "Epoch [66/100]\t Training Loss: 1.9088246932115092\t lr: 0.001\n",
      "Epoch [66/100]\t Validation Loss: 1.5404686565640606\t lr: 0.001\n",
      "Epoch [66/100]\t\n",
      "Epoch [67/100]\t Training Loss: 1.9091114973473122\t lr: 0.001\n",
      "Epoch [67/100]\t Validation Loss: 1.5412725617613974\t lr: 0.001\n",
      "Epoch [67/100]\t\n",
      "Epoch [68/100]\t Training Loss: 1.9080649907021876\t lr: 0.001\n",
      "Epoch [68/100]\t Validation Loss: 1.5338694340066066\t lr: 0.001\n",
      "Epoch [68/100]\t\n",
      "Epoch [69/100]\t Training Loss: 1.9066032018807844\t lr: 0.001\n",
      "Epoch [69/100]\t Validation Loss: 1.5333608117284654\t lr: 0.001\n",
      "Epoch [69/100]\t\n",
      "Epoch [70/100]\t Training Loss: 1.9055109164294075\t lr: 0.001\n",
      "Epoch [70/100]\t Validation Loss: 1.5276036262512207\t lr: 0.001\n",
      "Epoch [70/100]\t\n",
      "Epoch [71/100]\t Training Loss: 1.9047319965289378\t lr: 0.001\n",
      "Epoch [71/100]\t Validation Loss: 1.5385590550265735\t lr: 0.001\n",
      "Epoch [71/100]\t\n",
      "Epoch [72/100]\t Training Loss: 1.9041520341887803\t lr: 0.001\n",
      "Epoch [72/100]\t Validation Loss: 1.5330934494356565\t lr: 0.001\n",
      "Epoch [72/100]\t\n",
      "Epoch [73/100]\t Training Loss: 1.9035458784274129\t lr: 0.001\n",
      "Epoch [73/100]\t Validation Loss: 1.5321105597894402\t lr: 0.001\n",
      "Epoch [73/100]\t\n",
      "Epoch [74/100]\t Training Loss: 1.902407798315863\t lr: 0.001\n",
      "Epoch [74/100]\t Validation Loss: 1.5265995445130747\t lr: 0.001\n",
      "Epoch [74/100]\t\n",
      "Epoch [75/100]\t Training Loss: 1.902201778443573\t lr: 0.001\n",
      "Epoch [75/100]\t Validation Loss: 1.5435995753807357\t lr: 0.001\n",
      "Epoch [75/100]\t\n",
      "Epoch [76/100]\t Training Loss: 1.9019266251103042\t lr: 0.001\n",
      "Epoch [76/100]\t Validation Loss: 1.5343786010259315\t lr: 0.001\n",
      "Epoch [76/100]\t\n",
      "Epoch [77/100]\t Training Loss: 1.9013654203975903\t lr: 0.001\n",
      "Epoch [77/100]\t Validation Loss: 1.5239946374410316\t lr: 0.001\n",
      "Epoch [77/100]\t\n",
      "Epoch [78/100]\t Training Loss: 1.9001644337573624\t lr: 0.001\n",
      "Epoch [78/100]\t Validation Loss: 1.5264293724977518\t lr: 0.001\n",
      "Epoch [78/100]\t\n",
      "Epoch [79/100]\t Training Loss: 1.8996819102245828\t lr: 0.001\n",
      "Epoch [79/100]\t Validation Loss: 1.526097846936576\t lr: 0.001\n",
      "Epoch [79/100]\t\n",
      "Epoch [80/100]\t Training Loss: 1.898692801480403\t lr: 0.001\n",
      "Epoch [80/100]\t Validation Loss: 1.527681148504909\t lr: 0.001\n",
      "Epoch [80/100]\t\n",
      "Epoch [81/100]\t Training Loss: 1.8980066904326534\t lr: 0.001\n",
      "Epoch [81/100]\t Validation Loss: 1.5286425940598114\t lr: 0.001\n",
      "Epoch [81/100]\t\n",
      "Epoch [82/100]\t Training Loss: 1.8979776356836109\t lr: 0.001\n",
      "Epoch [82/100]\t Validation Loss: 1.526638098909885\t lr: 0.001\n",
      "Epoch [82/100]\t\n",
      "Epoch [83/100]\t Training Loss: 1.8965770067156429\t lr: 0.001\n",
      "Epoch [83/100]\t Validation Loss: 1.5258422139324719\t lr: 0.001\n",
      "Epoch [83/100]\t\n",
      "Epoch [84/100]\t Training Loss: 1.8963015591701888\t lr: 0.001\n",
      "Epoch [84/100]\t Validation Loss: 1.5233715425563763\t lr: 0.001\n",
      "Epoch [84/100]\t\n",
      "Epoch [85/100]\t Training Loss: 1.8953479065004821\t lr: 0.001\n",
      "Epoch [85/100]\t Validation Loss: 1.5163192296329933\t lr: 0.001\n",
      "Epoch [85/100]\t\n",
      "Epoch [86/100]\t Training Loss: 1.8945394489161498\t lr: 0.001\n",
      "Epoch [86/100]\t Validation Loss: 1.5204534696627268\t lr: 0.001\n",
      "Epoch [86/100]\t\n",
      "Epoch [87/100]\t Training Loss: 1.894703553460748\t lr: 0.001\n",
      "Epoch [87/100]\t Validation Loss: 1.5128042215033422\t lr: 0.001\n",
      "Epoch [87/100]\t\n",
      "Epoch [88/100]\t Training Loss: 1.8937720978046622\t lr: 0.001\n",
      "Epoch [88/100]\t Validation Loss: 1.5215560544895221\t lr: 0.001\n",
      "Epoch [88/100]\t\n",
      "Epoch [89/100]\t Training Loss: 1.8924103542362027\t lr: 0.001\n",
      "Epoch [89/100]\t Validation Loss: 1.5199024435840076\t lr: 0.001\n",
      "Epoch [89/100]\t\n",
      "Epoch [90/100]\t Training Loss: 1.8926054205735932\t lr: 0.001\n",
      "Epoch [90/100]\t Validation Loss: 1.5193473281739633\t lr: 0.001\n",
      "Epoch [90/100]\t\n",
      "Epoch [91/100]\t Training Loss: 1.891384202501048\t lr: 0.001\n",
      "Epoch [91/100]\t Validation Loss: 1.517865482764908\t lr: 0.001\n",
      "Epoch [91/100]\t\n",
      "Epoch [92/100]\t Training Loss: 1.890930678228588\t lr: 0.001\n",
      "Epoch [92/100]\t Validation Loss: 1.5146090607099896\t lr: 0.001\n",
      "Epoch [92/100]\t\n",
      "Epoch [93/100]\t Training Loss: 1.8898889853826264\t lr: 0.001\n",
      "Epoch [93/100]\t Validation Loss: 1.5147745654552798\t lr: 0.001\n",
      "Epoch [93/100]\t\n",
      "Epoch [94/100]\t Training Loss: 1.8904971524577616\t lr: 0.001\n",
      "Epoch [94/100]\t Validation Loss: 1.5177083845379986\t lr: 0.001\n",
      "Epoch [94/100]\t\n",
      "Epoch [95/100]\t Training Loss: 1.8897427780853817\t lr: 0.001\n",
      "Epoch [95/100]\t Validation Loss: 1.5129640902144998\t lr: 0.001\n",
      "Epoch [95/100]\t\n",
      "Epoch [96/100]\t Training Loss: 1.8886473602651026\t lr: 0.001\n",
      "Epoch [96/100]\t Validation Loss: 1.5119124288800396\t lr: 0.001\n",
      "Epoch [96/100]\t\n",
      "Epoch [97/100]\t Training Loss: 1.8889404081017769\t lr: 0.001\n",
      "Epoch [97/100]\t Validation Loss: 1.507971976376787\t lr: 0.001\n",
      "Epoch [97/100]\t\n",
      "Epoch [98/100]\t Training Loss: 1.8875454094098962\t lr: 0.001\n",
      "Epoch [98/100]\t Validation Loss: 1.5203443614742425\t lr: 0.001\n",
      "Epoch [98/100]\t\n",
      "Epoch [99/100]\t Training Loss: 1.887702399507508\t lr: 0.001\n",
      "Epoch [99/100]\t Validation Loss: 1.5067665350588062\t lr: 0.001\n",
      "Epoch [99/100]\t\n",
      "Epoch [100/100]\t Training Loss: 1.8865848456502265\t lr: 0.001\n",
      "Epoch [100/100]\t Validation Loss: 1.5159344914593273\t lr: 0.001\n",
      "Epoch [99/100]\t Time Taken: 27.398931578795114 minutes\n"
     ]
    }
   ],
   "source": [
    "train(net_adv, train_loader, train_criterion,val_loader, val_criterion, \n",
    "              optimizer, 100, writer, \n",
    "                 PATH, True, pgd_attack, device=\"cuda\",\n",
    "                  n_steps_show=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0410a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 48 %\n"
     ]
    }
   ],
   "source": [
    "#test clean accuray\n",
    "test(val_loader, net_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b713f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 43.13 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_adv, adv_images_fgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"adv model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47b5b47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 42.59 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_adv, adv_images_vmifgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"adv model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df98861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 42.91 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_adv, adv_images_nifgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"adv model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43823993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 43.04 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_adv, adv_images_pgd_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"adv model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d350815",
   "metadata": {},
   "source": [
    "# Adversarial Training by nifgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc69ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100]\t\n",
      "Epoch [1/100]\t Training Loss: 2.500364912745288\t lr: 0.001\n",
      "Epoch [1/100]\t Validation Loss: 1.729025531418716\t lr: 0.001\n",
      "Epoch [1/100]\t\n",
      "Epoch [2/100]\t Training Loss: 2.138056641649407\t lr: 0.001\n",
      "Epoch [2/100]\t Validation Loss: 1.6933015086982823\t lr: 0.001\n",
      "Epoch [2/100]\t\n",
      "Epoch [3/100]\t Training Loss: 2.081118220258552\t lr: 0.001\n",
      "Epoch [3/100]\t Validation Loss: 1.6526241408118718\t lr: 0.001\n",
      "Epoch [3/100]\t\n",
      "Epoch [4/100]\t Training Loss: 2.0502456917482266\t lr: 0.001\n",
      "Epoch [4/100]\t Validation Loss: 1.644403364084944\t lr: 0.001\n",
      "Epoch [4/100]\t\n",
      "Epoch [5/100]\t Training Loss: 2.027401148815594\t lr: 0.001\n",
      "Epoch [5/100]\t Validation Loss: 1.6298579505727262\t lr: 0.001\n",
      "Epoch [5/100]\t\n",
      "Epoch [6/100]\t Training Loss: 2.012241824508628\t lr: 0.001\n",
      "Epoch [6/100]\t Validation Loss: 1.612912728816648\t lr: 0.001\n",
      "Epoch [6/100]\t\n",
      "Epoch [7/100]\t Training Loss: 1.999943081375278\t lr: 0.001\n",
      "Epoch [7/100]\t Validation Loss: 1.6125406539892848\t lr: 0.001\n",
      "Epoch [7/100]\t\n",
      "Epoch [8/100]\t Training Loss: 1.988377822024743\t lr: 0.001\n",
      "Epoch [8/100]\t Validation Loss: 1.6057720289954656\t lr: 0.001\n",
      "Epoch [8/100]\t\n",
      "Epoch [9/100]\t Training Loss: 1.9803548377493154\t lr: 0.001\n",
      "Epoch [9/100]\t Validation Loss: 1.5959879672980006\t lr: 0.001\n",
      "Epoch [9/100]\t\n",
      "Epoch [10/100]\t Training Loss: 1.9725686863560201\t lr: 0.001\n",
      "Epoch [10/100]\t Validation Loss: 1.6025493099719663\t lr: 0.001\n",
      "Epoch [10/100]\t\n",
      "Epoch [11/100]\t Training Loss: 1.9665151105817322\t lr: 0.001\n",
      "Epoch [11/100]\t Validation Loss: 1.5896384640585017\t lr: 0.001\n",
      "Epoch [11/100]\t\n",
      "Epoch [12/100]\t Training Loss: 1.9604111794010757\t lr: 0.001\n",
      "Epoch [12/100]\t Validation Loss: 1.583390457720696\t lr: 0.001\n",
      "Epoch [12/100]\t\n",
      "Epoch [13/100]\t Training Loss: 1.9554786020532593\t lr: 0.001\n",
      "Epoch [13/100]\t Validation Loss: 1.5780164938938768\t lr: 0.001\n",
      "Epoch [13/100]\t\n",
      "Epoch [14/100]\t Training Loss: 1.9501672416087001\t lr: 0.001\n",
      "Epoch [14/100]\t Validation Loss: 1.5778469224519367\t lr: 0.001\n",
      "Epoch [14/100]\t\n",
      "Epoch [15/100]\t Training Loss: 1.9456378242853658\t lr: 0.001\n",
      "Epoch [15/100]\t Validation Loss: 1.5625256013266648\t lr: 0.001\n",
      "Epoch [15/100]\t\n",
      "Epoch [16/100]\t Training Loss: 1.9416861576802285\t lr: 0.001\n",
      "Epoch [16/100]\t Validation Loss: 1.5765612638449367\t lr: 0.001\n",
      "Epoch [16/100]\t\n",
      "Epoch [17/100]\t Training Loss: 1.9389705721984434\t lr: 0.001\n",
      "Epoch [17/100]\t Validation Loss: 1.5685470541821251\t lr: 0.001\n",
      "Epoch [17/100]\t\n",
      "Epoch [18/100]\t Training Loss: 1.9348941681635043\t lr: 0.001\n",
      "Epoch [18/100]\t Validation Loss: 1.5573621188537985\t lr: 0.001\n",
      "Epoch [18/100]\t\n",
      "Epoch [19/100]\t Training Loss: 1.9316079586058321\t lr: 0.001\n",
      "Epoch [19/100]\t Validation Loss: 1.5556459336341182\t lr: 0.001\n",
      "Epoch [19/100]\t\n",
      "Epoch [20/100]\t Training Loss: 1.928521448998805\t lr: 0.001\n",
      "Epoch [20/100]\t Validation Loss: 1.5578881034368202\t lr: 0.001\n",
      "Epoch [20/100]\t\n",
      "Epoch [21/100]\t Training Loss: 1.925719056897761\t lr: 0.001\n",
      "Epoch [21/100]\t Validation Loss: 1.5504566672482067\t lr: 0.001\n",
      "Epoch [21/100]\t\n",
      "Epoch [22/100]\t Training Loss: 1.9230779906368012\t lr: 0.001\n",
      "Epoch [22/100]\t Validation Loss: 1.5455036887639686\t lr: 0.001\n",
      "Epoch [22/100]\t\n",
      "Epoch [23/100]\t Training Loss: 1.919867659163902\t lr: 0.001\n",
      "Epoch [23/100]\t Validation Loss: 1.549444633194163\t lr: 0.001\n",
      "Epoch [23/100]\t\n",
      "Epoch [24/100]\t Training Loss: 1.9170921723860914\t lr: 0.001\n",
      "Epoch [24/100]\t Validation Loss: 1.5495222520224656\t lr: 0.001\n",
      "Epoch [24/100]\t\n",
      "Epoch [25/100]\t Training Loss: 1.9161119345203994\t lr: 0.001\n",
      "Epoch [25/100]\t Validation Loss: 1.5475875697558439\t lr: 0.001\n",
      "Epoch [25/100]\t\n",
      "Epoch [26/100]\t Training Loss: 1.9129124413365903\t lr: 0.001\n",
      "Epoch [26/100]\t Validation Loss: 1.5430827261526374\t lr: 0.001\n",
      "Epoch [26/100]\t\n",
      "Epoch [27/100]\t Training Loss: 1.911163046536848\t lr: 0.001\n",
      "Epoch [27/100]\t Validation Loss: 1.5339125741886188\t lr: 0.001\n",
      "Epoch [27/100]\t\n",
      "Epoch [28/100]\t Training Loss: 1.9080027128424486\t lr: 0.001\n",
      "Epoch [28/100]\t Validation Loss: 1.5279487914676908\t lr: 0.001\n",
      "Epoch [28/100]\t\n",
      "Epoch [29/100]\t Training Loss: 1.9068821882043043\t lr: 0.001\n",
      "Epoch [29/100]\t Validation Loss: 1.5352416732643224\t lr: 0.001\n",
      "Epoch [29/100]\t\n",
      "Epoch [30/100]\t Training Loss: 1.9045676541755268\t lr: 0.001\n",
      "Epoch [30/100]\t Validation Loss: 1.5322776881954339\t lr: 0.001\n",
      "Epoch [30/100]\t\n",
      "Epoch [31/100]\t Training Loss: 1.9014563386702477\t lr: 0.001\n",
      "Epoch [31/100]\t Validation Loss: 1.5307184638856333\t lr: 0.001\n",
      "Epoch [31/100]\t\n",
      "Epoch [32/100]\t Training Loss: 1.90036129158781\t lr: 0.001\n",
      "Epoch [32/100]\t Validation Loss: 1.5318871464910386\t lr: 0.001\n",
      "Epoch [32/100]\t\n",
      "Epoch [33/100]\t Training Loss: 1.8988748639440902\t lr: 0.001\n",
      "Epoch [33/100]\t Validation Loss: 1.5272016253652452\t lr: 0.001\n",
      "Epoch [33/100]\t\n",
      "Epoch [34/100]\t Training Loss: 1.8974395798295356\t lr: 0.001\n",
      "Epoch [34/100]\t Validation Loss: 1.5306119632117356\t lr: 0.001\n",
      "Epoch [34/100]\t\n",
      "Epoch [35/100]\t Training Loss: 1.8949751094783969\t lr: 0.001\n",
      "Epoch [35/100]\t Validation Loss: 1.518605316741557\t lr: 0.001\n",
      "Epoch [35/100]\t\n",
      "Epoch [36/100]\t Training Loss: 1.893469941890453\t lr: 0.001\n",
      "Epoch [36/100]\t Validation Loss: 1.5236613856086247\t lr: 0.001\n",
      "Epoch [36/100]\t\n",
      "Epoch [37/100]\t Training Loss: 1.891437474114206\t lr: 0.001\n",
      "Epoch [37/100]\t Validation Loss: 1.5240066730523412\t lr: 0.001\n",
      "Epoch [37/100]\t\n",
      "Epoch [38/100]\t Training Loss: 1.8904406417666189\t lr: 0.001\n",
      "Epoch [38/100]\t Validation Loss: 1.5162248988694782\t lr: 0.001\n",
      "Epoch [38/100]\t\n",
      "Epoch [39/100]\t Training Loss: 1.88813649845855\t lr: 0.001\n",
      "Epoch [39/100]\t Validation Loss: 1.5081298924699615\t lr: 0.001\n",
      "Epoch [39/100]\t\n",
      "Epoch [40/100]\t Training Loss: 1.8873536922132876\t lr: 0.001\n",
      "Epoch [40/100]\t Validation Loss: 1.5220123016381566\t lr: 0.001\n",
      "Epoch [40/100]\t\n",
      "Epoch [41/100]\t Training Loss: 1.8851971101882818\t lr: 0.001\n",
      "Epoch [41/100]\t Validation Loss: 1.5253999384143684\t lr: 0.001\n",
      "Epoch [41/100]\t\n",
      "Epoch [42/100]\t Training Loss: 1.8825782897222378\t lr: 0.001\n",
      "Epoch [42/100]\t Validation Loss: 1.5164356518395339\t lr: 0.001\n",
      "Epoch [42/100]\t\n",
      "Epoch [43/100]\t Training Loss: 1.8824860607571614\t lr: 0.001\n",
      "Epoch [43/100]\t Validation Loss: 1.514239333852937\t lr: 0.001\n",
      "Epoch [43/100]\t\n",
      "Epoch [44/100]\t Training Loss: 1.8804796574365756\t lr: 0.001\n",
      "Epoch [44/100]\t Validation Loss: 1.5146736902526663\t lr: 0.001\n",
      "Epoch [44/100]\t\n",
      "Epoch [45/100]\t Training Loss: 1.8790235016352075\t lr: 0.001\n",
      "Epoch [45/100]\t Validation Loss: 1.511192110520375\t lr: 0.001\n",
      "Epoch [45/100]\t\n",
      "Epoch [46/100]\t Training Loss: 1.877902715407369\t lr: 0.001\n",
      "Epoch [46/100]\t Validation Loss: 1.5039970316464388\t lr: 0.001\n",
      "Epoch [46/100]\t\n",
      "Epoch [47/100]\t Training Loss: 1.8761594633921943\t lr: 0.001\n",
      "Epoch [47/100]\t Validation Loss: 1.505342242083972\t lr: 0.001\n",
      "Epoch [47/100]\t\n",
      "Epoch [48/100]\t Training Loss: 1.8754189432124653\t lr: 0.001\n",
      "Epoch [48/100]\t Validation Loss: 1.5032313941400262\t lr: 0.001\n",
      "Epoch [48/100]\t\n",
      "Epoch [49/100]\t Training Loss: 1.8742779096983888\t lr: 0.001\n",
      "Epoch [49/100]\t Validation Loss: 1.5095266556438012\t lr: 0.001\n",
      "Epoch [49/100]\t\n",
      "Epoch [50/100]\t Training Loss: 1.8722964098386448\t lr: 0.001\n",
      "Epoch [50/100]\t Validation Loss: 1.505274478393265\t lr: 0.001\n",
      "Epoch [50/100]\t\n",
      "Epoch [51/100]\t Training Loss: 1.8718473646037108\t lr: 0.001\n",
      "Epoch [51/100]\t Validation Loss: 1.4959326665612716\t lr: 0.001\n",
      "Epoch [51/100]\t\n",
      "Epoch [52/100]\t Training Loss: 1.870227154563455\t lr: 0.001\n",
      "Epoch [52/100]\t Validation Loss: 1.4999315104907072\t lr: 0.001\n",
      "Epoch [52/100]\t\n",
      "Epoch [53/100]\t Training Loss: 1.8681675963999365\t lr: 0.001\n",
      "Epoch [53/100]\t Validation Loss: 1.4929524448853504\t lr: 0.001\n",
      "Epoch [53/100]\t\n",
      "Epoch [54/100]\t Training Loss: 1.867221041408646\t lr: 0.001\n",
      "Epoch [54/100]\t Validation Loss: 1.507399705391896\t lr: 0.001\n",
      "Epoch [54/100]\t\n",
      "Epoch [55/100]\t Training Loss: 1.8662496832630517\t lr: 0.001\n",
      "Epoch [55/100]\t Validation Loss: 1.4949355034888545\t lr: 0.001\n",
      "Epoch [55/100]\t\n",
      "Epoch [56/100]\t Training Loss: 1.8651021164091652\t lr: 0.001\n",
      "Epoch [56/100]\t Validation Loss: 1.4885660575914987\t lr: 0.001\n",
      "Epoch [56/100]\t\n",
      "Epoch [57/100]\t Training Loss: 1.8645021186765198\t lr: 0.001\n",
      "Epoch [57/100]\t Validation Loss: 1.4943145392816277\t lr: 0.001\n",
      "Epoch [57/100]\t\n",
      "Epoch [58/100]\t Training Loss: 1.8638464377054473\t lr: 0.001\n",
      "Epoch [58/100]\t Validation Loss: 1.4961080339890491\t lr: 0.001\n",
      "Epoch [58/100]\t\n",
      "Epoch [59/100]\t Training Loss: 1.8622973260977078\t lr: 0.001\n",
      "Epoch [59/100]\t Validation Loss: 1.4872977114930939\t lr: 0.001\n",
      "Epoch [59/100]\t\n",
      "Epoch [60/100]\t Training Loss: 1.8606001656988393\t lr: 0.001\n",
      "Epoch [60/100]\t Validation Loss: 1.495694113683097\t lr: 0.001\n",
      "Epoch [60/100]\t\n",
      "Epoch [61/100]\t Training Loss: 1.8601036166291103\t lr: 0.001\n",
      "Epoch [61/100]\t Validation Loss: 1.492681925809836\t lr: 0.001\n",
      "Epoch [61/100]\t\n",
      "Epoch [62/100]\t Training Loss: 1.8580348973383989\t lr: 0.001\n",
      "Epoch [62/100]\t Validation Loss: 1.4933908709996864\t lr: 0.001\n",
      "Epoch [62/100]\t\n",
      "Epoch [63/100]\t Training Loss: 1.8571990749720113\t lr: 0.001\n",
      "Epoch [63/100]\t Validation Loss: 1.4836483424222922\t lr: 0.001\n",
      "Epoch [63/100]\t\n",
      "Epoch [64/100]\t Training Loss: 1.8580074090786907\t lr: 0.001\n",
      "Epoch [64/100]\t Validation Loss: 1.4936807080160213\t lr: 0.001\n",
      "Epoch [64/100]\t\n",
      "Epoch [65/100]\t Training Loss: 1.8554840536068773\t lr: 0.001\n",
      "Epoch [65/100]\t Validation Loss: 1.4794664201857168\t lr: 0.001\n",
      "Epoch [65/100]\t\n",
      "Epoch [66/100]\t Training Loss: 1.8548722056781544\t lr: 0.001\n",
      "Epoch [66/100]\t Validation Loss: 1.4875645682781558\t lr: 0.001\n",
      "Epoch [66/100]\t\n",
      "Epoch [67/100]\t Training Loss: 1.8533225873547137\t lr: 0.001\n",
      "Epoch [67/100]\t Validation Loss: 1.48820797853832\t lr: 0.001\n",
      "Epoch [67/100]\t\n",
      "Epoch [68/100]\t Training Loss: 1.8532903477968767\t lr: 0.001\n",
      "Epoch [68/100]\t Validation Loss: 1.493097857583927\t lr: 0.001\n",
      "Epoch [68/100]\t\n",
      "Epoch [69/100]\t Training Loss: 1.8515313860705442\t lr: 0.001\n",
      "Epoch [69/100]\t Validation Loss: 1.4806311658666105\t lr: 0.001\n",
      "Epoch [69/100]\t\n",
      "Epoch [70/100]\t Training Loss: 1.851420028740183\t lr: 0.001\n",
      "Epoch [70/100]\t Validation Loss: 1.4781463312197336\t lr: 0.001\n",
      "Epoch [70/100]\t\n",
      "Epoch [71/100]\t Training Loss: 1.8495588269075165\t lr: 0.001\n",
      "Epoch [71/100]\t Validation Loss: 1.4847434638421746\t lr: 0.001\n",
      "Epoch [71/100]\t\n",
      "Epoch [72/100]\t Training Loss: 1.8492618515668318\t lr: 0.001\n",
      "Epoch [72/100]\t Validation Loss: 1.482297939590261\t lr: 0.001\n",
      "Epoch [72/100]\t\n",
      "Epoch [73/100]\t Training Loss: 1.848942107556726\t lr: 0.001\n",
      "Epoch [73/100]\t Validation Loss: 1.4849688931356502\t lr: 0.001\n",
      "Epoch [73/100]\t\n",
      "Epoch [74/100]\t Training Loss: 1.847064511550357\t lr: 0.001\n",
      "Epoch [74/100]\t Validation Loss: 1.4787640903569474\t lr: 0.001\n",
      "Epoch [74/100]\t\n",
      "Epoch [75/100]\t Training Loss: 1.8455859684883176\t lr: 0.001\n",
      "Epoch [75/100]\t Validation Loss: 1.4726468701905842\t lr: 0.001\n",
      "Epoch [75/100]\t\n",
      "Epoch [76/100]\t Training Loss: 1.8461489012784056\t lr: 0.001\n",
      "Epoch [76/100]\t Validation Loss: 1.4797368215609201\t lr: 0.001\n",
      "Epoch [76/100]\t\n",
      "Epoch [77/100]\t Training Loss: 1.844694147329501\t lr: 0.001\n",
      "Epoch [77/100]\t Validation Loss: 1.4740712235245523\t lr: 0.001\n",
      "Epoch [77/100]\t\n",
      "Epoch [78/100]\t Training Loss: 1.8434427034519518\t lr: 0.001\n",
      "Epoch [78/100]\t Validation Loss: 1.480323928820936\t lr: 0.001\n",
      "Epoch [78/100]\t\n",
      "Epoch [79/100]\t Training Loss: 1.842954758488004\t lr: 0.001\n",
      "Epoch [79/100]\t Validation Loss: 1.4714886010447634\t lr: 0.001\n",
      "Epoch [79/100]\t\n",
      "Epoch [80/100]\t Training Loss: 1.841217455656632\t lr: 0.001\n",
      "Epoch [80/100]\t Validation Loss: 1.4817797398265404\t lr: 0.001\n",
      "Epoch [80/100]\t\n",
      "Epoch [81/100]\t Training Loss: 1.8415605146866625\t lr: 0.001\n",
      "Epoch [81/100]\t Validation Loss: 1.4804921180387087\t lr: 0.001\n",
      "Epoch [81/100]\t\n",
      "Epoch [82/100]\t Training Loss: 1.8401906170198679\t lr: 0.001\n",
      "Epoch [82/100]\t Validation Loss: 1.4753279308729534\t lr: 0.001\n",
      "Epoch [82/100]\t\n",
      "Epoch [83/100]\t Training Loss: 1.8404755220388818\t lr: 0.001\n",
      "Epoch [83/100]\t Validation Loss: 1.4778061320510092\t lr: 0.001\n",
      "Epoch [83/100]\t\n",
      "Epoch [84/100]\t Training Loss: 1.8386175702599918\t lr: 0.001\n",
      "Epoch [84/100]\t Validation Loss: 1.4718624443947514\t lr: 0.001\n",
      "Epoch [84/100]\t\n",
      "Epoch [85/100]\t Training Loss: 1.8374318350916323\t lr: 0.001\n",
      "Epoch [85/100]\t Validation Loss: 1.471295424654514\t lr: 0.001\n",
      "Epoch [85/100]\t\n",
      "Epoch [86/100]\t Training Loss: 1.836962622449831\t lr: 0.001\n",
      "Epoch [86/100]\t Validation Loss: 1.4785841250721412\t lr: 0.001\n",
      "Epoch [86/100]\t\n",
      "Epoch [87/100]\t Training Loss: 1.8362646566327576\t lr: 0.001\n",
      "Epoch [87/100]\t Validation Loss: 1.468690855593621\t lr: 0.001\n",
      "Epoch [87/100]\t\n",
      "Epoch [88/100]\t Training Loss: 1.8356250259272582\t lr: 0.001\n",
      "Epoch [88/100]\t Validation Loss: 1.4634395656706412\t lr: 0.001\n",
      "Epoch [88/100]\t\n",
      "Epoch [89/100]\t Training Loss: 1.8345791747807847\t lr: 0.001\n",
      "Epoch [89/100]\t Validation Loss: 1.4688790297206444\t lr: 0.001\n",
      "Epoch [89/100]\t\n",
      "Epoch [90/100]\t Training Loss: 1.8342460811595478\t lr: 0.001\n",
      "Epoch [90/100]\t Validation Loss: 1.4710623161702217\t lr: 0.001\n",
      "Epoch [90/100]\t\n",
      "Epoch [91/100]\t Training Loss: 1.8335145117376772\t lr: 0.001\n",
      "Epoch [91/100]\t Validation Loss: 1.4606635419628289\t lr: 0.001\n",
      "Epoch [91/100]\t\n",
      "Epoch [92/100]\t Training Loss: 1.8327117045517163\t lr: 0.001\n",
      "Epoch [92/100]\t Validation Loss: 1.465108171294007\t lr: 0.001\n",
      "Epoch [92/100]\t\n",
      "Epoch [93/100]\t Training Loss: 1.8312527154717604\t lr: 0.001\n",
      "Epoch [93/100]\t Validation Loss: 1.466881669020351\t lr: 0.001\n",
      "Epoch [93/100]\t\n",
      "Epoch [94/100]\t Training Loss: 1.8311704256955315\t lr: 0.001\n",
      "Epoch [94/100]\t Validation Loss: 1.4670055757595013\t lr: 0.001\n",
      "Epoch [94/100]\t\n",
      "Epoch [95/100]\t Training Loss: 1.830883173076698\t lr: 0.001\n",
      "Epoch [95/100]\t Validation Loss: 1.471221481697469\t lr: 0.001\n",
      "Epoch [95/100]\t\n",
      "Epoch [96/100]\t Training Loss: 1.8296905926731237\t lr: 0.001\n",
      "Epoch [96/100]\t Validation Loss: 1.463872336134126\t lr: 0.001\n",
      "Epoch [96/100]\t\n",
      "Epoch [97/100]\t Training Loss: 1.8293891968324667\t lr: 0.001\n",
      "Epoch [97/100]\t Validation Loss: 1.4639701103862328\t lr: 0.001\n",
      "Epoch [97/100]\t\n",
      "Epoch [98/100]\t Training Loss: 1.8285546308893073\t lr: 0.001\n",
      "Epoch [98/100]\t Validation Loss: 1.4642573338520677\t lr: 0.001\n",
      "Epoch [98/100]\t\n",
      "Epoch [99/100]\t Training Loss: 1.8283049307210977\t lr: 0.001\n",
      "Epoch [99/100]\t Validation Loss: 1.461152140098282\t lr: 0.001\n",
      "Epoch [99/100]\t\n",
      "Epoch [100/100]\t Training Loss: 1.8272397691941322\t lr: 0.001\n",
      "Epoch [100/100]\t Validation Loss: 1.4649916464769388\t lr: 0.001\n",
      "Epoch [99/100]\t Time Taken: 27.612175023555757 minutes\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "net_adv_ni = copy.deepcopy(net_ori) #am I doing correct? \n",
    "\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_adv_ni.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = './cifar_base_adv_nifgsm.pth'\n",
    "device = \"cuda\"\n",
    "writer = SummaryWriter(\"Adv_Base_nifgsm\")\n",
    "\n",
    "ni_attack = generate_adv(net_adv_ni, \"nifgsm\")\n",
    "\n",
    "train(net_adv_ni, train_loader, train_criterion,val_loader, val_criterion, \n",
    "              optimizer, 100, writer, \n",
    "                 PATH, True, ni_attack, device=\"cuda\",\n",
    "                  n_steps_show=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5957488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 50 %\n"
     ]
    }
   ],
   "source": [
    "#test clean accuray\n",
    "test(val_loader, net_adv_ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebeef131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 44.23 %\n",
      "fgsm_sample\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_adv_ni, adv_images_fgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"fgsm_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32938eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 43.80 %\n",
      "vmifgsm_sample\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_adv_ni, adv_images_vmifgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"vmifgsm_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7af3ff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 44.11 %\n",
      "nifgsm_sample\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_adv_ni, adv_images_nifgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"nifgsm_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1a866c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 44.19 %\n",
      "pgd_sample\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_adv_ni, adv_images_pgd_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"pgd_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d1d9d",
   "metadata": {},
   "source": [
    "# Adversarial Training by vmifgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bce77f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100]\t\n",
      "Epoch [1/100]\t Training Loss: 2.6150772364243218\t lr: 0.001\n",
      "Epoch [1/100]\t Validation Loss: 1.844184164759479\t lr: 0.001\n",
      "Epoch [1/100]\t\n",
      "Epoch [2/100]\t Training Loss: 2.228119373931299\t lr: 0.001\n",
      "Epoch [2/100]\t Validation Loss: 1.8138783234584182\t lr: 0.001\n",
      "Epoch [2/100]\t\n",
      "Epoch [3/100]\t Training Loss: 2.1652783024341553\t lr: 0.001\n",
      "Epoch [3/100]\t Validation Loss: 1.7679892159715485\t lr: 0.001\n",
      "Epoch [3/100]\t\n",
      "Epoch [4/100]\t Training Loss: 2.128527822701827\t lr: 0.001\n",
      "Epoch [4/100]\t Validation Loss: 1.752752812602852\t lr: 0.001\n",
      "Epoch [4/100]\t\n",
      "Epoch [5/100]\t Training Loss: 2.1025349182241104\t lr: 0.001\n",
      "Epoch [5/100]\t Validation Loss: 1.7373844083351424\t lr: 0.001\n",
      "Epoch [5/100]\t\n",
      "Epoch [6/100]\t Training Loss: 2.083202747432777\t lr: 0.001\n",
      "Epoch [6/100]\t Validation Loss: 1.7255418949489352\t lr: 0.001\n",
      "Epoch [6/100]\t\n",
      "Epoch [7/100]\t Training Loss: 2.0671914245771323\t lr: 0.001\n",
      "Epoch [7/100]\t Validation Loss: 1.7078131769276872\t lr: 0.001\n",
      "Epoch [7/100]\t\n",
      "Epoch [8/100]\t Training Loss: 2.0563247456879874\t lr: 0.001\n",
      "Epoch [8/100]\t Validation Loss: 1.7197208495079717\t lr: 0.001\n",
      "Epoch [8/100]\t\n",
      "Epoch [9/100]\t Training Loss: 2.04562848882602\t lr: 0.001\n",
      "Epoch [9/100]\t Validation Loss: 1.6916077197352541\t lr: 0.001\n",
      "Epoch [9/100]\t\n",
      "Epoch [10/100]\t Training Loss: 2.037552670139791\t lr: 0.001\n",
      "Epoch [10/100]\t Validation Loss: 1.6792808031734032\t lr: 0.001\n",
      "Epoch [10/100]\t\n",
      "Epoch [11/100]\t Training Loss: 2.02966263653982\t lr: 0.001\n",
      "Epoch [11/100]\t Validation Loss: 1.669405854201015\t lr: 0.001\n",
      "Epoch [11/100]\t\n",
      "Epoch [12/100]\t Training Loss: 2.021981585970925\t lr: 0.001\n",
      "Epoch [12/100]\t Validation Loss: 1.6767685021026224\t lr: 0.001\n",
      "Epoch [12/100]\t\n",
      "Epoch [13/100]\t Training Loss: 2.016782805437932\t lr: 0.001\n",
      "Epoch [13/100]\t Validation Loss: 1.665238090708286\t lr: 0.001\n",
      "Epoch [13/100]\t\n",
      "Epoch [14/100]\t Training Loss: 2.010567200763146\t lr: 0.001\n",
      "Epoch [14/100]\t Validation Loss: 1.6508642751959306\t lr: 0.001\n",
      "Epoch [14/100]\t\n",
      "Epoch [15/100]\t Training Loss: 2.0063436702084356\t lr: 0.001\n",
      "Epoch [15/100]\t Validation Loss: 1.6544982858850985\t lr: 0.001\n",
      "Epoch [15/100]\t\n",
      "Epoch [16/100]\t Training Loss: 2.0029740086601824\t lr: 0.001\n",
      "Epoch [16/100]\t Validation Loss: 1.657663864425466\t lr: 0.001\n",
      "Epoch [16/100]\t\n",
      "Epoch [17/100]\t Training Loss: 1.997938242104962\t lr: 0.001\n",
      "Epoch [17/100]\t Validation Loss: 1.6430189171923866\t lr: 0.001\n",
      "Epoch [17/100]\t\n",
      "Epoch [18/100]\t Training Loss: 1.9939659681466535\t lr: 0.001\n",
      "Epoch [18/100]\t Validation Loss: 1.64137800132172\t lr: 0.001\n",
      "Epoch [18/100]\t\n",
      "Epoch [19/100]\t Training Loss: 1.9906879026261741\t lr: 0.001\n",
      "Epoch [19/100]\t Validation Loss: 1.6458068995536128\t lr: 0.001\n",
      "Epoch [19/100]\t\n",
      "Epoch [20/100]\t Training Loss: 1.9879597126675383\t lr: 0.001\n",
      "Epoch [20/100]\t Validation Loss: 1.640186893789074\t lr: 0.001\n",
      "Epoch [20/100]\t\n",
      "Epoch [21/100]\t Training Loss: 1.9847171785276565\t lr: 0.001\n",
      "Epoch [21/100]\t Validation Loss: 1.624974895127212\t lr: 0.001\n",
      "Epoch [21/100]\t\n",
      "Epoch [22/100]\t Training Loss: 1.9814859220134022\t lr: 0.001\n",
      "Epoch [22/100]\t Validation Loss: 1.6272797433635857\t lr: 0.001\n",
      "Epoch [22/100]\t\n",
      "Epoch [23/100]\t Training Loss: 1.9793897912935223\t lr: 0.001\n",
      "Epoch [23/100]\t Validation Loss: 1.614866255204889\t lr: 0.001\n",
      "Epoch [23/100]\t\n",
      "Epoch [24/100]\t Training Loss: 1.978256406686495\t lr: 0.001\n",
      "Epoch [24/100]\t Validation Loss: 1.6232668689534635\t lr: 0.001\n",
      "Epoch [24/100]\t\n",
      "Epoch [25/100]\t Training Loss: 1.973939788311034\t lr: 0.001\n",
      "Epoch [25/100]\t Validation Loss: 1.624476458452925\t lr: 0.001\n",
      "Epoch [25/100]\t\n",
      "Epoch [26/100]\t Training Loss: 1.9724425461591053\t lr: 0.001\n",
      "Epoch [26/100]\t Validation Loss: 1.623578684239448\t lr: 0.001\n",
      "Epoch [26/100]\t\n",
      "Epoch [27/100]\t Training Loss: 1.9702320165951233\t lr: 0.001\n",
      "Epoch [27/100]\t Validation Loss: 1.6113732208179523\t lr: 0.001\n",
      "Epoch [27/100]\t\n",
      "Epoch [28/100]\t Training Loss: 1.9680810508215825\t lr: 0.001\n",
      "Epoch [28/100]\t Validation Loss: 1.6098922702330578\t lr: 0.001\n",
      "Epoch [28/100]\t\n",
      "Epoch [29/100]\t Training Loss: 1.9654506210171048\t lr: 0.001\n",
      "Epoch [29/100]\t Validation Loss: 1.6046209969098055\t lr: 0.001\n",
      "Epoch [29/100]\t\n",
      "Epoch [30/100]\t Training Loss: 1.963593207356875\t lr: 0.001\n",
      "Epoch [30/100]\t Validation Loss: 1.6091257545012463\t lr: 0.001\n",
      "Epoch [30/100]\t\n",
      "Epoch [31/100]\t Training Loss: 1.9617797697291655\t lr: 0.001\n",
      "Epoch [31/100]\t Validation Loss: 1.6074190290668342\t lr: 0.001\n",
      "Epoch [31/100]\t\n",
      "Epoch [32/100]\t Training Loss: 1.9603573361321178\t lr: 0.001\n",
      "Epoch [32/100]\t Validation Loss: 1.6106654647030407\t lr: 0.001\n",
      "Epoch [32/100]\t\n",
      "Epoch [33/100]\t Training Loss: 1.9580756227683533\t lr: 0.001\n",
      "Epoch [33/100]\t Validation Loss: 1.5995325438583954\t lr: 0.001\n",
      "Epoch [33/100]\t\n",
      "Epoch [34/100]\t Training Loss: 1.9574551411601893\t lr: 0.001\n",
      "Epoch [34/100]\t Validation Loss: 1.606152065192597\t lr: 0.001\n",
      "Epoch [34/100]\t\n",
      "Epoch [35/100]\t Training Loss: 1.9548388750047025\t lr: 0.001\n",
      "Epoch [35/100]\t Validation Loss: 1.6026824682573728\t lr: 0.001\n",
      "Epoch [35/100]\t\n",
      "Epoch [36/100]\t Training Loss: 1.9539810617256652\t lr: 0.001\n",
      "Epoch [36/100]\t Validation Loss: 1.5927690448640268\t lr: 0.001\n",
      "Epoch [36/100]\t\n",
      "Epoch [37/100]\t Training Loss: 1.9519587447271323\t lr: 0.001\n",
      "Epoch [37/100]\t Validation Loss: 1.5943547577797612\t lr: 0.001\n",
      "Epoch [37/100]\t\n",
      "Epoch [38/100]\t Training Loss: 1.9503516763677378\t lr: 0.001\n",
      "Epoch [38/100]\t Validation Loss: 1.5960380125649367\t lr: 0.001\n",
      "Epoch [38/100]\t\n",
      "Epoch [39/100]\t Training Loss: 1.9491013587283357\t lr: 0.001\n",
      "Epoch [39/100]\t Validation Loss: 1.5906479388852663\t lr: 0.001\n",
      "Epoch [39/100]\t\n",
      "Epoch [40/100]\t Training Loss: 1.9477839497349145\t lr: 0.001\n",
      "Epoch [40/100]\t Validation Loss: 1.593662969673736\t lr: 0.001\n",
      "Epoch [40/100]\t\n",
      "Epoch [41/100]\t Training Loss: 1.9460311689035361\t lr: 0.001\n",
      "Epoch [41/100]\t Validation Loss: 1.5929213822642458\t lr: 0.001\n",
      "Epoch [41/100]\t\n",
      "Epoch [42/100]\t Training Loss: 1.9441445895168177\t lr: 0.001\n",
      "Epoch [42/100]\t Validation Loss: 1.5904710473893564\t lr: 0.001\n",
      "Epoch [42/100]\t\n",
      "Epoch [43/100]\t Training Loss: 1.9432648691679815\t lr: 0.001\n",
      "Epoch [43/100]\t Validation Loss: 1.5854513886608654\t lr: 0.001\n",
      "Epoch [43/100]\t\n",
      "Epoch [44/100]\t Training Loss: 1.9421810097706593\t lr: 0.001\n",
      "Epoch [44/100]\t Validation Loss: 1.5911716871623751\t lr: 0.001\n",
      "Epoch [44/100]\t\n",
      "Epoch [45/100]\t Training Loss: 1.9407353401184082\t lr: 0.001\n",
      "Epoch [45/100]\t Validation Loss: 1.5925381983382791\t lr: 0.001\n",
      "Epoch [45/100]\t\n",
      "Epoch [46/100]\t Training Loss: 1.939206231890432\t lr: 0.001\n",
      "Epoch [46/100]\t Validation Loss: 1.5861361072033267\t lr: 0.001\n",
      "Epoch [46/100]\t\n",
      "Epoch [47/100]\t Training Loss: 1.9386007343716634\t lr: 0.001\n",
      "Epoch [47/100]\t Validation Loss: 1.5834302374079257\t lr: 0.001\n",
      "Epoch [47/100]\t\n",
      "Epoch [48/100]\t Training Loss: 1.9372184179018221\t lr: 0.001\n",
      "Epoch [48/100]\t Validation Loss: 1.583692832838131\t lr: 0.001\n",
      "Epoch [48/100]\t\n",
      "Epoch [49/100]\t Training Loss: 1.9360663464002292\t lr: 0.001\n",
      "Epoch [49/100]\t Validation Loss: 1.5843454795547678\t lr: 0.001\n",
      "Epoch [49/100]\t\n",
      "Epoch [50/100]\t Training Loss: 1.9349068787396717\t lr: 0.001\n",
      "Epoch [50/100]\t Validation Loss: 1.5868063320087482\t lr: 0.001\n",
      "Epoch [50/100]\t\n",
      "Epoch [51/100]\t Training Loss: 1.9336460481214401\t lr: 0.001\n",
      "Epoch [51/100]\t Validation Loss: 1.579207705546029\t lr: 0.001\n",
      "Epoch [51/100]\t\n",
      "Epoch [52/100]\t Training Loss: 1.9317882966507427\t lr: 0.001\n",
      "Epoch [52/100]\t Validation Loss: 1.5827831105340886\t lr: 0.001\n",
      "Epoch [52/100]\t\n",
      "Epoch [53/100]\t Training Loss: 1.9307751814117822\t lr: 0.001\n",
      "Epoch [53/100]\t Validation Loss: 1.5780031484893606\t lr: 0.001\n",
      "Epoch [53/100]\t\n",
      "Epoch [54/100]\t Training Loss: 1.9298698661272482\t lr: 0.001\n",
      "Epoch [54/100]\t Validation Loss: 1.5788987814625608\t lr: 0.001\n",
      "Epoch [54/100]\t\n",
      "Epoch [55/100]\t Training Loss: 1.929073971250783\t lr: 0.001\n",
      "Epoch [55/100]\t Validation Loss: 1.5745903842056854\t lr: 0.001\n",
      "Epoch [55/100]\t\n",
      "Epoch [56/100]\t Training Loss: 1.9282652368326016\t lr: 0.001\n",
      "Epoch [56/100]\t Validation Loss: 1.5742166630829437\t lr: 0.001\n",
      "Epoch [56/100]\t\n",
      "Epoch [57/100]\t Training Loss: 1.926609454862297\t lr: 0.001\n",
      "Epoch [57/100]\t Validation Loss: 1.5768313875681237\t lr: 0.001\n",
      "Epoch [57/100]\t\n",
      "Epoch [58/100]\t Training Loss: 1.9256785263490799\t lr: 0.001\n",
      "Epoch [58/100]\t Validation Loss: 1.5663286462614807\t lr: 0.001\n",
      "Epoch [58/100]\t\n",
      "Epoch [59/100]\t Training Loss: 1.9248974125098695\t lr: 0.001\n",
      "Epoch [59/100]\t Validation Loss: 1.5654281136355823\t lr: 0.001\n",
      "Epoch [59/100]\t\n",
      "Epoch [60/100]\t Training Loss: 1.9232069839297048\t lr: 0.001\n",
      "Epoch [60/100]\t Validation Loss: 1.5726556521427781\t lr: 0.001\n",
      "Epoch [60/100]\t\n",
      "Epoch [61/100]\t Training Loss: 1.922964246376701\t lr: 0.001\n",
      "Epoch [61/100]\t Validation Loss: 1.5648036817961102\t lr: 0.001\n",
      "Epoch [61/100]\t\n",
      "Epoch [62/100]\t Training Loss: 1.9225229396844459\t lr: 0.001\n",
      "Epoch [62/100]\t Validation Loss: 1.5722511584245706\t lr: 0.001\n",
      "Epoch [62/100]\t\n",
      "Epoch [63/100]\t Training Loss: 1.9207413903892498\t lr: 0.001\n",
      "Epoch [63/100]\t Validation Loss: 1.5669327687613572\t lr: 0.001\n",
      "Epoch [63/100]\t\n",
      "Epoch [64/100]\t Training Loss: 1.9199414362992777\t lr: 0.001\n",
      "Epoch [64/100]\t Validation Loss: 1.5718060505541065\t lr: 0.001\n",
      "Epoch [64/100]\t\n",
      "Epoch [65/100]\t Training Loss: 1.918395722308732\t lr: 0.001\n",
      "Epoch [65/100]\t Validation Loss: 1.5653628261783454\t lr: 0.001\n",
      "Epoch [65/100]\t\n",
      "Epoch [66/100]\t Training Loss: 1.9179738372792978\t lr: 0.001\n",
      "Epoch [66/100]\t Validation Loss: 1.5667214031460919\t lr: 0.001\n",
      "Epoch [66/100]\t\n",
      "Epoch [67/100]\t Training Loss: 1.917155019462566\t lr: 0.001\n",
      "Epoch [67/100]\t Validation Loss: 1.5628159921380538\t lr: 0.001\n",
      "Epoch [67/100]\t\n",
      "Epoch [68/100]\t Training Loss: 1.9170880470129534\t lr: 0.001\n",
      "Epoch [68/100]\t Validation Loss: 1.5625739022146297\t lr: 0.001\n",
      "Epoch [68/100]\t\n",
      "Epoch [69/100]\t Training Loss: 1.9156931346029882\t lr: 0.001\n",
      "Epoch [69/100]\t Validation Loss: 1.5689180002936833\t lr: 0.001\n",
      "Epoch [69/100]\t\n",
      "Epoch [70/100]\t Training Loss: 1.9143501599426465\t lr: 0.001\n",
      "Epoch [70/100]\t Validation Loss: 1.5565978892241852\t lr: 0.001\n",
      "Epoch [70/100]\t\n",
      "Epoch [71/100]\t Training Loss: 1.9144729906335816\t lr: 0.001\n",
      "Epoch [71/100]\t Validation Loss: 1.552847515178632\t lr: 0.001\n",
      "Epoch [71/100]\t\n",
      "Epoch [72/100]\t Training Loss: 1.912947240083114\t lr: 0.001\n",
      "Epoch [72/100]\t Validation Loss: 1.5671736575380157\t lr: 0.001\n",
      "Epoch [72/100]\t\n",
      "Epoch [73/100]\t Training Loss: 1.9121652817177346\t lr: 0.001\n",
      "Epoch [73/100]\t Validation Loss: 1.5559283736385876\t lr: 0.001\n",
      "Epoch [73/100]\t\n",
      "Epoch [74/100]\t Training Loss: 1.91216106305037\t lr: 0.001\n",
      "Epoch [74/100]\t Validation Loss: 1.5510636236094222\t lr: 0.001\n",
      "Epoch [74/100]\t\n",
      "Epoch [75/100]\t Training Loss: 1.9114142674619279\t lr: 0.001\n",
      "Epoch [75/100]\t Validation Loss: 1.552912183954746\t lr: 0.001\n",
      "Epoch [75/100]\t\n",
      "Epoch [76/100]\t Training Loss: 1.9097787233264856\t lr: 0.001\n",
      "Epoch [76/100]\t Validation Loss: 1.5515537065795706\t lr: 0.001\n",
      "Epoch [76/100]\t\n",
      "Epoch [77/100]\t Training Loss: 1.909892429781082\t lr: 0.001\n",
      "Epoch [77/100]\t Validation Loss: 1.5550759578052955\t lr: 0.001\n",
      "Epoch [77/100]\t\n",
      "Epoch [78/100]\t Training Loss: 1.909358292589407\t lr: 0.001\n",
      "Epoch [78/100]\t Validation Loss: 1.5560262323934821\t lr: 0.001\n",
      "Epoch [78/100]\t\n",
      "Epoch [79/100]\t Training Loss: 1.909086396017343\t lr: 0.001\n",
      "Epoch [79/100]\t Validation Loss: 1.5537812377833113\t lr: 0.001\n",
      "Epoch [79/100]\t\n",
      "Epoch [80/100]\t Training Loss: 1.9075061034058671\t lr: 0.001\n",
      "Epoch [80/100]\t Validation Loss: 1.5644843080375768\t lr: 0.001\n",
      "Epoch [80/100]\t\n",
      "Epoch [81/100]\t Training Loss: 1.9076960230117563\t lr: 0.001\n",
      "Epoch [81/100]\t Validation Loss: 1.5480970642234706\t lr: 0.001\n",
      "Epoch [81/100]\t\n",
      "Epoch [82/100]\t Training Loss: 1.9063176218506015\t lr: 0.001\n",
      "Epoch [82/100]\t Validation Loss: 1.5578224115733859\t lr: 0.001\n",
      "Epoch [82/100]\t\n",
      "Epoch [83/100]\t Training Loss: 1.9061285501245953\t lr: 0.001\n",
      "Epoch [83/100]\t Validation Loss: 1.5448948808863192\t lr: 0.001\n",
      "Epoch [83/100]\t\n",
      "Epoch [84/100]\t Training Loss: 1.9055725427539758\t lr: 0.001\n",
      "Epoch [84/100]\t Validation Loss: 1.5506286968158771\t lr: 0.001\n",
      "Epoch [84/100]\t\n",
      "Epoch [85/100]\t Training Loss: 1.9054472794008377\t lr: 0.001\n",
      "Epoch [85/100]\t Validation Loss: 1.5477003206180622\t lr: 0.001\n",
      "Epoch [85/100]\t\n",
      "Epoch [86/100]\t Training Loss: 1.9049854449298986\t lr: 0.001\n",
      "Epoch [86/100]\t Validation Loss: 1.5508768513232847\t lr: 0.001\n",
      "Epoch [86/100]\t\n",
      "Epoch [87/100]\t Training Loss: 1.9046904470609582\t lr: 0.001\n",
      "Epoch [87/100]\t Validation Loss: 1.5519923409329186\t lr: 0.001\n",
      "Epoch [87/100]\t\n",
      "Epoch [88/100]\t Training Loss: 1.9037302488561176\t lr: 0.001\n",
      "Epoch [88/100]\t Validation Loss: 1.5446682172485544\t lr: 0.001\n",
      "Epoch [88/100]\t\n",
      "Epoch [89/100]\t Training Loss: 1.9027072941250813\t lr: 0.001\n",
      "Epoch [89/100]\t Validation Loss: 1.5454007206083853\t lr: 0.001\n",
      "Epoch [89/100]\t\n",
      "Epoch [90/100]\t Training Loss: 1.9021930322622704\t lr: 0.001\n",
      "Epoch [90/100]\t Validation Loss: 1.5538584989837454\t lr: 0.001\n",
      "Epoch [90/100]\t\n",
      "Epoch [91/100]\t Training Loss: 1.9013607535520782\t lr: 0.001\n",
      "Epoch [91/100]\t Validation Loss: 1.538885448552385\t lr: 0.001\n",
      "Epoch [91/100]\t\n",
      "Epoch [92/100]\t Training Loss: 1.9010269056500682\t lr: 0.001\n",
      "Epoch [92/100]\t Validation Loss: 1.5454876362522947\t lr: 0.001\n",
      "Epoch [92/100]\t\n",
      "Epoch [93/100]\t Training Loss: 1.8994399851850232\t lr: 0.001\n",
      "Epoch [93/100]\t Validation Loss: 1.541162112091161\t lr: 0.001\n",
      "Epoch [93/100]\t\n",
      "Epoch [94/100]\t Training Loss: 1.8994950397545114\t lr: 0.001\n",
      "Epoch [94/100]\t Validation Loss: 1.541426904593842\t lr: 0.001\n",
      "Epoch [94/100]\t\n",
      "Epoch [95/100]\t Training Loss: 1.8994105051240653\t lr: 0.001\n",
      "Epoch [95/100]\t Validation Loss: 1.538156418860713\t lr: 0.001\n",
      "Epoch [95/100]\t\n",
      "Epoch [96/100]\t Training Loss: 1.8977688700341813\t lr: 0.001\n",
      "Epoch [96/100]\t Validation Loss: 1.5402254681043988\t lr: 0.001\n",
      "Epoch [96/100]\t\n",
      "Epoch [97/100]\t Training Loss: 1.898252405169065\t lr: 0.001\n",
      "Epoch [97/100]\t Validation Loss: 1.5455070900011667\t lr: 0.001\n",
      "Epoch [97/100]\t\n",
      "Epoch [98/100]\t Training Loss: 1.8974669726608355\t lr: 0.001\n",
      "Epoch [98/100]\t Validation Loss: 1.5417314541490772\t lr: 0.001\n",
      "Epoch [98/100]\t\n",
      "Epoch [99/100]\t Training Loss: 1.897349183516734\t lr: 0.001\n",
      "Epoch [99/100]\t Validation Loss: 1.5465845563743688\t lr: 0.001\n",
      "Epoch [99/100]\t\n",
      "Epoch [100/100]\t Training Loss: 1.8957999865417285\t lr: 0.001\n",
      "Epoch [100/100]\t Validation Loss: 1.53994818126099\t lr: 0.001\n",
      "Epoch [99/100]\t Time Taken: 108.03003709316253 minutes\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "net_vmifgsm = copy.deepcopy(net_ori) \n",
    "\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_vmifgsm.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = './cifar_base_adv_vmifgsm.pth'\n",
    "device = \"cuda\"\n",
    "\n",
    "writer = SummaryWriter(\"Adv_Base_vmifgsm\")\n",
    "vmi_attack = generate_adv(net_vmifgsm, \"vmifgsm\")\n",
    "\n",
    "train(net_vmifgsm, train_loader, train_criterion,val_loader, val_criterion, \n",
    "              optimizer, 100, writer, \n",
    "                 PATH, True, vmi_attack, device=\"cuda\",\n",
    "                  n_steps_show=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4293c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 47 %\n"
     ]
    }
   ],
   "source": [
    "#test clean accuray\n",
    "test(val_loader, net_vmifgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "afe0bb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 42.77 %\n",
      "fgsm_sample\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_vmifgsm, adv_images_fgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"fgsm_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03b790fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 42.46 %\n",
      "vmifgsm_sample\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_vmifgsm, adv_images_vmifgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"vmifgsm_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5d70878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 42.69 %\n",
      "nifgsm_sample\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_vmifgsm, adv_images_nifgsm_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"nifgsm_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c034bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model loaded]\n",
      "Acc: 42.73 %\n",
      "pgd_sample\n"
     ]
    }
   ],
   "source": [
    "acc = clean_accuracy(net_vmifgsm, adv_images_pgd_adv, labels)\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))\n",
    "print(\"pgd_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662a6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
