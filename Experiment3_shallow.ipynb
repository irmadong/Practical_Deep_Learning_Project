{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4e8abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa4a355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "from model.baseline_model import *\n",
    "from trainer import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import robustbench\n",
    "#from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model, clean_accuracy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d31627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, train_loader, val_loader = CIFAR100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60e2f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ('plane', 'car', 'bird', 'cat',\n",
    "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "device = \"cuda\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0dee4e",
   "metadata": {},
   "source": [
    "# Train shallow CNN on CIFAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adbeaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "net_ori = BaseNet_test().to(device)\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_ori.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = './cifar100_base.pth'\n",
    "device = \"cuda\"\n",
    "writer = SummaryWriter(\"NormalBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "865ff15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100]\t\n",
      "Epoch [1/100]\t Training Loss: 4.605538014560709\t lr: 0.001\n",
      "Epoch [1/100]\t Validation Loss: 4.605128451238705\t lr: 0.001\n",
      "Epoch [1/100]\t\n",
      "Epoch [2/100]\t Training Loss: 4.605038394098696\t lr: 0.001\n",
      "Epoch [2/100]\t Validation Loss: 4.604664355893679\t lr: 0.001\n",
      "Epoch [2/100]\t\n",
      "Epoch [3/100]\t Training Loss: 4.604372770889945\t lr: 0.001\n",
      "Epoch [3/100]\t Validation Loss: 4.603858802891985\t lr: 0.001\n",
      "Epoch [3/100]\t\n",
      "Epoch [4/100]\t Training Loss: 4.6033164656070795\t lr: 0.001\n",
      "Epoch [4/100]\t Validation Loss: 4.602409718911859\t lr: 0.001\n",
      "Epoch [4/100]\t\n",
      "Epoch [5/100]\t Training Loss: 4.601454359186275\t lr: 0.001\n",
      "Epoch [5/100]\t Validation Loss: 4.599879832207402\t lr: 0.001\n",
      "Epoch [5/100]\t\n",
      "Epoch [6/100]\t Training Loss: 4.597498498609304\t lr: 0.001\n",
      "Epoch [6/100]\t Validation Loss: 4.593843870525118\t lr: 0.001\n",
      "Epoch [6/100]\t\n",
      "Epoch [7/100]\t Training Loss: 4.5875228725735795\t lr: 0.001\n",
      "Epoch [7/100]\t Validation Loss: 4.576691627502441\t lr: 0.001\n",
      "Epoch [7/100]\t\n",
      "Epoch [8/100]\t Training Loss: 4.548379817582153\t lr: 0.001\n",
      "Epoch [8/100]\t Validation Loss: 4.4916451912892015\t lr: 0.001\n",
      "Epoch [8/100]\t\n",
      "Epoch [9/100]\t Training Loss: 4.365627661995266\t lr: 0.001\n",
      "Epoch [9/100]\t Validation Loss: 4.242085849182515\t lr: 0.001\n",
      "Epoch [9/100]\t\n",
      "Epoch [10/100]\t Training Loss: 4.172884893539312\t lr: 0.001\n",
      "Epoch [10/100]\t Validation Loss: 4.123186712023578\t lr: 0.001\n",
      "Epoch [10/100]\t\n",
      "Epoch [11/100]\t Training Loss: 4.066884958530631\t lr: 0.001\n",
      "Epoch [11/100]\t Validation Loss: 4.045465961287293\t lr: 0.001\n",
      "Epoch [11/100]\t\n",
      "Epoch [12/100]\t Training Loss: 3.9886822859039697\t lr: 0.001\n",
      "Epoch [12/100]\t Validation Loss: 3.9610309812087046\t lr: 0.001\n",
      "Epoch [12/100]\t\n",
      "Epoch [13/100]\t Training Loss: 3.9173421670713693\t lr: 0.001\n",
      "Epoch [13/100]\t Validation Loss: 3.8975870247128643\t lr: 0.001\n",
      "Epoch [13/100]\t\n",
      "Epoch [14/100]\t Training Loss: 3.8519910485543254\t lr: 0.001\n",
      "Epoch [14/100]\t Validation Loss: 3.8483635986907574\t lr: 0.001\n",
      "Epoch [14/100]\t\n",
      "Epoch [15/100]\t Training Loss: 3.7854669087988033\t lr: 0.001\n",
      "Epoch [15/100]\t Validation Loss: 3.7709481867053842\t lr: 0.001\n",
      "Epoch [15/100]\t\n",
      "Epoch [16/100]\t Training Loss: 3.7202604243822415\t lr: 0.001\n",
      "Epoch [16/100]\t Validation Loss: 3.7243859103963346\t lr: 0.001\n",
      "Epoch [16/100]\t\n",
      "Epoch [17/100]\t Training Loss: 3.6613964022272993\t lr: 0.001\n",
      "Epoch [17/100]\t Validation Loss: 3.677086763744113\t lr: 0.001\n",
      "Epoch [17/100]\t\n",
      "Epoch [18/100]\t Training Loss: 3.602812872518359\t lr: 0.001\n",
      "Epoch [18/100]\t Validation Loss: 3.6186846328687063\t lr: 0.001\n",
      "Epoch [18/100]\t\n",
      "Epoch [19/100]\t Training Loss: 3.5448487744002084\t lr: 0.001\n",
      "Epoch [19/100]\t Validation Loss: 3.5901183840594713\t lr: 0.001\n",
      "Epoch [19/100]\t\n",
      "Epoch [20/100]\t Training Loss: 3.4944433470821137\t lr: 0.001\n",
      "Epoch [20/100]\t Validation Loss: 3.5429612503776067\t lr: 0.001\n",
      "Epoch [20/100]\t\n",
      "Epoch [21/100]\t Training Loss: 3.441887576866638\t lr: 0.001\n",
      "Epoch [21/100]\t Validation Loss: 3.4974996892711783\t lr: 0.001\n",
      "Epoch [21/100]\t\n",
      "Epoch [22/100]\t Training Loss: 3.3952462856117114\t lr: 0.001\n",
      "Epoch [22/100]\t Validation Loss: 3.4805756128287015\t lr: 0.001\n",
      "Epoch [22/100]\t\n",
      "Epoch [23/100]\t Training Loss: 3.3469722002668454\t lr: 0.001\n",
      "Epoch [23/100]\t Validation Loss: 3.4305981171282034\t lr: 0.001\n",
      "Epoch [23/100]\t\n",
      "Epoch [24/100]\t Training Loss: 3.3002611804191413\t lr: 0.001\n",
      "Epoch [24/100]\t Validation Loss: 3.4047185952150367\t lr: 0.001\n",
      "Epoch [24/100]\t\n",
      "Epoch [25/100]\t Training Loss: 3.253246079930259\t lr: 0.001\n",
      "Epoch [25/100]\t Validation Loss: 3.3684580839132963\t lr: 0.001\n",
      "Epoch [25/100]\t\n",
      "Epoch [26/100]\t Training Loss: 3.208637347306742\t lr: 0.001\n",
      "Epoch [26/100]\t Validation Loss: 3.323760714712022\t lr: 0.001\n",
      "Epoch [26/100]\t\n",
      "Epoch [27/100]\t Training Loss: 3.157870787793718\t lr: 0.001\n",
      "Epoch [27/100]\t Validation Loss: 3.2899766873709764\t lr: 0.001\n",
      "Epoch [27/100]\t\n",
      "Epoch [28/100]\t Training Loss: 3.1059645466182544\t lr: 0.001\n",
      "Epoch [28/100]\t Validation Loss: 3.2698874503751343\t lr: 0.001\n",
      "Epoch [28/100]\t\n",
      "Epoch [29/100]\t Training Loss: 3.058620604102874\t lr: 0.001\n",
      "Epoch [29/100]\t Validation Loss: 3.230181482773793\t lr: 0.001\n",
      "Epoch [29/100]\t\n",
      "Epoch [30/100]\t Training Loss: 3.0124239695956336\t lr: 0.001\n",
      "Epoch [30/100]\t Validation Loss: 3.209309327451489\t lr: 0.001\n",
      "Epoch [30/100]\t\n",
      "Epoch [31/100]\t Training Loss: 2.9646986530869817\t lr: 0.001\n",
      "Epoch [31/100]\t Validation Loss: 3.1722436071951177\t lr: 0.001\n",
      "Epoch [31/100]\t\n",
      "Epoch [32/100]\t Training Loss: 2.9250856330022788\t lr: 0.001\n",
      "Epoch [32/100]\t Validation Loss: 3.1611856931372535\t lr: 0.001\n",
      "Epoch [32/100]\t\n",
      "Epoch [33/100]\t Training Loss: 2.880951886286821\t lr: 0.001\n",
      "Epoch [33/100]\t Validation Loss: 3.1481083586246155\t lr: 0.001\n",
      "Epoch [33/100]\t\n",
      "Epoch [34/100]\t Training Loss: 2.832452836853769\t lr: 0.001\n",
      "Epoch [34/100]\t Validation Loss: 3.1183502281768414\t lr: 0.001\n",
      "Epoch [34/100]\t\n",
      "Epoch [35/100]\t Training Loss: 2.795910210865538\t lr: 0.001\n",
      "Epoch [35/100]\t Validation Loss: 3.1044143302531184\t lr: 0.001\n",
      "Epoch [35/100]\t\n",
      "Epoch [36/100]\t Training Loss: 2.750610238145989\t lr: 0.001\n",
      "Epoch [36/100]\t Validation Loss: 3.1091131681128394\t lr: 0.001\n",
      "Epoch [36/100]\t\n",
      "Epoch [37/100]\t Training Loss: 2.7050844214456466\t lr: 0.001\n",
      "Epoch [37/100]\t Validation Loss: 3.101943833918511\t lr: 0.001\n",
      "Epoch [37/100]\t\n",
      "Epoch [38/100]\t Training Loss: 2.66085533900639\t lr: 0.001\n",
      "Epoch [38/100]\t Validation Loss: 3.109672211393525\t lr: 0.001\n",
      "Epoch [38/100]\t\n",
      "Epoch [39/100]\t Training Loss: 2.6183747708645013\t lr: 0.001\n",
      "Epoch [39/100]\t Validation Loss: 3.0718295845804335\t lr: 0.001\n",
      "Epoch [39/100]\t\n",
      "Epoch [40/100]\t Training Loss: 2.575298997142431\t lr: 0.001\n",
      "Epoch [40/100]\t Validation Loss: 3.1040370313427115\t lr: 0.001\n",
      "Epoch [40/100]\t\n",
      "Epoch [41/100]\t Training Loss: 2.5266676162514847\t lr: 0.001\n",
      "Epoch [41/100]\t Validation Loss: 3.0641361399541927\t lr: 0.001\n",
      "Epoch [41/100]\t\n",
      "Epoch [42/100]\t Training Loss: 2.4798503769633107\t lr: 0.001\n",
      "Epoch [42/100]\t Validation Loss: 3.123847484588623\t lr: 0.001\n",
      "Epoch [42/100]\t\n",
      "Epoch [43/100]\t Training Loss: 2.4375605412456385\t lr: 0.001\n",
      "Epoch [43/100]\t Validation Loss: 3.0868087177035175\t lr: 0.001\n",
      "Epoch [43/100]\t\n",
      "Epoch [44/100]\t Training Loss: 2.382514052073974\t lr: 0.001\n",
      "Epoch [44/100]\t Validation Loss: 3.095665165140659\t lr: 0.001\n",
      "Epoch [44/100]\t\n",
      "Epoch [45/100]\t Training Loss: 2.336332401046363\t lr: 0.001\n",
      "Epoch [45/100]\t Validation Loss: 3.1069844101048725\t lr: 0.001\n",
      "Epoch [45/100]\t\n",
      "Epoch [46/100]\t Training Loss: 2.2816110632913498\t lr: 0.001\n",
      "Epoch [46/100]\t Validation Loss: 3.0969807799858384\t lr: 0.001\n",
      "Epoch [46/100]\t\n",
      "Epoch [47/100]\t Training Loss: 2.2332422727209225\t lr: 0.001\n",
      "Epoch [47/100]\t Validation Loss: 3.141486466685428\t lr: 0.001\n",
      "Epoch [47/100]\t\n",
      "Epoch [48/100]\t Training Loss: 2.191947266268913\t lr: 0.001\n",
      "Epoch [48/100]\t Validation Loss: 3.150306372702876\t lr: 0.001\n",
      "Epoch [48/100]\t\n",
      "Epoch [49/100]\t Training Loss: 2.1348268738792986\t lr: 0.001\n",
      "Epoch [49/100]\t Validation Loss: 3.1701495677609985\t lr: 0.001\n",
      "Epoch [49/100]\t\n",
      "Epoch [50/100]\t Training Loss: 2.08289117459446\t lr: 0.001\n",
      "Epoch [50/100]\t Validation Loss: 3.2015232436264616\t lr: 0.001\n",
      "Epoch [50/100]\t\n",
      "Epoch [51/100]\t Training Loss: 2.0196809201594204\t lr: 0.001\n",
      "Epoch [51/100]\t Validation Loss: 3.2198758728896517\t lr: 0.001\n",
      "Epoch [51/100]\t\n",
      "Epoch [52/100]\t Training Loss: 1.9674044548702971\t lr: 0.001\n",
      "Epoch [52/100]\t Validation Loss: 3.2495137453079224\t lr: 0.001\n",
      "Epoch [52/100]\t\n",
      "Epoch [53/100]\t Training Loss: 1.9028460644090268\t lr: 0.001\n",
      "Epoch [53/100]\t Validation Loss: 3.3214579171772245\t lr: 0.001\n",
      "Epoch [53/100]\t\n",
      "Epoch [54/100]\t Training Loss: 1.85076347549858\t lr: 0.001\n",
      "Epoch [54/100]\t Validation Loss: 3.318539356883568\t lr: 0.001\n",
      "Epoch [54/100]\t\n",
      "Epoch [55/100]\t Training Loss: 1.7842830687837528\t lr: 0.001\n",
      "Epoch [55/100]\t Validation Loss: 3.3158298927017404\t lr: 0.001\n",
      "Epoch [55/100]\t\n",
      "Epoch [56/100]\t Training Loss: 1.7268181883770486\t lr: 0.001\n",
      "Epoch [56/100]\t Validation Loss: 3.4341887914681735\t lr: 0.001\n",
      "Epoch [56/100]\t\n",
      "Epoch [57/100]\t Training Loss: 1.670973265567399\t lr: 0.001\n",
      "Epoch [57/100]\t Validation Loss: 3.4538007929355286\t lr: 0.001\n",
      "Epoch [57/100]\t\n",
      "Epoch [58/100]\t Training Loss: 1.604852251079686\t lr: 0.001\n",
      "Epoch [58/100]\t Validation Loss: 3.484685303289679\t lr: 0.001\n",
      "Epoch [58/100]\t\n",
      "Epoch [59/100]\t Training Loss: 1.5404918154182337\t lr: 0.001\n",
      "Epoch [59/100]\t Validation Loss: 3.6167402267456055\t lr: 0.001\n",
      "Epoch [59/100]\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100]\t Training Loss: 1.4697922270011414\t lr: 0.001\n",
      "Epoch [60/100]\t Validation Loss: 3.7062741020057777\t lr: 0.001\n",
      "Epoch [60/100]\t\n",
      "Epoch [61/100]\t Training Loss: 1.4087214494300315\t lr: 0.001\n",
      "Epoch [61/100]\t Validation Loss: 3.7080571711817876\t lr: 0.001\n",
      "Epoch [61/100]\t\n",
      "Epoch [62/100]\t Training Loss: 1.3511397003212853\t lr: 0.001\n",
      "Epoch [62/100]\t Validation Loss: 3.806745447689974\t lr: 0.001\n",
      "Epoch [62/100]\t\n",
      "Epoch [63/100]\t Training Loss: 1.2907610896908108\t lr: 0.001\n",
      "Epoch [63/100]\t Validation Loss: 3.906908835036845\t lr: 0.001\n",
      "Epoch [63/100]\t\n",
      "Epoch [64/100]\t Training Loss: 1.2256876899458258\t lr: 0.001\n",
      "Epoch [64/100]\t Validation Loss: 3.91516763047327\t lr: 0.001\n",
      "Epoch [64/100]\t\n",
      "Epoch [65/100]\t Training Loss: 1.1737164242188338\t lr: 0.001\n",
      "Epoch [65/100]\t Validation Loss: 4.077345169043239\t lr: 0.001\n",
      "Epoch [65/100]\t\n",
      "Epoch [66/100]\t Training Loss: 1.1084116261328578\t lr: 0.001\n",
      "Epoch [66/100]\t Validation Loss: 4.163233548780031\t lr: 0.001\n",
      "Epoch [66/100]\t\n",
      "Epoch [67/100]\t Training Loss: 1.0433625456927074\t lr: 0.001\n",
      "Epoch [67/100]\t Validation Loss: 4.221014424215389\t lr: 0.001\n",
      "Epoch [67/100]\t\n",
      "Epoch [68/100]\t Training Loss: 0.9800315993216336\t lr: 0.001\n",
      "Epoch [68/100]\t Validation Loss: 4.357313940796671\t lr: 0.001\n",
      "Epoch [68/100]\t\n",
      "Epoch [69/100]\t Training Loss: 0.9240815730960777\t lr: 0.001\n",
      "Epoch [69/100]\t Validation Loss: 4.418749133242836\t lr: 0.001\n",
      "Epoch [69/100]\t\n",
      "Epoch [70/100]\t Training Loss: 0.8620460752940848\t lr: 0.001\n",
      "Epoch [70/100]\t Validation Loss: 4.59205173540719\t lr: 0.001\n",
      "Epoch [70/100]\t\n",
      "Epoch [71/100]\t Training Loss: 0.8252111673355103\t lr: 0.001\n",
      "Epoch [71/100]\t Validation Loss: 4.724410594264163\t lr: 0.001\n",
      "Epoch [71/100]\t\n",
      "Epoch [72/100]\t Training Loss: 0.7716079662218118\t lr: 0.001\n",
      "Epoch [72/100]\t Validation Loss: 4.877865100208717\t lr: 0.001\n",
      "Epoch [72/100]\t\n",
      "Epoch [73/100]\t Training Loss: 0.7130513934375685\t lr: 0.001\n",
      "Epoch [73/100]\t Validation Loss: 4.948579770100268\t lr: 0.001\n",
      "Epoch [73/100]\t\n",
      "Epoch [74/100]\t Training Loss: 0.6728338966589145\t lr: 0.001\n",
      "Epoch [74/100]\t Validation Loss: 5.1119573086122925\t lr: 0.001\n",
      "Epoch [74/100]\t\n",
      "Epoch [75/100]\t Training Loss: 0.6328784317311729\t lr: 0.001\n",
      "Epoch [75/100]\t Validation Loss: 5.203765905356105\t lr: 0.001\n",
      "Epoch [75/100]\t\n",
      "Epoch [76/100]\t Training Loss: 0.5781508122411225\t lr: 0.001\n",
      "Epoch [76/100]\t Validation Loss: 5.351467066173312\t lr: 0.001\n",
      "Epoch [76/100]\t\n",
      "Epoch [77/100]\t Training Loss: 0.5259201131056032\t lr: 0.001\n",
      "Epoch [77/100]\t Validation Loss: 5.459846774234047\t lr: 0.001\n",
      "Epoch [77/100]\t\n",
      "Epoch [78/100]\t Training Loss: 0.5000769547031968\t lr: 0.001\n",
      "Epoch [78/100]\t Validation Loss: 5.635331413413905\t lr: 0.001\n",
      "Epoch [78/100]\t\n",
      "Epoch [79/100]\t Training Loss: 0.4548648984916985\t lr: 0.001\n",
      "Epoch [79/100]\t Validation Loss: 5.87708173824262\t lr: 0.001\n",
      "Epoch [79/100]\t\n",
      "Epoch [80/100]\t Training Loss: 0.43245227650150925\t lr: 0.001\n",
      "Epoch [80/100]\t Validation Loss: 5.932988921298256\t lr: 0.001\n",
      "Epoch [80/100]\t\n",
      "Epoch [81/100]\t Training Loss: 0.42207059492845367\t lr: 0.001\n",
      "Epoch [81/100]\t Validation Loss: 6.0819423953189125\t lr: 0.001\n",
      "Epoch [81/100]\t\n",
      "Epoch [82/100]\t Training Loss: 0.37213738594213713\t lr: 0.001\n",
      "Epoch [82/100]\t Validation Loss: 6.158928605574596\t lr: 0.001\n",
      "Epoch [82/100]\t\n",
      "Epoch [83/100]\t Training Loss: 0.3355276611302515\t lr: 0.001\n",
      "Epoch [83/100]\t Validation Loss: 6.303440118137794\t lr: 0.001\n",
      "Epoch [83/100]\t\n",
      "Epoch [84/100]\t Training Loss: 0.3085986918119518\t lr: 0.001\n",
      "Epoch [84/100]\t Validation Loss: 6.41536223133908\t lr: 0.001\n",
      "Epoch [84/100]\t\n",
      "Epoch [85/100]\t Training Loss: 0.280470590838386\t lr: 0.001\n",
      "Epoch [85/100]\t Validation Loss: 6.644153757940365\t lr: 0.001\n",
      "Epoch [85/100]\t\n",
      "Epoch [86/100]\t Training Loss: 0.2844636311845096\t lr: 0.001\n",
      "Epoch [86/100]\t Validation Loss: 6.736958721016027\t lr: 0.001\n",
      "Epoch [86/100]\t\n",
      "Epoch [87/100]\t Training Loss: 0.2714253316068893\t lr: 0.001\n",
      "Epoch [87/100]\t Validation Loss: 6.916362152823919\t lr: 0.001\n",
      "Epoch [87/100]\t\n",
      "Epoch [88/100]\t Training Loss: 0.2731424344660681\t lr: 0.001\n",
      "Epoch [88/100]\t Validation Loss: 7.14577447915379\t lr: 0.001\n",
      "Epoch [88/100]\t\n",
      "Epoch [89/100]\t Training Loss: 0.2727693478050439\t lr: 0.001\n",
      "Epoch [89/100]\t Validation Loss: 7.087231726586064\t lr: 0.001\n",
      "Epoch [89/100]\t\n",
      "Epoch [90/100]\t Training Loss: 0.25801937377361384\t lr: 0.001\n",
      "Epoch [90/100]\t Validation Loss: 7.1919863193849976\t lr: 0.001\n",
      "Epoch [90/100]\t\n",
      "Epoch [91/100]\t Training Loss: 0.2226672066027856\t lr: 0.001\n",
      "Epoch [91/100]\t Validation Loss: 7.330273260044146\t lr: 0.001\n",
      "Epoch [91/100]\t\n",
      "Epoch [92/100]\t Training Loss: 0.1722302598226101\t lr: 0.001\n",
      "Epoch [92/100]\t Validation Loss: 7.447021381764472\t lr: 0.001\n",
      "Epoch [92/100]\t\n",
      "Epoch [93/100]\t Training Loss: 0.12545517338511278\t lr: 0.001\n",
      "Epoch [93/100]\t Validation Loss: 7.6527917475640015\t lr: 0.001\n",
      "Epoch [93/100]\t\n",
      "Epoch [94/100]\t Training Loss: 0.10217207807409184\t lr: 0.001\n",
      "Epoch [94/100]\t Validation Loss: 7.662009824680377\t lr: 0.001\n",
      "Epoch [94/100]\t\n",
      "Epoch [95/100]\t Training Loss: 0.06572085080663566\t lr: 0.001\n",
      "Epoch [95/100]\t Validation Loss: 7.880970152118538\t lr: 0.001\n",
      "Epoch [95/100]\t\n",
      "Epoch [96/100]\t Training Loss: 0.041898782214964445\t lr: 0.001\n",
      "Epoch [96/100]\t Validation Loss: 7.796953436694568\t lr: 0.001\n",
      "Epoch [96/100]\t\n",
      "Epoch [97/100]\t Training Loss: 0.027499673632033112\t lr: 0.001\n",
      "Epoch [97/100]\t Validation Loss: 7.895880288715604\t lr: 0.001\n",
      "Epoch [97/100]\t\n",
      "Epoch [98/100]\t Training Loss: 0.023962104180946832\t lr: 0.001\n",
      "Epoch [98/100]\t Validation Loss: 7.955832378773749\t lr: 0.001\n",
      "Epoch [98/100]\t\n",
      "Epoch [99/100]\t Training Loss: 0.019660030567870877\t lr: 0.001\n",
      "Epoch [99/100]\t Validation Loss: 8.019319027285032\t lr: 0.001\n",
      "Epoch [99/100]\t\n",
      "Epoch [100/100]\t Training Loss: 0.01594561688921145\t lr: 0.001\n",
      "Epoch [100/100]\t Validation Loss: 8.140459730655332\t lr: 0.001\n",
      "Epoch [99/100]\t Time Taken: 11.45328366359075 minutes\n"
     ]
    }
   ],
   "source": [
    "train(net_ori, train_loader, train_criterion,val_loader, val_criterion, \n",
    "              optimizer, 100, writer, \n",
    "                 PATH, False, None, device=\"cuda\",\n",
    "                  n_steps_show=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c5deda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 27 %\n"
     ]
    }
   ],
   "source": [
    "#test clean accuray\n",
    "test(val_loader, net_ori)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1784277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD\n",
      "[Model loaded]\n",
      "Acc: 1.17 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"PGD\")\n",
    "pgd_attack_transfer = generate_adv(net_ori, \"pgd\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "pgd_acc = test_attack(val_loader, pgd_attack_transfer, net_ori, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca72f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgsm\n",
      "[Model loaded]\n",
      "Acc: 2.56 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"fgsm\")\n",
    "fgsm_attack_transfer = generate_adv(net_ori, \"fgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "fgsm_acc = test_attack(val_loader, fgsm_attack_transfer, net_ori, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e52ce96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nifgsm\n",
      "[Model loaded]\n",
      "Acc: 1.77 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"nifgsm\")\n",
    "nifgsm_attack_transfer = generate_adv(net_ori, \"nifgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "nifgsm_acc = test_attack(val_loader, nifgsm_attack_transfer, net_ori, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "980c3acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmifgsm\n",
      "[Model loaded]\n",
      "Acc: 1.39 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"vmifgsm\")\n",
    "vmifgsm_attack_transfer = generate_adv(net_ori, \"vmifgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "vmifgsm_acc = test_attack(val_loader, vmifgsm_attack_transfer, net_ori, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb31099",
   "metadata": {},
   "source": [
    "# Adversarial Training on PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eeafbdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 6.8848e-01,  7.1635e-01,  8.7562e-01,  7.3762e-01,  7.1002e-01],\n",
      "          [ 9.6923e-01,  9.3511e-01,  1.1260e+00,  1.0334e+00,  8.8704e-01],\n",
      "          [ 1.0669e+00,  1.2664e+00,  1.2851e+00,  1.2665e+00,  1.0994e+00],\n",
      "          [ 8.0321e-01,  1.0094e+00,  1.3014e+00,  1.2376e+00,  1.0452e+00],\n",
      "          [ 6.0307e-01,  8.9163e-01,  1.0605e+00,  1.0001e+00,  7.4786e-01]],\n",
      "\n",
      "         [[-4.6893e-02,  7.6766e-02,  1.7172e-01,  7.9294e-02,  1.1587e-01],\n",
      "          [ 2.4507e-01,  3.0737e-01,  3.8427e-01,  3.0031e-01,  2.2433e-01],\n",
      "          [ 2.9689e-01,  4.6346e-01,  4.3376e-01,  2.7984e-01,  3.9987e-01],\n",
      "          [ 3.6214e-02,  2.0558e-01,  3.0059e-01,  2.3015e-01,  2.2317e-01],\n",
      "          [-1.4972e-01,  9.5225e-02,  8.3450e-02,  1.7958e-01,  1.7341e-01]],\n",
      "\n",
      "         [[-3.9027e-01, -3.6181e-01, -3.4574e-01, -1.9384e-01, -2.1003e-01],\n",
      "          [-1.0891e-01,  6.7987e-02,  3.7039e-02, -1.8180e-02, -1.7407e-01],\n",
      "          [-1.2289e-01,  1.4269e-01,  1.9234e-01,  2.5228e-02,  3.1312e-02],\n",
      "          [-6.1923e-02,  1.3957e-01,  7.6027e-02,  1.9176e-01,  1.2974e-01],\n",
      "          [-3.1519e-01, -1.9724e-01, -1.4682e-01, -1.3390e-02, -7.3150e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1145e-01, -8.3311e-02,  7.2795e-02,  1.9872e-02,  2.1379e-01],\n",
      "          [ 1.8984e-01,  2.2470e-01,  3.3030e-01,  3.6725e-01,  3.2856e-01],\n",
      "          [ 7.0925e-02,  2.4904e-01,  1.5939e-01,  1.3230e-01,  1.6793e-01],\n",
      "          [-1.2891e-01, -1.1778e-01, -2.7705e-01, -2.4411e-01, -2.3502e-01],\n",
      "          [-3.1559e-01, -2.8601e-01, -3.5488e-01, -3.5852e-01, -3.5078e-01]],\n",
      "\n",
      "         [[-6.8492e-02, -1.0494e-01,  6.4262e-03,  5.1212e-02,  9.6608e-02],\n",
      "          [ 2.2563e-01,  2.9311e-01,  2.6156e-01,  2.4380e-01,  1.0133e-01],\n",
      "          [ 1.7571e-01,  9.8767e-02,  1.0297e-01,  2.5496e-01, -1.6406e-02],\n",
      "          [-1.5906e-01, -4.4609e-02, -2.2650e-01, -1.4224e-01, -2.3859e-01],\n",
      "          [-2.3569e-01, -2.6594e-01, -3.6366e-01, -3.2661e-01, -3.1507e-01]],\n",
      "\n",
      "         [[ 7.0437e-02, -4.8038e-02,  5.3689e-02,  8.4594e-02,  2.3535e-02],\n",
      "          [ 2.9296e-02,  8.8913e-02,  1.8874e-01,  3.1467e-01,  1.1120e-01],\n",
      "          [ 9.0218e-02,  1.6035e-01,  1.2968e-01,  2.6962e-01,  1.6701e-01],\n",
      "          [ 1.1819e-02, -5.9728e-02, -4.8134e-02, -2.3463e-01, -9.1228e-02],\n",
      "          [-2.1295e-01, -1.2632e-01, -1.8790e-01, -3.5724e-01, -2.0511e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3880e-02,  1.5082e-01,  1.0295e-01,  2.2933e-01,  1.4634e-01],\n",
      "          [ 1.3466e-01,  3.5216e-01,  4.7903e-01,  2.2437e-01,  2.4900e-01],\n",
      "          [ 3.0001e-01,  5.2965e-01,  5.6147e-01,  5.2569e-01,  2.8969e-01],\n",
      "          [ 3.1936e-01,  2.9519e-01,  5.3635e-01,  4.5937e-01,  3.2962e-01],\n",
      "          [-3.7246e-03,  1.9684e-01,  1.6285e-01,  1.4332e-01,  1.0536e-01]],\n",
      "\n",
      "         [[-1.0882e-01,  1.6446e-02,  1.3083e-01, -1.0968e-01, -1.6136e-01],\n",
      "          [ 1.9651e-01,  1.3587e-01,  2.3369e-01,  1.1701e-01,  2.5301e-02],\n",
      "          [ 2.0613e-01,  2.1312e-01,  4.1365e-01,  2.9587e-01,  2.2789e-02],\n",
      "          [ 1.0717e-01,  7.4418e-02,  1.6472e-01,  2.0214e-01,  1.1695e-01],\n",
      "          [-9.8105e-02, -1.8044e-02,  1.2902e-01,  3.0045e-02, -1.1769e-01]],\n",
      "\n",
      "         [[ 6.6626e-01,  7.6791e-01,  9.0340e-01,  9.0356e-01,  6.5558e-01],\n",
      "          [ 9.3690e-01,  1.0297e+00,  1.1299e+00,  1.0830e+00,  8.6284e-01],\n",
      "          [ 9.3047e-01,  1.1363e+00,  1.1773e+00,  1.0070e+00,  8.8675e-01],\n",
      "          [ 8.7208e-01,  1.0470e+00,  9.8787e-01,  8.8723e-01,  8.8498e-01],\n",
      "          [ 6.3294e-01,  7.4036e-01,  8.5254e-01,  8.4694e-01,  7.3819e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.3685e-03,  9.3489e-02, -1.1408e-01,  6.1729e-02, -3.2891e-02],\n",
      "          [-2.5574e-02, -9.4116e-02, -2.7239e-02, -1.0865e-02, -1.9074e-02],\n",
      "          [ 7.5433e-03, -8.3537e-02, -8.9152e-02, -1.5600e-02, -8.3835e-02],\n",
      "          [-5.5693e-02, -5.0586e-02,  3.6615e-02, -3.2275e-02, -4.9304e-02],\n",
      "          [ 4.1573e-02,  2.1667e-02, -8.7752e-02, -1.1188e-02,  1.2610e-02]],\n",
      "\n",
      "         [[ 5.3799e-02,  1.1487e-02,  1.1219e-02,  3.8733e-02, -5.3037e-02],\n",
      "          [ 2.9074e-03,  6.1122e-02,  4.6598e-02,  5.1745e-02,  6.8219e-03],\n",
      "          [-1.0399e-01,  1.1636e-03, -9.2886e-02,  9.3520e-02, -3.8512e-02],\n",
      "          [ 2.8995e-02, -6.2940e-02,  6.0799e-03, -4.3929e-02,  6.3567e-02],\n",
      "          [ 9.7704e-02, -1.2066e-02, -7.7972e-02, -3.8750e-02,  7.8082e-02]],\n",
      "\n",
      "         [[-7.4044e-02,  5.0664e-02,  5.0844e-02, -5.6705e-02,  1.1050e-01],\n",
      "          [-6.8261e-02,  9.3936e-03,  5.6549e-02,  4.4457e-03,  1.7331e-02],\n",
      "          [ 4.0151e-02,  4.3922e-02, -9.2459e-03,  1.1187e-01,  9.0139e-02],\n",
      "          [-5.3589e-02,  1.0475e-01, -4.7053e-02, -2.6837e-02, -9.3690e-02],\n",
      "          [-8.9152e-02,  5.3813e-02, -1.5523e-02, -7.1307e-03, -5.7890e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.5596e-02,  3.0873e-02, -8.9165e-02,  2.8435e-02, -1.3179e-01],\n",
      "          [ 1.5620e-01,  2.3906e-01,  2.5917e-01,  1.3128e-01, -4.0638e-02],\n",
      "          [ 1.7617e-01,  2.3250e-01,  2.4692e-01,  1.3443e-01,  2.0710e-01],\n",
      "          [-4.7150e-02,  2.3363e-01,  2.9182e-01,  9.2678e-02,  5.6494e-02],\n",
      "          [-6.6640e-02,  1.4076e-01,  1.7118e-01,  7.5866e-02, -6.6414e-02]],\n",
      "\n",
      "         [[ 6.3079e-01,  7.5666e-01,  8.2678e-01,  7.1994e-01,  5.9498e-01],\n",
      "          [ 8.7474e-01,  9.8259e-01,  1.0080e+00,  8.2335e-01,  7.9641e-01],\n",
      "          [ 8.2369e-01,  1.0835e+00,  1.0943e+00,  9.9691e-01,  8.6943e-01],\n",
      "          [ 6.7928e-01,  8.2230e-01,  9.4906e-01,  9.2194e-01,  7.8490e-01],\n",
      "          [ 5.3541e-01,  8.2999e-01,  7.5488e-01,  6.4358e-01,  6.5163e-01]],\n",
      "\n",
      "         [[ 4.9068e-01,  5.8237e-01,  4.6766e-01,  4.6352e-01,  3.5595e-01],\n",
      "          [ 6.2648e-01,  7.1020e-01,  6.8073e-01,  6.9643e-01,  4.6295e-01],\n",
      "          [ 5.8294e-01,  8.0413e-01,  7.9806e-01,  7.4661e-01,  6.2706e-01],\n",
      "          [ 2.9013e-01,  5.8323e-01,  6.0710e-01,  5.2173e-01,  5.8779e-01],\n",
      "          [ 2.5268e-01,  3.3952e-01,  4.7674e-01,  4.2986e-01,  3.7328e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.3603e-02, -1.1117e-01,  8.6127e-02,  8.2465e-02,  1.0110e-01],\n",
      "          [-1.1426e-01, -3.3620e-02, -1.4783e-01,  4.0217e-02,  6.4291e-02],\n",
      "          [ 3.1854e-01,  1.4572e-01,  1.3012e-01,  1.2937e-01,  3.2809e-01],\n",
      "          [ 5.3581e-01,  5.0210e-01,  5.6726e-01,  4.1537e-01,  3.3030e-01],\n",
      "          [ 5.6128e-01,  5.2394e-01,  5.7165e-01,  5.4384e-01,  3.7628e-01]],\n",
      "\n",
      "         [[-4.7748e-01, -5.8056e-01, -4.4669e-01, -4.0306e-01, -3.5091e-01],\n",
      "          [-5.0384e-01, -5.8863e-01, -6.1024e-01, -4.1552e-01, -1.7953e-01],\n",
      "          [-2.9619e-01, -2.2709e-01, -2.9545e-01, -2.1671e-01, -7.6668e-02],\n",
      "          [ 8.3397e-02, -7.0119e-02, -6.4654e-02, -2.1401e-02, -7.8243e-02],\n",
      "          [ 1.6348e-01,  1.9405e-02,  1.3058e-01,  1.0029e-01,  1.2579e-01]],\n",
      "\n",
      "         [[-1.5643e-01, -3.3477e-01, -3.2424e-01, -1.3808e-01, -4.7223e-02],\n",
      "          [-2.9055e-01, -4.0182e-01, -2.0490e-01, -3.0845e-01, -3.9713e-02],\n",
      "          [ 1.9508e-04,  4.4568e-02, -9.9594e-02,  1.1552e-01,  2.2031e-02],\n",
      "          [ 3.4137e-01,  3.1919e-01,  3.0604e-01,  2.6540e-01,  2.7227e-01],\n",
      "          [ 4.0126e-01,  2.6393e-01,  2.4564e-01,  2.6837e-01,  4.1168e-01]]]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.8455,  0.6285, -1.3379, -0.0449,  0.6607,  0.2548], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-1.9136e-01, -2.1551e-01, -1.8147e-01, -2.9432e-02,  7.2791e-02],\n",
      "          [ 2.5704e-02, -3.4393e-02,  2.2917e-02,  1.1381e-01,  1.0657e-01],\n",
      "          [ 1.0094e-01,  7.8757e-02,  9.5553e-02,  3.3031e-02,  5.0084e-03],\n",
      "          [-2.1806e-01, -2.7800e-01, -1.2527e-01, -2.3994e-02,  4.3314e-02],\n",
      "          [-1.7225e-01, -3.1177e-01, -1.3715e-01,  7.0778e-02,  2.7347e-01]],\n",
      "\n",
      "         [[ 3.9900e-02,  9.9821e-02, -1.2449e-02,  5.1525e-03,  9.6751e-02],\n",
      "          [-2.3796e-02,  7.7984e-02,  4.8580e-03,  1.2199e-01,  5.7153e-02],\n",
      "          [ 1.1842e-01,  1.0148e-01,  5.2981e-02,  1.1544e-01,  1.2141e-01],\n",
      "          [ 1.9065e-01,  7.8777e-02,  3.6849e-02, -6.2321e-03, -5.9810e-02],\n",
      "          [ 6.7245e-02,  5.3009e-02, -1.4876e-02,  5.3216e-02,  9.1171e-02]],\n",
      "\n",
      "         [[-1.3383e-01, -1.2575e-01,  1.2131e-01,  2.7553e-01,  4.6415e-01],\n",
      "          [ 1.3315e-01,  3.5727e-02,  1.1160e-01,  3.4924e-01,  3.6141e-01],\n",
      "          [ 1.8652e-01,  1.9936e-01,  2.1019e-01,  2.0930e-01,  2.9285e-01],\n",
      "          [-1.6339e-01, -1.9434e-01, -4.6889e-02,  1.1261e-01,  2.6548e-01],\n",
      "          [-3.4279e-01, -3.1181e-01, -1.2414e-01,  2.4984e-01,  3.0711e-01]],\n",
      "\n",
      "         [[ 7.8769e-02,  5.1520e-03, -8.7893e-03,  3.4898e-02, -6.2736e-02],\n",
      "          [ 1.0862e-03,  1.3829e-02, -4.2447e-02, -8.0382e-02,  1.5203e-02],\n",
      "          [ 5.0197e-03,  3.9301e-02, -8.1834e-03, -4.7250e-02, -5.1963e-02],\n",
      "          [ 4.1831e-02, -7.8120e-02, -7.7976e-02, -7.8536e-02,  6.2826e-02],\n",
      "          [ 3.9980e-02,  5.0455e-03, -6.7941e-02, -3.0988e-02,  4.0070e-02]],\n",
      "\n",
      "         [[-1.8282e-01, -1.2835e-01, -4.3311e-02,  1.6073e-01,  3.6895e-01],\n",
      "          [-3.5527e-02, -1.6084e-01,  4.4350e-02,  7.7512e-02,  3.1594e-01],\n",
      "          [-1.1730e-02,  4.5437e-02,  4.4157e-02,  3.9944e-02,  4.5442e-02],\n",
      "          [-2.7303e-01, -2.4491e-01, -1.6499e-01,  8.2526e-02,  6.5094e-02],\n",
      "          [-5.2420e-01, -4.9147e-01, -3.0854e-01,  8.9184e-02,  2.8129e-01]],\n",
      "\n",
      "         [[ 1.6557e-01,  1.9081e-01,  1.2337e-01,  1.3692e-01,  1.4192e-01],\n",
      "          [ 1.3649e-01,  1.4793e-01,  1.9360e-01,  1.5761e-01,  3.7893e-02],\n",
      "          [ 3.8226e-02,  1.1796e-01,  1.5415e-01,  4.2682e-03,  9.0330e-03],\n",
      "          [ 1.2304e-02,  9.9718e-02,  1.1203e-01,  5.2098e-02,  1.8967e-01],\n",
      "          [ 1.3924e-01,  2.1378e-01,  2.3117e-01,  2.1639e-01,  1.1413e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.9274e-01, -1.3122e-01,  4.7533e-02,  7.0164e-02,  1.8433e-01],\n",
      "          [-3.2860e-01, -6.1336e-02,  8.0311e-02,  3.1805e-01,  3.3457e-01],\n",
      "          [-2.8730e-01, -3.7874e-02,  2.0224e-01,  4.0637e-01,  4.4109e-01],\n",
      "          [-3.9774e-01, -1.0528e-01,  3.9792e-02,  3.4765e-01,  3.3565e-01],\n",
      "          [-3.9483e-01, -1.1105e-01,  4.6628e-03,  8.1714e-02,  9.2027e-02]],\n",
      "\n",
      "         [[-4.7211e-02, -1.0951e-01, -5.4004e-02, -1.3090e-02, -6.7269e-02],\n",
      "          [ 3.6032e-03, -1.1467e-01, -9.2353e-02, -9.7464e-02, -2.1460e-02],\n",
      "          [-3.3144e-02,  2.7194e-02, -1.4112e-02,  3.3649e-02, -2.0379e-02],\n",
      "          [-1.2526e-01,  2.2559e-02, -1.0702e-01, -8.1258e-02, -8.5413e-02],\n",
      "          [-7.1960e-02,  3.3239e-02, -6.6478e-02, -2.8403e-02, -4.2306e-02]],\n",
      "\n",
      "         [[ 1.1205e-01,  5.3814e-02,  5.5426e-02, -4.9743e-02, -7.7040e-02],\n",
      "          [ 1.6286e-01,  1.3882e-01, -5.2495e-02, -4.0213e-02, -1.8309e-01],\n",
      "          [ 1.9771e-01,  2.3536e-02, -2.4558e-02, -7.2435e-02, -2.2351e-01],\n",
      "          [ 2.4001e-01,  1.6390e-01, -1.7532e-02, -1.4307e-01, -1.5814e-01],\n",
      "          [ 2.8187e-01,  1.8765e-01,  1.0822e-01, -4.7768e-02, -2.9445e-02]],\n",
      "\n",
      "         [[ 2.9777e-02, -6.2987e-02,  1.1994e-02,  6.5517e-02, -3.9506e-02],\n",
      "          [-2.5566e-02,  2.3680e-02,  4.5071e-02, -2.6672e-02, -1.1763e-02],\n",
      "          [-5.4970e-02,  5.5704e-02, -2.5538e-02,  7.7437e-02, -5.3773e-02],\n",
      "          [ 7.7230e-02, -4.6158e-02, -1.4391e-02,  4.6554e-02,  7.6528e-02],\n",
      "          [-1.3848e-03, -4.5905e-02,  1.5754e-02, -5.3145e-02, -3.0294e-04]],\n",
      "\n",
      "         [[ 1.1014e-01,  5.4433e-02,  2.0896e-02,  3.9566e-02, -2.6481e-02],\n",
      "          [ 1.6259e-01, -5.9698e-02, -7.0762e-02, -1.2613e-01, -1.5138e-01],\n",
      "          [ 3.4337e-02,  3.9176e-02, -1.0641e-01, -2.9432e-01, -2.8759e-01],\n",
      "          [ 1.9702e-01, -3.2328e-02, -4.8821e-02, -2.6262e-01, -2.7047e-01],\n",
      "          [ 2.2297e-01,  3.7076e-02, -3.5273e-02, -1.5173e-01, -6.7246e-02]],\n",
      "\n",
      "         [[-7.7646e-02,  4.3451e-03, -1.1159e-01,  2.6220e-02,  3.2371e-02],\n",
      "          [-1.6192e-01, -6.2903e-02, -9.7549e-02, -4.0987e-03,  4.2861e-03],\n",
      "          [-9.2533e-02, -2.6402e-02, -5.6325e-02,  7.2790e-02,  8.6519e-02],\n",
      "          [-1.7288e-01, -2.9424e-02, -4.6870e-03,  1.0914e-01,  1.1989e-01],\n",
      "          [-6.7493e-02, -1.3537e-01, -9.1098e-02,  8.0638e-02,  2.8251e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8933e-01, -5.1801e-01, -6.5749e-01, -6.0580e-01, -3.2290e-01],\n",
      "          [-2.4642e-01, -2.3101e-01, -2.6577e-01, -2.1704e-01,  4.4415e-02],\n",
      "          [ 2.6230e-02,  1.7686e-01,  3.9987e-01,  3.0599e-01,  2.8327e-01],\n",
      "          [ 1.2957e-01,  4.0843e-01,  6.3961e-01,  5.5300e-01,  2.8635e-01],\n",
      "          [-1.9490e-02,  3.6129e-01,  4.4741e-01,  2.5518e-01,  1.2351e-01]],\n",
      "\n",
      "         [[ 3.0243e-02,  1.0328e-01,  4.1745e-02, -2.3624e-03,  8.5658e-02],\n",
      "          [-2.8127e-02,  4.7175e-03, -1.8809e-02, -9.4754e-03, -1.5979e-02],\n",
      "          [-6.7678e-02, -4.9530e-03,  2.8280e-02, -2.4006e-02,  1.4138e-02],\n",
      "          [ 3.4511e-02, -4.8360e-02,  4.5950e-02, -1.5414e-02,  3.6842e-02],\n",
      "          [ 8.9393e-02,  1.0711e-01,  6.9406e-02, -1.8703e-02,  9.3637e-04]],\n",
      "\n",
      "         [[-2.1580e-01, -5.4543e-01, -6.8168e-01, -6.0247e-01, -3.2445e-01],\n",
      "          [-2.3112e-01, -4.2618e-01, -4.5072e-01, -3.8934e-01, -1.9707e-01],\n",
      "          [-1.5408e-01,  8.9108e-02,  1.5420e-01,  2.2996e-01,  1.3032e-01],\n",
      "          [ 1.1399e-01,  3.6860e-01,  6.1283e-01,  5.4168e-01,  2.1498e-01],\n",
      "          [ 7.5350e-02,  2.2672e-01,  4.3439e-01,  3.8842e-01,  1.1706e-01]],\n",
      "\n",
      "         [[-1.9455e-02, -1.9926e-02,  3.7422e-02,  1.7864e-02, -1.8447e-02],\n",
      "          [-9.5418e-03,  7.5372e-02,  4.2439e-02, -1.6138e-02,  3.6161e-02],\n",
      "          [-5.2676e-02,  7.2730e-02, -8.0150e-04, -5.6663e-02,  3.9658e-02],\n",
      "          [-6.4710e-03, -1.1902e-02,  6.1409e-02,  3.1223e-02, -3.4759e-03],\n",
      "          [-3.5361e-02, -1.4713e-02, -7.2317e-02,  4.8659e-02,  4.7579e-03]],\n",
      "\n",
      "         [[-1.0581e-01, -4.7186e-01, -7.2266e-01, -6.9851e-01, -3.4628e-01],\n",
      "          [-2.5544e-01, -3.6795e-01, -5.0441e-01, -3.9254e-01, -1.1391e-01],\n",
      "          [-1.0982e-01,  1.1697e-01,  1.8012e-01,  2.8565e-01,  1.9430e-01],\n",
      "          [ 7.9147e-02,  4.4847e-01,  5.1926e-01,  5.0072e-01,  2.4362e-01],\n",
      "          [-4.4068e-02,  3.0189e-01,  4.2672e-01,  4.3473e-01,  2.0949e-01]],\n",
      "\n",
      "         [[-6.7733e-02, -1.3554e-01,  4.4606e-02,  7.4382e-02,  9.2566e-02],\n",
      "          [ 1.7853e-02,  1.1779e-01,  1.9238e-01,  2.4499e-01,  1.2516e-01],\n",
      "          [ 1.1676e-01,  8.4424e-02,  2.9834e-01,  2.0386e-01,  1.1520e-01],\n",
      "          [-1.6199e-02,  1.4350e-01,  1.2615e-01,  1.5571e-02,  8.3721e-02],\n",
      "          [-8.4086e-02, -6.6632e-02, -1.1070e-01, -6.0460e-02,  1.4988e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.5226e-02,  8.5331e-02,  2.5720e-01,  4.0326e-01,  3.3689e-01],\n",
      "          [-6.0024e-02, -1.4443e-01,  8.2648e-02,  1.7280e-01,  2.2208e-01],\n",
      "          [-8.4569e-02, -1.7192e-01, -1.9785e-01, -2.8031e-01, -2.7388e-01],\n",
      "          [ 8.6914e-02, -1.0228e-01, -3.3080e-01, -4.8262e-01, -3.9443e-01],\n",
      "          [ 2.3419e-01,  2.6337e-01,  7.7939e-02, -1.7329e-01, -1.8095e-01]],\n",
      "\n",
      "         [[ 1.6418e-01,  4.8660e-02,  1.1415e-01,  7.6219e-02,  1.8106e-01],\n",
      "          [ 9.8440e-02,  1.2804e-01,  2.6122e-01,  2.1078e-01,  1.6731e-01],\n",
      "          [ 9.9957e-02,  1.0211e-01,  1.7089e-01,  1.8307e-01,  1.3948e-01],\n",
      "          [-8.0882e-03, -3.8744e-02,  7.0081e-02,  1.1883e-01,  3.1028e-02],\n",
      "          [-1.0952e-03,  1.0593e-01, -2.5108e-02,  6.5108e-03, -1.3260e-02]],\n",
      "\n",
      "         [[-1.4507e-01, -1.4197e-02,  3.0490e-01,  5.6383e-01,  5.7286e-01],\n",
      "          [-1.3406e-01, -1.0434e-01,  1.1080e-01,  3.4993e-01,  2.7885e-01],\n",
      "          [ 5.3279e-02, -2.6273e-01, -2.1668e-01, -1.5482e-01, -1.5321e-01],\n",
      "          [ 2.1742e-01,  1.0547e-02, -2.7106e-01, -4.0153e-01, -4.4572e-01],\n",
      "          [ 3.2333e-01,  3.9052e-01,  2.0213e-01, -1.6403e-01, -9.3425e-02]],\n",
      "\n",
      "         [[-7.1418e-02, -7.1270e-02,  4.9047e-02,  5.7568e-02,  4.6688e-02],\n",
      "          [ 7.9284e-02,  5.4138e-02, -3.2717e-02, -2.5844e-02,  6.4829e-02],\n",
      "          [ 2.3782e-02, -1.8299e-03, -5.7371e-02, -3.0894e-02, -4.7789e-02],\n",
      "          [ 8.1042e-02, -6.8623e-02, -7.7063e-02,  7.3838e-02,  4.4462e-02],\n",
      "          [-2.8473e-02, -6.7516e-02, -3.7975e-03,  6.2221e-02,  7.4072e-02]],\n",
      "\n",
      "         [[-5.4122e-02, -8.2285e-03,  3.8576e-01,  6.0584e-01,  4.8155e-01],\n",
      "          [-2.1048e-01, -1.4934e-01,  1.7499e-01,  3.7678e-01,  3.9226e-01],\n",
      "          [-6.0248e-02, -3.0199e-01, -3.8568e-01, -2.4494e-01, -2.8402e-01],\n",
      "          [ 1.7502e-01,  2.8644e-03, -3.2528e-01, -6.0852e-01, -3.9699e-01],\n",
      "          [ 4.1176e-01,  3.6203e-01, -8.3459e-03, -2.5378e-01, -2.7649e-01]],\n",
      "\n",
      "         [[ 1.8096e-02,  1.2657e-01,  1.5045e-01,  9.5178e-02,  9.8994e-02],\n",
      "          [ 8.6541e-02,  2.2346e-02,  1.2298e-01,  4.7905e-02, -6.9280e-02],\n",
      "          [ 9.2851e-02,  8.2746e-02,  9.1907e-02,  3.0948e-02, -2.9559e-02],\n",
      "          [ 1.6744e-01,  1.5534e-01,  1.6366e-01,  1.6216e-01,  1.0158e-01],\n",
      "          [ 1.1879e-01,  8.0930e-02,  2.5359e-01,  2.1384e-01,  1.7482e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.7518e-02,  2.7450e-02, -8.5895e-02, -7.0106e-02, -7.0800e-02],\n",
      "          [-6.4184e-03, -2.4700e-02,  5.5930e-02, -4.2457e-02, -7.1006e-02],\n",
      "          [ 8.9488e-03, -3.2307e-02, -7.2253e-02,  4.6314e-02,  2.3073e-02],\n",
      "          [-5.1557e-02, -3.2726e-02, -5.5501e-02,  7.1409e-02, -3.8305e-02],\n",
      "          [-3.9913e-02,  5.6438e-03, -7.4562e-02, -4.3861e-02, -5.2993e-02]],\n",
      "\n",
      "         [[ 2.7356e-02, -6.0627e-02, -7.0830e-02,  3.7727e-02, -7.8344e-04],\n",
      "          [-3.3740e-02, -7.3028e-02,  4.0034e-02,  7.2398e-02, -1.5140e-02],\n",
      "          [ 5.6403e-02, -1.0023e-02, -2.8393e-02, -7.0266e-02, -7.9650e-02],\n",
      "          [ 9.6887e-03,  2.1904e-02,  6.8931e-02, -3.0603e-02,  1.7826e-03],\n",
      "          [ 5.5027e-02,  4.2375e-02,  4.3525e-02, -7.2960e-02,  3.6539e-02]],\n",
      "\n",
      "         [[ 6.5965e-02, -2.1924e-02, -6.3628e-02,  7.9598e-02,  6.2976e-02],\n",
      "          [-3.1863e-02, -2.9505e-03, -2.4111e-02, -5.9536e-02, -2.2164e-02],\n",
      "          [ 1.0503e-02, -8.2739e-02,  5.9905e-03,  4.7965e-02, -6.2725e-02],\n",
      "          [-2.1096e-02,  8.5775e-02, -2.8423e-02, -7.6515e-03,  1.1055e-02],\n",
      "          [ 8.1535e-02,  6.1051e-02, -4.6192e-02, -1.1161e-02,  9.0711e-02]],\n",
      "\n",
      "         [[ 2.1538e-02, -6.7092e-02,  4.3707e-02, -3.4153e-02,  7.4525e-02],\n",
      "          [ 3.8780e-02, -4.3850e-02,  3.2315e-02, -6.7470e-02,  4.7628e-02],\n",
      "          [-4.0030e-02,  2.4213e-02, -1.6643e-02, -8.0029e-02,  3.5618e-02],\n",
      "          [-1.1519e-02, -5.1505e-02, -1.0027e-02, -2.0282e-02,  2.3104e-02],\n",
      "          [-3.6775e-02, -6.3754e-02,  5.1140e-02,  1.5596e-02, -5.8802e-02]],\n",
      "\n",
      "         [[ 3.3961e-02, -6.1792e-02,  2.9286e-02,  1.6093e-02, -7.5948e-02],\n",
      "          [-4.9302e-02, -7.1347e-02, -2.3336e-02, -2.6320e-02, -7.1129e-02],\n",
      "          [ 2.1879e-02,  8.3182e-03,  4.4597e-02, -1.5141e-02,  7.5893e-02],\n",
      "          [-3.0490e-02, -6.7140e-02,  9.0711e-02,  8.3144e-02, -3.4788e-02],\n",
      "          [ 5.5178e-02, -2.2103e-02,  4.8046e-04,  6.8216e-02,  7.7485e-02]],\n",
      "\n",
      "         [[ 3.1437e-02,  2.4052e-02,  7.3346e-02,  1.3847e-03,  1.4052e-02],\n",
      "          [ 2.0887e-02, -9.1292e-03,  7.8427e-02, -5.1668e-02, -5.9865e-02],\n",
      "          [-5.5305e-02, -4.3700e-02, -1.6336e-02, -7.3005e-02,  5.5354e-02],\n",
      "          [-7.0972e-02,  5.9338e-02,  5.4475e-02, -1.3366e-02,  2.0086e-02],\n",
      "          [ 1.0990e-02,  2.2641e-02,  2.7916e-02, -4.8995e-02,  7.4386e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1942e-01,  1.3027e-02,  3.3712e-02, -6.5592e-02, -1.2006e-02],\n",
      "          [ 1.5788e-01,  7.6223e-02,  1.1367e-01,  5.8896e-02,  3.8822e-02],\n",
      "          [ 2.7799e-01,  2.4714e-01,  1.1596e-01,  1.3860e-01,  1.5353e-01],\n",
      "          [ 3.0750e-01,  2.7787e-01,  1.4236e-01,  1.9049e-01,  5.0544e-02],\n",
      "          [ 3.4884e-01,  3.0894e-01,  3.8186e-01,  3.0672e-01,  2.8717e-01]],\n",
      "\n",
      "         [[-1.5588e-02, -1.4008e-01, -1.8264e-01, -5.7634e-02, -3.6816e-02],\n",
      "          [-5.4652e-02, -6.8588e-02, -8.7883e-02, -3.5165e-02, -1.0864e-01],\n",
      "          [-9.0119e-02, -8.3438e-02, -6.1334e-02, -1.4122e-01, -1.7035e-01],\n",
      "          [-1.2219e-01, -8.7330e-02, -1.5215e-01, -3.5512e-02, -8.4903e-02],\n",
      "          [-1.5943e-01, -1.4307e-01, -1.5536e-01, -1.8811e-01, -4.4799e-02]],\n",
      "\n",
      "         [[ 1.1662e-01, -1.0079e-01, -9.3303e-02, -1.2139e-01, -4.3762e-02],\n",
      "          [ 7.3335e-02, -1.4422e-01, -1.6245e-01, -9.8108e-02, -2.0529e-01],\n",
      "          [ 3.9737e-02, -1.9598e-01, -2.7981e-01, -2.1398e-01, -2.3192e-01],\n",
      "          [ 5.3221e-02, -1.0165e-01, -2.1514e-01, -2.3180e-01, -3.8404e-01],\n",
      "          [-9.5589e-02, -1.6501e-01, -2.2387e-01, -2.0586e-01, -2.9661e-01]],\n",
      "\n",
      "         [[-7.1303e-02, -5.7852e-02,  3.2679e-02, -5.7545e-02, -5.8225e-02],\n",
      "          [ 2.8183e-02,  6.9657e-02, -3.2931e-02, -4.6604e-02, -4.4516e-02],\n",
      "          [ 7.8659e-02, -8.1898e-02, -7.9275e-02,  4.1835e-02, -5.3358e-02],\n",
      "          [ 3.8849e-03, -1.7714e-02,  7.9716e-02,  7.8445e-03, -9.6230e-03],\n",
      "          [ 2.7995e-02, -6.4477e-03, -5.7217e-02,  4.2452e-02, -7.1376e-02]],\n",
      "\n",
      "         [[ 1.5867e-01,  6.3844e-02, -4.8119e-02,  1.0467e-02,  1.4559e-01],\n",
      "          [ 4.5909e-02,  1.5620e-02, -4.4580e-03,  1.3596e-03,  1.0074e-01],\n",
      "          [ 7.3728e-02, -5.7013e-02, -5.9201e-02,  9.0312e-02, -1.8399e-02],\n",
      "          [ 7.2253e-02,  1.5028e-01, -2.4900e-02,  9.5712e-02,  6.5353e-02],\n",
      "          [ 1.5193e-01,  1.6521e-01,  1.4088e-01,  7.9696e-02, -2.5472e-02]],\n",
      "\n",
      "         [[-1.4836e-01, -2.0857e-01, -1.8837e-01, -2.3530e-01, -1.4217e-01],\n",
      "          [-1.5354e-01, -1.0856e-01, -8.1626e-02, -1.7640e-01, -1.3772e-01],\n",
      "          [-6.0183e-02, -1.8200e-01, -1.6080e-01, -1.1367e-01, -1.1852e-01],\n",
      "          [-1.4477e-01, -1.4881e-01, -1.0745e-01, -2.2796e-01, -1.5475e-01],\n",
      "          [-1.2536e-01, -1.0791e-01, -8.9973e-02, -1.7689e-01, -7.3876e-02]]]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0144, -0.1632,  0.2325,  0.1220,  0.0291, -0.1321,  0.4489,  0.4665,\n",
      "        -0.0475, -0.0406, -0.4139,  0.1322, -0.3426,  0.0684,  0.0203,  0.0472],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.8386e-03, -3.1017e-02,  3.1996e-02,  ..., -4.8293e-02,\n",
      "         -1.9586e-02, -5.1650e-02],\n",
      "        [ 1.4260e-02,  5.3003e-02,  7.1896e-02,  ...,  2.9775e-02,\n",
      "          3.2152e-02,  2.5731e-03],\n",
      "        [ 3.0030e-02, -3.1928e-02, -9.3982e-05,  ...,  6.2069e-03,\n",
      "          4.8640e-02,  3.4170e-02],\n",
      "        ...,\n",
      "        [ 3.1885e-02,  1.5747e-02, -7.2247e-02,  ...,  3.4980e-03,\n",
      "          1.1495e-03,  2.4627e-03],\n",
      "        [ 1.7782e-02, -3.4813e-02, -4.1826e-02,  ...,  8.0923e-03,\n",
      "          1.3949e-02, -2.0897e-02],\n",
      "        [ 4.2927e-02, -3.4350e-02, -2.7768e-04,  ..., -1.4119e-02,\n",
      "          1.4420e-02, -2.7608e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0314,  0.0129, -0.0484,  ..., -0.0302,  0.0058, -0.0085],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0106,  0.0316,  0.0033,  ...,  0.0063,  0.0237, -0.0204],\n",
      "        [ 0.0038, -0.0222, -0.0185,  ..., -0.0018,  0.0128,  0.0157],\n",
      "        [-0.0321,  0.0047,  0.0421,  ...,  0.0378,  0.0215,  0.0031],\n",
      "        ...,\n",
      "        [-0.0475,  0.0509, -0.0132,  ...,  0.0212, -0.0404,  0.0070],\n",
      "        [ 0.0293,  0.0148, -0.0384,  ...,  0.0129, -0.0154,  0.0370],\n",
      "        [-0.0313,  0.0368,  0.0070,  ..., -0.0258, -0.0426,  0.0267]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0623,  0.0378, -0.0316,  0.0027, -0.0489, -0.0141, -0.0440, -0.0290,\n",
      "         0.0205, -0.0336,  0.0037,  0.0385, -0.0422,  0.0501,  0.0251,  0.0058,\n",
      "        -0.0588, -0.0439,  0.0034, -0.0181,  0.0011, -0.0249,  0.0972,  0.0471,\n",
      "        -0.0238, -0.0224,  0.0465,  0.0096, -0.0438,  0.0215,  0.0147,  0.0294,\n",
      "         0.0318,  0.0176,  0.0042, -0.0164,  0.0606, -0.0079, -0.0083,  0.0076,\n",
      "        -0.0171, -0.0194,  0.0537,  0.0014, -0.0322, -0.0431,  0.0567, -0.0198,\n",
      "        -0.0218, -0.0133, -0.0301,  0.0270,  0.0457,  0.0258,  0.0200,  0.0364,\n",
      "         0.1139,  0.0129, -0.0264, -0.0361, -0.0316,  0.0070,  0.0410,  0.0363,\n",
      "         0.0226,  0.0125,  0.0138,  0.0723, -0.0044, -0.0447, -0.0576,  0.0577,\n",
      "         0.0266, -0.0575, -0.0090, -0.0411,  0.0003,  0.0120, -0.0127,  0.0326,\n",
      "        -0.0073,  0.0836, -0.0242, -0.0169, -0.0550,  0.0479,  0.0479,  0.0749,\n",
      "         0.0207,  0.0032, -0.0135,  0.0292,  0.0082,  0.0025,  0.0306,  0.1368,\n",
      "         0.0322,  0.0150, -0.0706, -0.0152, -0.0347,  0.0098, -0.0120, -0.0778,\n",
      "         0.0589,  0.0540, -0.0102, -0.0144,  0.0024, -0.0502,  0.0668, -0.0111,\n",
      "        -0.0015, -0.0070,  0.0097, -0.0161,  0.0097, -0.0283, -0.0159, -0.0141,\n",
      "        -0.0225,  0.0213, -0.0109,  0.0604, -0.0518, -0.0060, -0.0048,  0.0152,\n",
      "         0.0309,  0.0119, -0.0295,  0.0427,  0.0519, -0.0040,  0.0240,  0.0208,\n",
      "         0.0068,  0.0187,  0.0190, -0.0337, -0.0032,  0.0438,  0.0478, -0.0043,\n",
      "        -0.0169,  0.0470, -0.0302, -0.0006,  0.0146, -0.0246, -0.0377, -0.0104,\n",
      "        -0.0299, -0.0237, -0.0186,  0.0208, -0.0068,  0.0374,  0.0199,  0.0047,\n",
      "        -0.0050,  0.0225,  0.0228,  0.0181, -0.0305,  0.0542, -0.0237, -0.0161,\n",
      "        -0.0429,  0.0117, -0.0424,  0.0098,  0.0620, -0.0578,  0.0135,  0.0376,\n",
      "        -0.0041, -0.0295,  0.0061,  0.0338, -0.0312,  0.0063,  0.0514, -0.0537,\n",
      "         0.0079, -0.0631,  0.0137,  0.0016,  0.0823, -0.0126, -0.0483, -0.0113,\n",
      "        -0.0149, -0.0127, -0.0007, -0.0240,  0.0184,  0.0492, -0.0077,  0.0099,\n",
      "        -0.0098,  0.0426, -0.0072, -0.0048, -0.0087, -0.0464,  0.0255, -0.0171,\n",
      "         0.0231,  0.0446,  0.0127, -0.0124,  0.0313, -0.0219, -0.0061, -0.0228,\n",
      "        -0.0548, -0.0283,  0.0351, -0.0108,  0.0793, -0.0086, -0.0367, -0.0365,\n",
      "        -0.0264, -0.0259, -0.0223,  0.0342, -0.0181, -0.0020,  0.0343, -0.0491,\n",
      "         0.0306, -0.0225, -0.0074, -0.0342, -0.0074,  0.0199, -0.0218,  0.0701,\n",
      "        -0.0048, -0.0079, -0.0221,  0.0206, -0.0152,  0.0048,  0.0069,  0.0362,\n",
      "        -0.0203,  0.0390,  0.0068,  0.0964, -0.0411,  0.0119, -0.0552,  0.1020,\n",
      "         0.0128,  0.0611, -0.0144, -0.0374, -0.0113, -0.0156, -0.0272, -0.0237,\n",
      "        -0.0015,  0.0086,  0.0219,  0.0253, -0.0182, -0.0678, -0.0456,  0.0483,\n",
      "         0.0292,  0.0005, -0.0471,  0.0242,  0.0189, -0.0163,  0.0082, -0.0069,\n",
      "        -0.0338, -0.0435,  0.0213, -0.0229,  0.0878,  0.0099,  0.1055,  0.0010,\n",
      "        -0.0121,  0.0301,  0.0336, -0.0119, -0.0230, -0.0142, -0.0032, -0.0480,\n",
      "         0.0013, -0.0314,  0.0166,  0.0117,  0.0173,  0.0613,  0.0836,  0.0207,\n",
      "        -0.0621,  0.0292,  0.0574, -0.0011,  0.0259,  0.0561, -0.0105, -0.0169,\n",
      "        -0.0881, -0.0049, -0.0310,  0.0329,  0.0004,  0.0132,  0.0942,  0.0062,\n",
      "         0.0345,  0.0173, -0.0130,  0.0334,  0.0060,  0.0429, -0.0042,  0.0052,\n",
      "         0.0289,  0.0481,  0.0575, -0.0287, -0.0219,  0.0205,  0.0519,  0.0151,\n",
      "         0.0787, -0.0102,  0.0113, -0.0172,  0.0220,  0.0466,  0.0104, -0.0279,\n",
      "         0.0213, -0.0184,  0.0052,  0.0213,  0.0624, -0.0151, -0.0129, -0.0051,\n",
      "        -0.0041,  0.0197, -0.0158,  0.0139,  0.0035, -0.0371,  0.0012,  0.0628,\n",
      "         0.0025,  0.0254, -0.0178, -0.0246,  0.0177,  0.0745,  0.0483,  0.0532,\n",
      "         0.0331, -0.0200, -0.0151, -0.0174,  0.0141,  0.0216, -0.0038,  0.0300,\n",
      "        -0.0178,  0.0197, -0.0453, -0.0332,  0.0550, -0.0434, -0.0041,  0.0887,\n",
      "        -0.0174,  0.0419, -0.0327, -0.0261,  0.0284, -0.0283,  0.0821, -0.0136,\n",
      "        -0.0173, -0.0541, -0.0036,  0.0060, -0.0140, -0.0266,  0.0816, -0.0324,\n",
      "        -0.0323,  0.0402, -0.0056,  0.0112,  0.0136, -0.0088, -0.0083, -0.0147,\n",
      "        -0.0424, -0.0321,  0.0058, -0.0022,  0.0691, -0.0362, -0.0040, -0.0073,\n",
      "         0.0004, -0.0245, -0.0094, -0.0089,  0.0125, -0.0220, -0.0117,  0.0152,\n",
      "         0.0463, -0.0321,  0.0822, -0.0391,  0.0096, -0.0683,  0.0009,  0.0741,\n",
      "        -0.0076,  0.0609, -0.0046, -0.0248, -0.0580,  0.0122,  0.0003,  0.0466,\n",
      "        -0.0018, -0.0375,  0.0392, -0.0254,  0.0017, -0.0089, -0.0226, -0.0388,\n",
      "        -0.0023, -0.0334, -0.0251, -0.0064, -0.0600, -0.0150,  0.0294, -0.0506,\n",
      "        -0.0156,  0.0303, -0.0112,  0.0151, -0.0204, -0.0324,  0.0016,  0.0402,\n",
      "        -0.0371, -0.0293,  0.0965, -0.0062, -0.0382, -0.0411, -0.0116, -0.0160,\n",
      "        -0.0005, -0.0027,  0.0125, -0.0006, -0.0023, -0.0103, -0.0288, -0.0343,\n",
      "         0.0130,  0.0175, -0.0084,  0.0362, -0.0180, -0.0226, -0.0189, -0.0044,\n",
      "         0.0087,  0.0379, -0.0245, -0.0241, -0.0432,  0.0044,  0.0720,  0.1035,\n",
      "         0.0072, -0.0583, -0.0060,  0.0207,  0.0326,  0.0217, -0.0104,  0.1170,\n",
      "        -0.0420, -0.0312, -0.0153, -0.0023,  0.0294,  0.0336,  0.0327, -0.0107],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1167, -0.0499,  0.0636,  ...,  0.0147,  0.0315, -0.0864],\n",
      "        [-0.1101,  0.0109, -0.0053,  ..., -0.0317, -0.0421,  0.0636],\n",
      "        [-0.0476, -0.0245,  0.0467,  ..., -0.0007, -0.0423, -0.0548],\n",
      "        ...,\n",
      "        [-0.0198, -0.0379,  0.0118,  ..., -0.1548, -0.1167, -0.0275],\n",
      "        [ 0.0032, -0.0257, -0.0771,  ...,  0.0430, -0.0517,  0.0112],\n",
      "        [-0.0097, -0.0028, -0.0156,  ...,  0.0620,  0.0752,  0.0726]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.4442e-01, -6.1475e-02, -3.1178e-02,  1.4722e-01,  2.2967e-01,\n",
      "        -1.1606e-01, -5.0841e-02, -3.2671e-02,  7.3843e-02, -4.1352e-02,\n",
      "        -9.2979e-02, -9.1196e-02, -8.8150e-03, -6.0763e-02,  5.9008e-02,\n",
      "        -4.7675e-02, -4.6238e-02, -5.8783e-02,  1.4221e-02, -1.0586e-02,\n",
      "        -1.9064e-01,  2.0820e-01, -6.3726e-02, -2.1103e-02, -1.0106e-01,\n",
      "        -6.0503e-02,  1.7499e-02,  8.3452e-02, -1.3328e-02, -6.9869e-02,\n",
      "         6.1013e-02,  1.4084e-01, -3.2244e-02,  3.2005e-01, -1.4118e-02,\n",
      "        -6.2638e-02, -1.5373e-01, -2.2177e-02,  5.1346e-02, -1.4091e-03,\n",
      "        -1.6108e-01, -1.2420e-01,  1.4382e-01, -2.0296e-03,  6.8073e-02,\n",
      "         5.1967e-02, -1.1762e-02,  3.8617e-02,  7.6791e-03, -1.9030e-02,\n",
      "         2.5033e-02, -4.5153e-02,  5.5844e-02, -2.4101e-01, -1.3895e-01,\n",
      "         8.8638e-02,  5.4774e-03, -2.0014e-01, -9.0898e-03,  9.2488e-02,\n",
      "        -7.2981e-02, -1.1385e-01, -1.2311e-01,  1.9964e-01,  9.2489e-02,\n",
      "         2.2576e-02,  1.7685e-01,  4.5620e-02,  7.4285e-03, -1.0136e-01,\n",
      "        -1.2730e-01, -5.0557e-02,  2.6714e-02,  8.5834e-05,  2.3962e-01,\n",
      "         3.3262e-02,  2.7904e-03,  2.7121e-02,  1.1327e-01,  1.3900e-01,\n",
      "         2.4994e-02,  2.8106e-02, -4.2093e-02, -1.7662e-01, -3.8360e-02,\n",
      "        -5.6095e-02, -1.3276e-01, -1.8248e-02,  1.2390e-01,  7.2821e-02,\n",
      "         1.7170e-02, -7.0569e-02, -7.4475e-02,  2.0076e-02,  8.8136e-02,\n",
      "        -4.1724e-02,  1.1627e-01,  2.1634e-02, -7.9324e-02,  1.2077e-01],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "net_adv = copy.deepcopy(net_ori) \n",
    "for param in net_adv.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5635cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_adv.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = './cifar_base_adv_pgd_3.pth'\n",
    "device = \"cuda\"\n",
    "writer = SummaryWriter(\"Adv_Base_PGD\")\n",
    "pgd_attack = generate_adv(net_adv, \"pgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08ee62d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100]\t\n",
      "Epoch [1/100]\t Training Loss: 5.61242687610714\t lr: 0.001\n",
      "Epoch [1/100]\t Validation Loss: 4.052706042422524\t lr: 0.001\n",
      "Epoch [1/100]\t\n",
      "Epoch [2/100]\t Training Loss: 4.524453789986613\t lr: 0.001\n",
      "Epoch [2/100]\t Validation Loss: 3.8696791914444937\t lr: 0.001\n",
      "Epoch [2/100]\t\n",
      "Epoch [3/100]\t Training Loss: 4.373876644827216\t lr: 0.001\n",
      "Epoch [3/100]\t Validation Loss: 3.7542997130864784\t lr: 0.001\n",
      "Epoch [3/100]\t\n",
      "Epoch [4/100]\t Training Loss: 4.277758375153212\t lr: 0.001\n",
      "Epoch [4/100]\t Validation Loss: 3.6750665797462947\t lr: 0.001\n",
      "Epoch [4/100]\t\n",
      "Epoch [5/100]\t Training Loss: 4.204787092745456\t lr: 0.001\n",
      "Epoch [5/100]\t Validation Loss: 3.6098694016661823\t lr: 0.001\n",
      "Epoch [5/100]\t\n",
      "Epoch [6/100]\t Training Loss: 4.150112204539502\t lr: 0.001\n",
      "Epoch [6/100]\t Validation Loss: 3.5501100262509118\t lr: 0.001\n",
      "Epoch [6/100]\t\n",
      "Epoch [7/100]\t Training Loss: 4.106544750730705\t lr: 0.001\n",
      "Epoch [7/100]\t Validation Loss: 3.528781199757057\t lr: 0.001\n",
      "Epoch [7/100]\t\n",
      "Epoch [8/100]\t Training Loss: 4.0671445298987585\t lr: 0.001\n",
      "Epoch [8/100]\t Validation Loss: 3.486771010145356\t lr: 0.001\n",
      "Epoch [8/100]\t\n",
      "Epoch [9/100]\t Training Loss: 4.0345531613625525\t lr: 0.001\n",
      "Epoch [9/100]\t Validation Loss: 3.4669415256645104\t lr: 0.001\n",
      "Epoch [9/100]\t\n",
      "Epoch [10/100]\t Training Loss: 4.007588198117893\t lr: 0.001\n",
      "Epoch [10/100]\t Validation Loss: 3.436621068399164\t lr: 0.001\n",
      "Epoch [10/100]\t\n",
      "Epoch [11/100]\t Training Loss: 3.980327700714931\t lr: 0.001\n",
      "Epoch [11/100]\t Validation Loss: 3.4252594664127014\t lr: 0.001\n",
      "Epoch [11/100]\t\n",
      "Epoch [12/100]\t Training Loss: 3.9571490135339213\t lr: 0.001\n",
      "Epoch [12/100]\t Validation Loss: 3.397051379650454\t lr: 0.001\n",
      "Epoch [12/100]\t\n",
      "Epoch [13/100]\t Training Loss: 3.9345666130485437\t lr: 0.001\n",
      "Epoch [13/100]\t Validation Loss: 3.3684466881088064\t lr: 0.001\n",
      "Epoch [13/100]\t\n",
      "Epoch [14/100]\t Training Loss: 3.9126102832881995\t lr: 0.001\n",
      "Epoch [14/100]\t Validation Loss: 3.359531381462194\t lr: 0.001\n",
      "Epoch [14/100]\t\n",
      "Epoch [15/100]\t Training Loss: 3.890764659018163\t lr: 0.001\n",
      "Epoch [15/100]\t Validation Loss: 3.346590030042431\t lr: 0.001\n",
      "Epoch [15/100]\t\n",
      "Epoch [16/100]\t Training Loss: 3.8713251959027537\t lr: 0.001\n",
      "Epoch [16/100]\t Validation Loss: 3.3260675291471844\t lr: 0.001\n",
      "Epoch [16/100]\t\n",
      "Epoch [17/100]\t Training Loss: 3.8514171018624856\t lr: 0.001\n",
      "Epoch [17/100]\t Validation Loss: 3.324653936337821\t lr: 0.001\n",
      "Epoch [17/100]\t\n",
      "Epoch [18/100]\t Training Loss: 3.8324859605725767\t lr: 0.001\n",
      "Epoch [18/100]\t Validation Loss: 3.3013576706753502\t lr: 0.001\n",
      "Epoch [18/100]\t\n",
      "Epoch [19/100]\t Training Loss: 3.813228005948274\t lr: 0.001\n",
      "Epoch [19/100]\t Validation Loss: 3.276536715181568\t lr: 0.001\n",
      "Epoch [19/100]\t\n",
      "Epoch [20/100]\t Training Loss: 3.795121375861985\t lr: 0.001\n",
      "Epoch [20/100]\t Validation Loss: 3.2821050354197054\t lr: 0.001\n",
      "Epoch [20/100]\t\n",
      "Epoch [21/100]\t Training Loss: 3.777627902872422\t lr: 0.001\n",
      "Epoch [21/100]\t Validation Loss: 3.262719685518289\t lr: 0.001\n",
      "Epoch [21/100]\t\n",
      "Epoch [22/100]\t Training Loss: 3.7602852439636463\t lr: 0.001\n",
      "Epoch [22/100]\t Validation Loss: 3.2594945611833017\t lr: 0.001\n",
      "Epoch [22/100]\t\n",
      "Epoch [23/100]\t Training Loss: 3.7439393960606413\t lr: 0.001\n",
      "Epoch [23/100]\t Validation Loss: 3.240642499320115\t lr: 0.001\n",
      "Epoch [23/100]\t\n",
      "Epoch [24/100]\t Training Loss: 3.7263169197170325\t lr: 0.001\n",
      "Epoch [24/100]\t Validation Loss: 3.2466032867190204\t lr: 0.001\n",
      "Epoch [24/100]\t\n",
      "Epoch [25/100]\t Training Loss: 3.709526502262906\t lr: 0.001\n",
      "Epoch [25/100]\t Validation Loss: 3.2425262414956393\t lr: 0.001\n",
      "Epoch [25/100]\t\n",
      "Epoch [26/100]\t Training Loss: 3.6928426743773244\t lr: 0.001\n",
      "Epoch [26/100]\t Validation Loss: 3.209165316593798\t lr: 0.001\n",
      "Epoch [26/100]\t\n",
      "Epoch [27/100]\t Training Loss: 3.6771237307497304\t lr: 0.001\n",
      "Epoch [27/100]\t Validation Loss: 3.213387196577048\t lr: 0.001\n",
      "Epoch [27/100]\t\n",
      "Epoch [28/100]\t Training Loss: 3.6598404959949384\t lr: 0.001\n",
      "Epoch [28/100]\t Validation Loss: 3.2081905045086825\t lr: 0.001\n",
      "Epoch [28/100]\t\n",
      "Epoch [29/100]\t Training Loss: 3.646772943799148\t lr: 0.001\n",
      "Epoch [29/100]\t Validation Loss: 3.199020841453649\t lr: 0.001\n",
      "Epoch [29/100]\t\n",
      "Epoch [30/100]\t Training Loss: 3.6293917019348925\t lr: 0.001\n",
      "Epoch [30/100]\t Validation Loss: 3.190995415554771\t lr: 0.001\n",
      "Epoch [30/100]\t\n",
      "Epoch [31/100]\t Training Loss: 3.6158612774461125\t lr: 0.001\n",
      "Epoch [31/100]\t Validation Loss: 3.187894413742838\t lr: 0.001\n",
      "Epoch [31/100]\t\n",
      "Epoch [32/100]\t Training Loss: 3.6010139250694335\t lr: 0.001\n",
      "Epoch [32/100]\t Validation Loss: 3.1805753888963144\t lr: 0.001\n",
      "Epoch [32/100]\t\n",
      "Epoch [33/100]\t Training Loss: 3.583502123727823\t lr: 0.001\n",
      "Epoch [33/100]\t Validation Loss: 3.17122678817073\t lr: 0.001\n",
      "Epoch [33/100]\t\n",
      "Epoch [34/100]\t Training Loss: 3.572536949611381\t lr: 0.001\n",
      "Epoch [34/100]\t Validation Loss: 3.1616986582550823\t lr: 0.001\n",
      "Epoch [34/100]\t\n",
      "Epoch [35/100]\t Training Loss: 3.554270221144342\t lr: 0.001\n",
      "Epoch [35/100]\t Validation Loss: 3.165594783010362\t lr: 0.001\n",
      "Epoch [35/100]\t\n",
      "Epoch [36/100]\t Training Loss: 3.540958846621501\t lr: 0.001\n",
      "Epoch [36/100]\t Validation Loss: 3.158579747888106\t lr: 0.001\n",
      "Epoch [36/100]\t\n",
      "Epoch [37/100]\t Training Loss: 3.5243480583590925\t lr: 0.001\n",
      "Epoch [37/100]\t Validation Loss: 3.146305539939977\t lr: 0.001\n",
      "Epoch [37/100]\t\n",
      "Epoch [38/100]\t Training Loss: 3.510630685045286\t lr: 0.001\n",
      "Epoch [38/100]\t Validation Loss: 3.1512383539465407\t lr: 0.001\n",
      "Epoch [38/100]\t\n",
      "Epoch [39/100]\t Training Loss: 3.4961571162923826\t lr: 0.001\n",
      "Epoch [39/100]\t Validation Loss: 3.143646210054808\t lr: 0.001\n",
      "Epoch [39/100]\t\n",
      "Epoch [40/100]\t Training Loss: 3.4825738348314523\t lr: 0.001\n",
      "Epoch [40/100]\t Validation Loss: 3.1315103482596482\t lr: 0.001\n",
      "Epoch [40/100]\t\n",
      "Epoch [41/100]\t Training Loss: 3.4682555887705226\t lr: 0.001\n",
      "Epoch [41/100]\t Validation Loss: 3.142422295823882\t lr: 0.001\n",
      "Epoch [41/100]\t\n",
      "Epoch [42/100]\t Training Loss: 3.454750147621955\t lr: 0.001\n",
      "Epoch [42/100]\t Validation Loss: 3.1316103995600835\t lr: 0.001\n",
      "Epoch [42/100]\t\n",
      "Epoch [43/100]\t Training Loss: 3.438880785651829\t lr: 0.001\n",
      "Epoch [43/100]\t Validation Loss: 3.1356749202631695\t lr: 0.001\n",
      "Epoch [43/100]\t\n",
      "Epoch [44/100]\t Training Loss: 3.424266466399288\t lr: 0.001\n",
      "Epoch [44/100]\t Validation Loss: 3.119464666028566\t lr: 0.001\n",
      "Epoch [44/100]\t\n",
      "Epoch [45/100]\t Training Loss: 3.4103529026441257\t lr: 0.001\n",
      "Epoch [45/100]\t Validation Loss: 3.118186790731889\t lr: 0.001\n",
      "Epoch [45/100]\t\n",
      "Epoch [46/100]\t Training Loss: 3.396721643560073\t lr: 0.001\n",
      "Epoch [46/100]\t Validation Loss: 3.1206833169430115\t lr: 0.001\n",
      "Epoch [46/100]\t\n",
      "Epoch [47/100]\t Training Loss: 3.3806697639358014\t lr: 0.001\n",
      "Epoch [47/100]\t Validation Loss: 3.123756523373761\t lr: 0.001\n",
      "Epoch [47/100]\t\n",
      "Epoch [48/100]\t Training Loss: 3.3680285395258833\t lr: 0.001\n",
      "Epoch [48/100]\t Validation Loss: 3.1040445370010183\t lr: 0.001\n",
      "Epoch [48/100]\t\n",
      "Epoch [49/100]\t Training Loss: 3.3527045963365403\t lr: 0.001\n",
      "Epoch [49/100]\t Validation Loss: 3.114587331119972\t lr: 0.001\n",
      "Epoch [49/100]\t\n",
      "Epoch [50/100]\t Training Loss: 3.3401601345032987\t lr: 0.001\n",
      "Epoch [50/100]\t Validation Loss: 3.1133333308787288\t lr: 0.001\n",
      "Epoch [50/100]\t\n",
      "Epoch [51/100]\t Training Loss: 3.326375583248675\t lr: 0.001\n",
      "Epoch [51/100]\t Validation Loss: 3.104616403579712\t lr: 0.001\n",
      "Epoch [51/100]\t\n",
      "Epoch [52/100]\t Training Loss: 3.3116360710709905\t lr: 0.001\n",
      "Epoch [52/100]\t Validation Loss: 3.113764334328567\t lr: 0.001\n",
      "Epoch [52/100]\t\n",
      "Epoch [53/100]\t Training Loss: 3.2953586401536947\t lr: 0.001\n",
      "Epoch [53/100]\t Validation Loss: 3.1088384435146668\t lr: 0.001\n",
      "Epoch [53/100]\t\n",
      "Epoch [54/100]\t Training Loss: 3.284355578824992\t lr: 0.001\n",
      "Epoch [54/100]\t Validation Loss: 3.102072392837911\t lr: 0.001\n",
      "Epoch [54/100]\t\n",
      "Epoch [55/100]\t Training Loss: 3.27161189845151\t lr: 0.001\n",
      "Epoch [55/100]\t Validation Loss: 3.1034945174108577\t lr: 0.001\n",
      "Epoch [55/100]\t\n",
      "Epoch [56/100]\t Training Loss: 3.256178807724467\t lr: 0.001\n",
      "Epoch [56/100]\t Validation Loss: 3.104156077662601\t lr: 0.001\n",
      "Epoch [56/100]\t\n",
      "Epoch [57/100]\t Training Loss: 3.242927918958542\t lr: 0.001\n",
      "Epoch [57/100]\t Validation Loss: 3.1073380421988572\t lr: 0.001\n",
      "Epoch [57/100]\t\n",
      "Epoch [58/100]\t Training Loss: 3.228297141506849\t lr: 0.001\n",
      "Epoch [58/100]\t Validation Loss: 3.108026139343841\t lr: 0.001\n",
      "Epoch [58/100]\t\n",
      "Epoch [59/100]\t Training Loss: 3.2154822422720284\t lr: 0.001\n",
      "Epoch [59/100]\t Validation Loss: 3.114783425874348\t lr: 0.001\n",
      "Epoch [59/100]\t\n",
      "Epoch [60/100]\t Training Loss: 3.2012205117803707\t lr: 0.001\n",
      "Epoch [60/100]\t Validation Loss: 3.108999161780635\t lr: 0.001\n",
      "Epoch [60/100]\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100]\t Training Loss: 3.19021529919656\t lr: 0.001\n",
      "Epoch [61/100]\t Validation Loss: 3.1020521604562106\t lr: 0.001\n",
      "Epoch [61/100]\t\n",
      "Epoch [62/100]\t Training Loss: 3.1720830912480267\t lr: 0.001\n",
      "Epoch [62/100]\t Validation Loss: 3.0996938023386122\t lr: 0.001\n",
      "Epoch [62/100]\t\n",
      "Epoch [63/100]\t Training Loss: 3.160540592335069\t lr: 0.001\n",
      "Epoch [63/100]\t Validation Loss: 3.1083232179472717\t lr: 0.001\n",
      "Epoch [63/100]\t\n",
      "Epoch [64/100]\t Training Loss: 3.147827190511367\t lr: 0.001\n",
      "Epoch [64/100]\t Validation Loss: 3.1087988328330125\t lr: 0.001\n",
      "Epoch [64/100]\t\n",
      "Epoch [65/100]\t Training Loss: 3.1325503932240673\t lr: 0.001\n",
      "Epoch [65/100]\t Validation Loss: 3.1053486685209637\t lr: 0.001\n",
      "Epoch [65/100]\t\n",
      "Epoch [66/100]\t Training Loss: 3.1196492821969035\t lr: 0.001\n",
      "Epoch [66/100]\t Validation Loss: 3.117337564878826\t lr: 0.001\n",
      "Epoch [66/100]\t\n",
      "Epoch [67/100]\t Training Loss: 3.108081431644957\t lr: 0.001\n",
      "Epoch [67/100]\t Validation Loss: 3.1088086774077595\t lr: 0.001\n",
      "Epoch [67/100]\t\n",
      "Epoch [68/100]\t Training Loss: 3.0909189866936724\t lr: 0.001\n",
      "Epoch [68/100]\t Validation Loss: 3.1088242229027085\t lr: 0.001\n",
      "Epoch [68/100]\t\n",
      "Epoch [69/100]\t Training Loss: 3.079611831308936\t lr: 0.001\n",
      "Epoch [69/100]\t Validation Loss: 3.1226453811307495\t lr: 0.001\n",
      "Epoch [69/100]\t\n",
      "Epoch [70/100]\t Training Loss: 3.0684868880854848\t lr: 0.001\n",
      "Epoch [70/100]\t Validation Loss: 3.114628152002262\t lr: 0.001\n",
      "Epoch [70/100]\t\n",
      "Epoch [71/100]\t Training Loss: 3.0518054840204965\t lr: 0.001\n",
      "Epoch [71/100]\t Validation Loss: 3.1170005375825904\t lr: 0.001\n",
      "Epoch [71/100]\t\n",
      "Epoch [72/100]\t Training Loss: 3.039885855086929\t lr: 0.001\n",
      "Epoch [72/100]\t Validation Loss: 3.126073921783061\t lr: 0.001\n",
      "Epoch [72/100]\t\n",
      "Epoch [73/100]\t Training Loss: 3.0253860938274646\t lr: 0.001\n",
      "Epoch [73/100]\t Validation Loss: 3.1187566503693787\t lr: 0.001\n",
      "Epoch [73/100]\t\n",
      "Epoch [74/100]\t Training Loss: 3.0128057472541205\t lr: 0.001\n",
      "Epoch [74/100]\t Validation Loss: 3.129989799064926\t lr: 0.001\n",
      "Epoch [74/100]\t\n",
      "Epoch [75/100]\t Training Loss: 3.001030954253643\t lr: 0.001\n",
      "Epoch [75/100]\t Validation Loss: 3.123921168001392\t lr: 0.001\n",
      "Epoch [75/100]\t\n",
      "Epoch [76/100]\t Training Loss: 2.9864016634119137\t lr: 0.001\n",
      "Epoch [76/100]\t Validation Loss: 3.126597449749331\t lr: 0.001\n",
      "Epoch [76/100]\t\n",
      "Epoch [77/100]\t Training Loss: 2.9734263523765234\t lr: 0.001\n",
      "Epoch [77/100]\t Validation Loss: 3.1316140693954275\t lr: 0.001\n",
      "Epoch [77/100]\t\n",
      "Epoch [78/100]\t Training Loss: 2.9593729052092415\t lr: 0.001\n",
      "Epoch [78/100]\t Validation Loss: 3.1376900672912598\t lr: 0.001\n",
      "Epoch [78/100]\t\n",
      "Epoch [79/100]\t Training Loss: 2.948370940240143\t lr: 0.001\n",
      "Epoch [79/100]\t Validation Loss: 3.1597779732716234\t lr: 0.001\n",
      "Epoch [79/100]\t\n",
      "Epoch [80/100]\t Training Loss: 2.9342901103027033\t lr: 0.001\n",
      "Epoch [80/100]\t Validation Loss: 3.148111922831475\t lr: 0.001\n",
      "Epoch [80/100]\t\n",
      "Epoch [81/100]\t Training Loss: 2.923500754949077\t lr: 0.001\n",
      "Epoch [81/100]\t Validation Loss: 3.1592059889926185\t lr: 0.001\n",
      "Epoch [81/100]\t\n",
      "Epoch [82/100]\t Training Loss: 2.9111336680019604\t lr: 0.001\n",
      "Epoch [82/100]\t Validation Loss: 3.158091182950177\t lr: 0.001\n",
      "Epoch [82/100]\t\n",
      "Epoch [83/100]\t Training Loss: 2.898761262674161\t lr: 0.001\n",
      "Epoch [83/100]\t Validation Loss: 3.1539188879954665\t lr: 0.001\n",
      "Epoch [83/100]\t\n",
      "Epoch [84/100]\t Training Loss: 2.883016439959826\t lr: 0.001\n",
      "Epoch [84/100]\t Validation Loss: 3.1620594066909598\t lr: 0.001\n",
      "Epoch [84/100]\t\n",
      "Epoch [85/100]\t Training Loss: 2.8700709342956543\t lr: 0.001\n",
      "Epoch [85/100]\t Validation Loss: 3.165835012363482\t lr: 0.001\n",
      "Epoch [85/100]\t\n",
      "Epoch [86/100]\t Training Loss: 2.8549859761582006\t lr: 0.001\n",
      "Epoch [86/100]\t Validation Loss: 3.175221636325498\t lr: 0.001\n",
      "Epoch [86/100]\t\n",
      "Epoch [87/100]\t Training Loss: 2.8425265419513672\t lr: 0.001\n",
      "Epoch [87/100]\t Validation Loss: 3.186191136323953\t lr: 0.001\n",
      "Epoch [87/100]\t\n",
      "Epoch [88/100]\t Training Loss: 2.830781565907666\t lr: 0.001\n",
      "Epoch [88/100]\t Validation Loss: 3.170144198816034\t lr: 0.001\n",
      "Epoch [88/100]\t\n",
      "Epoch [89/100]\t Training Loss: 2.8178621338456487\t lr: 0.001\n",
      "Epoch [89/100]\t Validation Loss: 3.185007668748687\t lr: 0.001\n",
      "Epoch [89/100]\t\n",
      "Epoch [90/100]\t Training Loss: 2.8054243402407906\t lr: 0.001\n",
      "Epoch [90/100]\t Validation Loss: 3.1965867024433763\t lr: 0.001\n",
      "Epoch [90/100]\t\n",
      "Epoch [91/100]\t Training Loss: 2.7933127550822694\t lr: 0.001\n",
      "Epoch [91/100]\t Validation Loss: 3.199403762817383\t lr: 0.001\n",
      "Epoch [91/100]\t\n",
      "Epoch [92/100]\t Training Loss: 2.7809249037671884\t lr: 0.001\n",
      "Epoch [92/100]\t Validation Loss: 3.2105269884761376\t lr: 0.001\n",
      "Epoch [92/100]\t\n",
      "Epoch [93/100]\t Training Loss: 2.7665956776465297\t lr: 0.001\n",
      "Epoch [93/100]\t Validation Loss: 3.2091011819960196\t lr: 0.001\n",
      "Epoch [93/100]\t\n",
      "Epoch [94/100]\t Training Loss: 2.756053366624486\t lr: 0.001\n",
      "Epoch [94/100]\t Validation Loss: 3.2139516631259193\t lr: 0.001\n",
      "Epoch [94/100]\t\n",
      "Epoch [95/100]\t Training Loss: 2.7426779745789744\t lr: 0.001\n",
      "Epoch [95/100]\t Validation Loss: 3.227245240271846\t lr: 0.001\n",
      "Epoch [95/100]\t\n",
      "Epoch [96/100]\t Training Loss: 2.732421531701637\t lr: 0.001\n",
      "Epoch [96/100]\t Validation Loss: 3.232826426059385\t lr: 0.001\n",
      "Epoch [96/100]\t\n",
      "Epoch [97/100]\t Training Loss: 2.7190377870788964\t lr: 0.001\n",
      "Epoch [97/100]\t Validation Loss: 3.2339375924460496\t lr: 0.001\n",
      "Epoch [97/100]\t\n",
      "Epoch [98/100]\t Training Loss: 2.7064555947433044\t lr: 0.001\n",
      "Epoch [98/100]\t Validation Loss: 3.2753060799610765\t lr: 0.001\n",
      "Epoch [98/100]\t\n",
      "Epoch [99/100]\t Training Loss: 2.6958592840472755\t lr: 0.001\n",
      "Epoch [99/100]\t Validation Loss: 3.2213678390164917\t lr: 0.001\n",
      "Epoch [99/100]\t\n",
      "Epoch [100/100]\t Training Loss: 2.683293848086501\t lr: 0.001\n",
      "Epoch [100/100]\t Validation Loss: 3.254357911363433\t lr: 0.001\n",
      "Epoch [99/100]\t Time Taken: 22.18813091913859 minutes\n"
     ]
    }
   ],
   "source": [
    "train(net_adv, train_loader, train_criterion,val_loader, val_criterion, \n",
    "              optimizer, 100, writer, \n",
    "                 PATH, True, pgd_attack, device=\"cuda\",\n",
    "                  n_steps_show=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0410a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 27 %\n"
     ]
    }
   ],
   "source": [
    "#test clean accuray\n",
    "test(val_loader, net_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d74b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD\n",
      "[Model loaded]\n",
      "Acc: 19.96 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"PGD\")\n",
    "pgd_attack_transfer = generate_adv(net_ori, \"pgd\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "pgd_acc = test_attack(val_loader, pgd_attack_transfer, net_adv, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f665fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgsm\n",
      "[Model loaded]\n",
      "Acc: 19.88 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"fgsm\")\n",
    "fgsm_attack_transfer = generate_adv(net_ori, \"fgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "fgsm_acc = test_attack(val_loader, fgsm_attack_transfer, net_adv, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc3a7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nifgsm\n",
      "[Model loaded]\n",
      "Acc: 20.20 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"nifgsm\")\n",
    "nifgsm_attack_transfer = generate_adv(net_ori, \"nifgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "nifgsm_acc = test_attack(val_loader, nifgsm_attack_transfer, net_adv, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c15b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmifgsm\n",
      "[Model loaded]\n",
      "Acc: 19.85 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"vmifgsm\")\n",
    "vmifgsm_attack_transfer = generate_adv(net_ori, \"vmifgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "vmifgsm_acc = test_attack(val_loader, vmifgsm_attack_transfer, net_adv, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d350815",
   "metadata": {},
   "source": [
    "# TODO: Adversarial Training by nifgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc69ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100]\t\n",
      "Epoch [1/100]\t Training Loss: 5.406936675996122\t lr: 0.001\n",
      "Epoch [1/100]\t Validation Loss: 3.9515698620035677\t lr: 0.001\n",
      "Epoch [1/100]\t\n",
      "Epoch [2/100]\t Training Loss: 4.428898472310332\t lr: 0.001\n",
      "Epoch [2/100]\t Validation Loss: 3.7453821007209487\t lr: 0.001\n",
      "Epoch [2/100]\t\n",
      "Epoch [3/100]\t Training Loss: 4.261738850332587\t lr: 0.001\n",
      "Epoch [3/100]\t Validation Loss: 3.6030167235603816\t lr: 0.001\n",
      "Epoch [3/100]\t\n",
      "Epoch [4/100]\t Training Loss: 4.144184082670285\t lr: 0.001\n",
      "Epoch [4/100]\t Validation Loss: 3.52122652681568\t lr: 0.001\n",
      "Epoch [4/100]\t\n",
      "Epoch [5/100]\t Training Loss: 4.051571045995064\t lr: 0.001\n",
      "Epoch [5/100]\t Validation Loss: 3.4617975633355638\t lr: 0.001\n",
      "Epoch [5/100]\t\n",
      "Epoch [6/100]\t Training Loss: 3.9700740932503624\t lr: 0.001\n",
      "Epoch [6/100]\t Validation Loss: 3.415811061859131\t lr: 0.001\n",
      "Epoch [6/100]\t\n",
      "Epoch [7/100]\t Training Loss: 3.899870818838134\t lr: 0.001\n",
      "Epoch [7/100]\t Validation Loss: 3.3736180746102633\t lr: 0.001\n",
      "Epoch [7/100]\t\n",
      "Epoch [8/100]\t Training Loss: 3.8354372569667103\t lr: 0.001\n",
      "Epoch [8/100]\t Validation Loss: 3.341060264201104\t lr: 0.001\n",
      "Epoch [8/100]\t\n",
      "Epoch [9/100]\t Training Loss: 3.7740861881724403\t lr: 0.001\n",
      "Epoch [9/100]\t Validation Loss: 3.3238424802128272\t lr: 0.001\n",
      "Epoch [9/100]\t\n",
      "Epoch [10/100]\t Training Loss: 3.715667258748008\t lr: 0.001\n",
      "Epoch [10/100]\t Validation Loss: 3.288987597332725\t lr: 0.001\n",
      "Epoch [10/100]\t\n",
      "Epoch [11/100]\t Training Loss: 3.6597774431223757\t lr: 0.001\n",
      "Epoch [11/100]\t Validation Loss: 3.2661866206157057\t lr: 0.001\n",
      "Epoch [11/100]\t\n",
      "Epoch [12/100]\t Training Loss: 3.6057042276767817\t lr: 0.001\n",
      "Epoch [12/100]\t Validation Loss: 3.2442102945303617\t lr: 0.001\n",
      "Epoch [12/100]\t\n",
      "Epoch [13/100]\t Training Loss: 3.554031017796158\t lr: 0.001\n",
      "Epoch [13/100]\t Validation Loss: 3.235694637781457\t lr: 0.001\n",
      "Epoch [13/100]\t\n",
      "Epoch [14/100]\t Training Loss: 3.505851589505325\t lr: 0.001\n",
      "Epoch [14/100]\t Validation Loss: 3.210996190203896\t lr: 0.001\n",
      "Epoch [14/100]\t\n",
      "Epoch [15/100]\t Training Loss: 3.4582036693992517\t lr: 0.001\n",
      "Epoch [15/100]\t Validation Loss: 3.196901535686058\t lr: 0.001\n",
      "Epoch [15/100]\t\n",
      "Epoch [16/100]\t Training Loss: 3.4117283552808835\t lr: 0.001\n",
      "Epoch [16/100]\t Validation Loss: 3.2095671303664584\t lr: 0.001\n",
      "Epoch [16/100]\t\n",
      "Epoch [17/100]\t Training Loss: 3.3673334261950325\t lr: 0.001\n",
      "Epoch [17/100]\t Validation Loss: 3.1921377423443373\t lr: 0.001\n",
      "Epoch [17/100]\t\n",
      "Epoch [18/100]\t Training Loss: 3.324201056414553\t lr: 0.001\n",
      "Epoch [18/100]\t Validation Loss: 3.181676680528665\t lr: 0.001\n",
      "Epoch [18/100]\t\n",
      "Epoch [19/100]\t Training Loss: 3.280967389836031\t lr: 0.001\n",
      "Epoch [19/100]\t Validation Loss: 3.176486386528498\t lr: 0.001\n",
      "Epoch [19/100]\t\n",
      "Epoch [20/100]\t Training Loss: 3.239986875173076\t lr: 0.001\n",
      "Epoch [20/100]\t Validation Loss: 3.179142475128174\t lr: 0.001\n",
      "Epoch [20/100]\t\n",
      "Epoch [21/100]\t Training Loss: 3.199688783387089\t lr: 0.001\n",
      "Epoch [21/100]\t Validation Loss: 3.1727110283284246\t lr: 0.001\n",
      "Epoch [21/100]\t\n",
      "Epoch [22/100]\t Training Loss: 3.161712887342019\t lr: 0.001\n",
      "Epoch [22/100]\t Validation Loss: 3.1596784923650043\t lr: 0.001\n",
      "Epoch [22/100]\t\n",
      "Epoch [23/100]\t Training Loss: 3.1240546056986465\t lr: 0.001\n",
      "Epoch [23/100]\t Validation Loss: 3.1621797628040555\t lr: 0.001\n",
      "Epoch [23/100]\t\n",
      "Epoch [24/100]\t Training Loss: 3.0859020750236024\t lr: 0.001\n",
      "Epoch [24/100]\t Validation Loss: 3.1545295172099825\t lr: 0.001\n",
      "Epoch [24/100]\t\n",
      "Epoch [25/100]\t Training Loss: 3.047384990145788\t lr: 0.001\n",
      "Epoch [25/100]\t Validation Loss: 3.1628799015962623\t lr: 0.001\n",
      "Epoch [25/100]\t\n",
      "Epoch [26/100]\t Training Loss: 3.012322536209965\t lr: 0.001\n",
      "Epoch [26/100]\t Validation Loss: 3.179505269738692\t lr: 0.001\n",
      "Epoch [26/100]\t\n",
      "Epoch [27/100]\t Training Loss: 2.9772686098542667\t lr: 0.001\n",
      "Epoch [27/100]\t Validation Loss: 3.1646356643000737\t lr: 0.001\n",
      "Epoch [27/100]\t\n",
      "Epoch [28/100]\t Training Loss: 2.941561097684114\t lr: 0.001\n",
      "Epoch [28/100]\t Validation Loss: 3.1766624933556664\t lr: 0.001\n",
      "Epoch [28/100]\t\n",
      "Epoch [29/100]\t Training Loss: 2.9091212590941993\t lr: 0.001\n",
      "Epoch [29/100]\t Validation Loss: 3.188689177549338\t lr: 0.001\n",
      "Epoch [29/100]\t\n",
      "Epoch [30/100]\t Training Loss: 2.875185085989325\t lr: 0.001\n",
      "Epoch [30/100]\t Validation Loss: 3.190850550615335\t lr: 0.001\n",
      "Epoch [30/100]\t\n",
      "Epoch [31/100]\t Training Loss: 2.8410495065362253\t lr: 0.001\n",
      "Epoch [31/100]\t Validation Loss: 3.1875703546065317\t lr: 0.001\n",
      "Epoch [31/100]\t\n",
      "Epoch [32/100]\t Training Loss: 2.811204905400191\t lr: 0.001\n",
      "Epoch [32/100]\t Validation Loss: 3.198491383202468\t lr: 0.001\n",
      "Epoch [32/100]\t\n",
      "Epoch [33/100]\t Training Loss: 2.7764195981233017\t lr: 0.001\n",
      "Epoch [33/100]\t Validation Loss: 3.19516072997564\t lr: 0.001\n",
      "Epoch [33/100]\t\n",
      "Epoch [34/100]\t Training Loss: 2.7462033341302896\t lr: 0.001\n",
      "Epoch [34/100]\t Validation Loss: 3.2064883135542086\t lr: 0.001\n",
      "Epoch [34/100]\t\n",
      "Epoch [35/100]\t Training Loss: 2.7170440717731292\t lr: 0.001\n",
      "Epoch [35/100]\t Validation Loss: 3.22517334962193\t lr: 0.001\n",
      "Epoch [35/100]\t\n",
      "Epoch [36/100]\t Training Loss: 2.684449145251223\t lr: 0.001\n",
      "Epoch [36/100]\t Validation Loss: 3.2263332258296917\t lr: 0.001\n",
      "Epoch [36/100]\t\n",
      "Epoch [37/100]\t Training Loss: 2.6557043452397027\t lr: 0.001\n",
      "Epoch [37/100]\t Validation Loss: 3.240355135519293\t lr: 0.001\n",
      "Epoch [37/100]\t\n",
      "Epoch [38/100]\t Training Loss: 2.6225215254537284\t lr: 0.001\n",
      "Epoch [38/100]\t Validation Loss: 3.249346361884588\t lr: 0.001\n",
      "Epoch [38/100]\t\n",
      "Epoch [39/100]\t Training Loss: 2.595300415287847\t lr: 0.001\n",
      "Epoch [39/100]\t Validation Loss: 3.2627196885362455\t lr: 0.001\n",
      "Epoch [39/100]\t\n",
      "Epoch [40/100]\t Training Loss: 2.565287934544751\t lr: 0.001\n",
      "Epoch [40/100]\t Validation Loss: 3.2935945474648776\t lr: 0.001\n",
      "Epoch [40/100]\t\n",
      "Epoch [41/100]\t Training Loss: 2.538197424710559\t lr: 0.001\n",
      "Epoch [41/100]\t Validation Loss: 3.2817161807531043\t lr: 0.001\n",
      "Epoch [41/100]\t\n",
      "Epoch [42/100]\t Training Loss: 2.5111364848778375\t lr: 0.001\n",
      "Epoch [42/100]\t Validation Loss: 3.3170377182055124\t lr: 0.001\n",
      "Epoch [42/100]\t\n",
      "Epoch [43/100]\t Training Loss: 2.4841772817894627\t lr: 0.001\n",
      "Epoch [43/100]\t Validation Loss: 3.3329260107837144\t lr: 0.001\n",
      "Epoch [43/100]\t\n",
      "Epoch [44/100]\t Training Loss: 2.455972247111523\t lr: 0.001\n",
      "Epoch [44/100]\t Validation Loss: 3.342188394522365\t lr: 0.001\n",
      "Epoch [44/100]\t\n",
      "Epoch [45/100]\t Training Loss: 2.4279149292070237\t lr: 0.001\n",
      "Epoch [45/100]\t Validation Loss: 3.344225394574902\t lr: 0.001\n",
      "Epoch [45/100]\t\n",
      "Epoch [46/100]\t Training Loss: 2.4027851890115177\t lr: 0.001\n",
      "Epoch [46/100]\t Validation Loss: 3.361818183826495\t lr: 0.001\n",
      "Epoch [46/100]\t\n",
      "Epoch [47/100]\t Training Loss: 2.3732559821184944\t lr: 0.001\n",
      "Epoch [47/100]\t Validation Loss: 3.371449295478531\t lr: 0.001\n",
      "Epoch [47/100]\t\n",
      "Epoch [48/100]\t Training Loss: 2.3492290278529877\t lr: 0.001\n",
      "Epoch [48/100]\t Validation Loss: 3.393088165717789\t lr: 0.001\n",
      "Epoch [48/100]\t\n",
      "Epoch [49/100]\t Training Loss: 2.3200792775434604\t lr: 0.001\n",
      "Epoch [49/100]\t Validation Loss: 3.4153559570071064\t lr: 0.001\n",
      "Epoch [49/100]\t\n",
      "Epoch [50/100]\t Training Loss: 2.298387373194975\t lr: 0.001\n",
      "Epoch [50/100]\t Validation Loss: 3.4306505088564716\t lr: 0.001\n",
      "Epoch [50/100]\t\n",
      "Epoch [51/100]\t Training Loss: 2.2742997532915274\t lr: 0.001\n",
      "Epoch [51/100]\t Validation Loss: 3.422647467142419\t lr: 0.001\n",
      "Epoch [51/100]\t\n",
      "Epoch [52/100]\t Training Loss: 2.2529477051761755\t lr: 0.001\n",
      "Epoch [52/100]\t Validation Loss: 3.439731513397603\t lr: 0.001\n",
      "Epoch [52/100]\t\n",
      "Epoch [53/100]\t Training Loss: 2.2275845223985367\t lr: 0.001\n",
      "Epoch [53/100]\t Validation Loss: 3.4654782904854304\t lr: 0.001\n",
      "Epoch [53/100]\t\n",
      "Epoch [54/100]\t Training Loss: 2.20072199438539\t lr: 0.001\n",
      "Epoch [54/100]\t Validation Loss: 3.5019767435291147\t lr: 0.001\n",
      "Epoch [54/100]\t\n",
      "Epoch [55/100]\t Training Loss: 2.1772267733083663\t lr: 0.001\n",
      "Epoch [55/100]\t Validation Loss: 3.525626007514664\t lr: 0.001\n",
      "Epoch [55/100]\t\n",
      "Epoch [56/100]\t Training Loss: 2.1500512026155087\t lr: 0.001\n",
      "Epoch [56/100]\t Validation Loss: 3.538610723954213\t lr: 0.001\n",
      "Epoch [56/100]\t\n",
      "Epoch [57/100]\t Training Loss: 2.133593106818626\t lr: 0.001\n",
      "Epoch [57/100]\t Validation Loss: 3.5488762282118014\t lr: 0.001\n",
      "Epoch [57/100]\t\n",
      "Epoch [58/100]\t Training Loss: 2.115880246967306\t lr: 0.001\n",
      "Epoch [58/100]\t Validation Loss: 3.5618234827548645\t lr: 0.001\n",
      "Epoch [58/100]\t\n",
      "Epoch [59/100]\t Training Loss: 2.0915522535743616\t lr: 0.001\n",
      "Epoch [59/100]\t Validation Loss: 3.6157538377785983\t lr: 0.001\n",
      "Epoch [59/100]\t\n",
      "Epoch [60/100]\t Training Loss: 2.0637089453085\t lr: 0.001\n",
      "Epoch [60/100]\t Validation Loss: 3.6261970031110544\t lr: 0.001\n",
      "Epoch [60/100]\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100]\t Training Loss: 2.0421513236697066\t lr: 0.001\n",
      "Epoch [61/100]\t Validation Loss: 3.6591210154038443\t lr: 0.001\n",
      "Epoch [61/100]\t\n",
      "Epoch [62/100]\t Training Loss: 2.022821721213553\t lr: 0.001\n",
      "Epoch [62/100]\t Validation Loss: 3.6626508779163602\t lr: 0.001\n",
      "Epoch [62/100]\t\n",
      "Epoch [63/100]\t Training Loss: 2.002176820164751\t lr: 0.001\n",
      "Epoch [63/100]\t Validation Loss: 3.7089401317548147\t lr: 0.001\n",
      "Epoch [63/100]\t\n",
      "Epoch [64/100]\t Training Loss: 1.9826039868547483\t lr: 0.001\n",
      "Epoch [64/100]\t Validation Loss: 3.700410353986523\t lr: 0.001\n",
      "Epoch [64/100]\t\n",
      "Epoch [65/100]\t Training Loss: 1.9654374704946338\t lr: 0.001\n",
      "Epoch [65/100]\t Validation Loss: 3.694437078282803\t lr: 0.001\n",
      "Epoch [65/100]\t\n",
      "Epoch [66/100]\t Training Loss: 1.9439314780637735\t lr: 0.001\n",
      "Epoch [66/100]\t Validation Loss: 3.7472692169720614\t lr: 0.001\n",
      "Epoch [66/100]\t\n",
      "Epoch [67/100]\t Training Loss: 1.9276827305479123\t lr: 0.001\n",
      "Epoch [67/100]\t Validation Loss: 3.7558007210115845\t lr: 0.001\n",
      "Epoch [67/100]\t\n",
      "Epoch [68/100]\t Training Loss: 1.9063801283726607\t lr: 0.001\n",
      "Epoch [68/100]\t Validation Loss: 3.767597916760022\t lr: 0.001\n",
      "Epoch [68/100]\t\n",
      "Epoch [69/100]\t Training Loss: 1.8871210786082862\t lr: 0.001\n",
      "Epoch [69/100]\t Validation Loss: 3.821662220773818\t lr: 0.001\n",
      "Epoch [69/100]\t\n",
      "Epoch [70/100]\t Training Loss: 1.8720489932448052\t lr: 0.001\n",
      "Epoch [70/100]\t Validation Loss: 3.845337940167777\t lr: 0.001\n",
      "Epoch [70/100]\t\n",
      "Epoch [71/100]\t Training Loss: 1.8494214005482472\t lr: 0.001\n",
      "Epoch [71/100]\t Validation Loss: 3.824543687361705\t lr: 0.001\n",
      "Epoch [71/100]\t\n",
      "Epoch [72/100]\t Training Loss: 1.8276731385599316\t lr: 0.001\n",
      "Epoch [72/100]\t Validation Loss: 3.8436228142508977\t lr: 0.001\n",
      "Epoch [72/100]\t\n",
      "Epoch [73/100]\t Training Loss: 1.817532367413611\t lr: 0.001\n",
      "Epoch [73/100]\t Validation Loss: 3.8537137508392334\t lr: 0.001\n",
      "Epoch [73/100]\t\n",
      "Epoch [74/100]\t Training Loss: 1.7974290634360155\t lr: 0.001\n",
      "Epoch [74/100]\t Validation Loss: 3.9169104159632817\t lr: 0.001\n",
      "Epoch [74/100]\t\n",
      "Epoch [75/100]\t Training Loss: 1.785689601812826\t lr: 0.001\n",
      "Epoch [75/100]\t Validation Loss: 3.9115933225124695\t lr: 0.001\n",
      "Epoch [75/100]\t\n",
      "Epoch [76/100]\t Training Loss: 1.7590695006768111\t lr: 0.001\n",
      "Epoch [76/100]\t Validation Loss: 3.956200319000437\t lr: 0.001\n",
      "Epoch [76/100]\t\n",
      "Epoch [77/100]\t Training Loss: 1.7520048490265752\t lr: 0.001\n",
      "Epoch [77/100]\t Validation Loss: 3.934354576883437\t lr: 0.001\n",
      "Epoch [77/100]\t\n",
      "Epoch [78/100]\t Training Loss: 1.732583865790111\t lr: 0.001\n",
      "Epoch [78/100]\t Validation Loss: 3.9848434019692336\t lr: 0.001\n",
      "Epoch [78/100]\t\n",
      "Epoch [79/100]\t Training Loss: 1.7141985371899422\t lr: 0.001\n",
      "Epoch [79/100]\t Validation Loss: 3.9998512509502944\t lr: 0.001\n",
      "Epoch [79/100]\t\n",
      "Epoch [80/100]\t Training Loss: 1.6997142490523551\t lr: 0.001\n",
      "Epoch [80/100]\t Validation Loss: 4.016462974910494\t lr: 0.001\n",
      "Epoch [80/100]\t\n",
      "Epoch [81/100]\t Training Loss: 1.6862271402193152\t lr: 0.001\n",
      "Epoch [81/100]\t Validation Loss: 4.0357973424694205\t lr: 0.001\n",
      "Epoch [81/100]\t\n",
      "Epoch [82/100]\t Training Loss: 1.6711282705711892\t lr: 0.001\n",
      "Epoch [82/100]\t Validation Loss: 4.067379516891286\t lr: 0.001\n",
      "Epoch [82/100]\t\n",
      "Epoch [83/100]\t Training Loss: 1.6547370996621564\t lr: 0.001\n",
      "Epoch [83/100]\t Validation Loss: 4.148085044909127\t lr: 0.001\n",
      "Epoch [83/100]\t\n",
      "Epoch [84/100]\t Training Loss: 1.6474588627705489\t lr: 0.001\n",
      "Epoch [84/100]\t Validation Loss: 4.080255294147926\t lr: 0.001\n",
      "Epoch [84/100]\t\n",
      "Epoch [85/100]\t Training Loss: 1.6285362929639304\t lr: 0.001\n",
      "Epoch [85/100]\t Validation Loss: 4.114179719852496\t lr: 0.001\n",
      "Epoch [85/100]\t\n",
      "Epoch [86/100]\t Training Loss: 1.6113555318559223\t lr: 0.001\n",
      "Epoch [86/100]\t Validation Loss: 4.182171740109408\t lr: 0.001\n",
      "Epoch [86/100]\t\n",
      "Epoch [87/100]\t Training Loss: 1.593743281596152\t lr: 0.001\n",
      "Epoch [87/100]\t Validation Loss: 4.172725514520573\t lr: 0.001\n",
      "Epoch [87/100]\t\n",
      "Epoch [88/100]\t Training Loss: 1.5756243549649367\t lr: 0.001\n",
      "Epoch [88/100]\t Validation Loss: 4.2074399809294105\t lr: 0.001\n",
      "Epoch [88/100]\t\n",
      "Epoch [89/100]\t Training Loss: 1.5696180412531509\t lr: 0.001\n",
      "Epoch [89/100]\t Validation Loss: 4.221068527125105\t lr: 0.001\n",
      "Epoch [89/100]\t\n",
      "Epoch [90/100]\t Training Loss: 1.5545848992169666\t lr: 0.001\n",
      "Epoch [90/100]\t Validation Loss: 4.1988089597677885\t lr: 0.001\n",
      "Epoch [90/100]\t\n",
      "Epoch [91/100]\t Training Loss: 1.5456609680219684\t lr: 0.001\n",
      "Epoch [91/100]\t Validation Loss: 4.2416236611861216\t lr: 0.001\n",
      "Epoch [91/100]\t\n",
      "Epoch [92/100]\t Training Loss: 1.527102088684316\t lr: 0.001\n",
      "Epoch [92/100]\t Validation Loss: 4.272642980647992\t lr: 0.001\n",
      "Epoch [92/100]\t\n",
      "Epoch [93/100]\t Training Loss: 1.5140171743110014\t lr: 0.001\n",
      "Epoch [93/100]\t Validation Loss: 4.308876460111594\t lr: 0.001\n",
      "Epoch [93/100]\t\n",
      "Epoch [94/100]\t Training Loss: 1.5047514514849925\t lr: 0.001\n",
      "Epoch [94/100]\t Validation Loss: 4.35599878468091\t lr: 0.001\n",
      "Epoch [94/100]\t\n",
      "Epoch [95/100]\t Training Loss: 1.4833851857563418\t lr: 0.001\n",
      "Epoch [95/100]\t Validation Loss: 4.3207895997204355\t lr: 0.001\n",
      "Epoch [95/100]\t\n",
      "Epoch [96/100]\t Training Loss: 1.4736794522961083\t lr: 0.001\n",
      "Epoch [96/100]\t Validation Loss: 4.378935222384296\t lr: 0.001\n",
      "Epoch [96/100]\t\n",
      "Epoch [97/100]\t Training Loss: 1.4641120317951797\t lr: 0.001\n",
      "Epoch [97/100]\t Validation Loss: 4.385768639890453\t lr: 0.001\n",
      "Epoch [97/100]\t\n",
      "Epoch [98/100]\t Training Loss: 1.4529536928972016\t lr: 0.001\n",
      "Epoch [98/100]\t Validation Loss: 4.433043078531193\t lr: 0.001\n",
      "Epoch [98/100]\t\n",
      "Epoch [99/100]\t Training Loss: 1.4404979796360826\t lr: 0.001\n",
      "Epoch [99/100]\t Validation Loss: 4.469889025144939\t lr: 0.001\n",
      "Epoch [99/100]\t\n",
      "Epoch [100/100]\t Training Loss: 1.425695172661101\t lr: 0.001\n",
      "Epoch [100/100]\t Validation Loss: 4.480880710143078\t lr: 0.001\n",
      "Epoch [99/100]\t Time Taken: 21.99596269528071 minutes\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "net_adv_ni = copy.deepcopy(net_ori) #am I doing correct? \n",
    "\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_adv_ni.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = './cifar_base_adv_nifgsm.pth'\n",
    "device = \"cuda\"\n",
    "writer = SummaryWriter(\"Adv_Base_nifgsm\")\n",
    "\n",
    "ni_attack = generate_adv(net_adv_ni, \"nifgsm\")\n",
    "\n",
    "train(net_adv_ni, train_loader, train_criterion,val_loader, val_criterion, \n",
    "              optimizer, 100, writer, \n",
    "                 PATH, True, ni_attack, device=\"cuda\",\n",
    "                  n_steps_show=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "badd6cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 26 %\n"
     ]
    }
   ],
   "source": [
    "#test clean accuray\n",
    "test(val_loader, net_adv_ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15341ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD\n",
      "[Model loaded]\n",
      "Acc: 17.50 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"PGD\")\n",
    "pgd_attack_transfer = generate_adv(net_ori, \"pgd\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "pgd_acc = test_attack(val_loader, pgd_attack_transfer, net_adv_ni, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ca0852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgsm\n",
      "[Model loaded]\n",
      "Acc: 17.41 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"fgsm\")\n",
    "fgsm_attack_transfer = generate_adv(net_ori, \"fgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "fgsm_acc = test_attack(val_loader, fgsm_attack_transfer, net_adv_ni, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70a20e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nifgsm\n",
      "[Model loaded]\n",
      "Acc: 17.63 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"nifgsm\")\n",
    "nifgsm_attack_transfer = generate_adv(net_ori, \"nifgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "nifgsm_acc = test_attack(val_loader, nifgsm_attack_transfer, net_adv_ni, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1cde73b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmifgsm\n",
      "[Model loaded]\n",
      "Acc: 17.17 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"vmifgsm\")\n",
    "vmifgsm_attack_transfer = generate_adv(net_ori, \"vmifgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "vmifgsm_acc = test_attack(val_loader, vmifgsm_attack_transfer, net_adv_ni, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d1d9d",
   "metadata": {},
   "source": [
    "# TODO: Adversarial Training by vmifgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bce77f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100]\t\n",
      "Epoch [1/100]\t Training Loss: 5.593234006096335\t lr: 0.001\n",
      "Epoch [1/100]\t Validation Loss: 4.114244599885579\t lr: 0.001\n",
      "Epoch [1/100]\t\n",
      "Epoch [2/100]\t Training Loss: 4.539997804500258\t lr: 0.001\n",
      "Epoch [2/100]\t Validation Loss: 3.916336949867538\t lr: 0.001\n",
      "Epoch [2/100]\t\n",
      "Epoch [3/100]\t Training Loss: 4.402561780436875\t lr: 0.001\n",
      "Epoch [3/100]\t Validation Loss: 3.8532697339601154\t lr: 0.001\n",
      "Epoch [3/100]\t\n",
      "Epoch [4/100]\t Training Loss: 4.308990769983862\t lr: 0.001\n",
      "Epoch [4/100]\t Validation Loss: 3.7676802586905564\t lr: 0.001\n",
      "Epoch [4/100]\t\n",
      "Epoch [5/100]\t Training Loss: 4.240994041837999\t lr: 0.001\n",
      "Epoch [5/100]\t Validation Loss: 3.6924394112599046\t lr: 0.001\n",
      "Epoch [5/100]\t\n",
      "Epoch [6/100]\t Training Loss: 4.188209241613403\t lr: 0.001\n",
      "Epoch [6/100]\t Validation Loss: 3.635173864002469\t lr: 0.001\n",
      "Epoch [6/100]\t\n",
      "Epoch [7/100]\t Training Loss: 4.142478986164493\t lr: 0.001\n",
      "Epoch [7/100]\t Validation Loss: 3.5910115423081796\t lr: 0.001\n",
      "Epoch [7/100]\t\n",
      "Epoch [8/100]\t Training Loss: 4.1043764258284705\t lr: 0.001\n",
      "Epoch [8/100]\t Validation Loss: 3.551342257970496\t lr: 0.001\n",
      "Epoch [8/100]\t\n",
      "Epoch [9/100]\t Training Loss: 4.070921768007985\t lr: 0.001\n",
      "Epoch [9/100]\t Validation Loss: 3.53062658973887\t lr: 0.001\n",
      "Epoch [9/100]\t\n",
      "Epoch [10/100]\t Training Loss: 4.039475704397996\t lr: 0.001\n",
      "Epoch [10/100]\t Validation Loss: 3.5027338190923762\t lr: 0.001\n",
      "Epoch [10/100]\t\n",
      "Epoch [11/100]\t Training Loss: 4.013275610516443\t lr: 0.001\n",
      "Epoch [11/100]\t Validation Loss: 3.4763606741458557\t lr: 0.001\n",
      "Epoch [11/100]\t\n",
      "Epoch [12/100]\t Training Loss: 3.9862319953606256\t lr: 0.001\n",
      "Epoch [12/100]\t Validation Loss: 3.4629068827327294\t lr: 0.001\n",
      "Epoch [12/100]\t\n",
      "Epoch [13/100]\t Training Loss: 3.961923250456905\t lr: 0.001\n",
      "Epoch [13/100]\t Validation Loss: 3.439589983300318\t lr: 0.001\n",
      "Epoch [13/100]\t\n",
      "Epoch [14/100]\t Training Loss: 3.940082451876472\t lr: 0.001\n",
      "Epoch [14/100]\t Validation Loss: 3.4116117199764977\t lr: 0.001\n",
      "Epoch [14/100]\t\n",
      "Epoch [15/100]\t Training Loss: 3.918913144894573\t lr: 0.001\n",
      "Epoch [15/100]\t Validation Loss: 3.402170320100422\t lr: 0.001\n",
      "Epoch [15/100]\t\n",
      "Epoch [16/100]\t Training Loss: 3.8975862407928235\t lr: 0.001\n",
      "Epoch [16/100]\t Validation Loss: 3.3832411766052246\t lr: 0.001\n",
      "Epoch [16/100]\t\n",
      "Epoch [17/100]\t Training Loss: 3.878651604932897\t lr: 0.001\n",
      "Epoch [17/100]\t Validation Loss: 3.3656239781198622\t lr: 0.001\n",
      "Epoch [17/100]\t\n",
      "Epoch [18/100]\t Training Loss: 3.8578880882019275\t lr: 0.001\n",
      "Epoch [18/100]\t Validation Loss: 3.352239044406746\t lr: 0.001\n",
      "Epoch [18/100]\t\n",
      "Epoch [19/100]\t Training Loss: 3.838983733330846\t lr: 0.001\n",
      "Epoch [19/100]\t Validation Loss: 3.3307053318506554\t lr: 0.001\n",
      "Epoch [19/100]\t\n",
      "Epoch [20/100]\t Training Loss: 3.822046174417676\t lr: 0.001\n",
      "Epoch [20/100]\t Validation Loss: 3.321318529829194\t lr: 0.001\n",
      "Epoch [20/100]\t\n",
      "Epoch [21/100]\t Training Loss: 3.8029667270153076\t lr: 0.001\n",
      "Epoch [21/100]\t Validation Loss: 3.314187922055208\t lr: 0.001\n",
      "Epoch [21/100]\t\n",
      "Epoch [22/100]\t Training Loss: 3.7860808415181193\t lr: 0.001\n",
      "Epoch [22/100]\t Validation Loss: 3.3058667846872836\t lr: 0.001\n",
      "Epoch [22/100]\t\n",
      "Epoch [23/100]\t Training Loss: 3.769466823933984\t lr: 0.001\n",
      "Epoch [23/100]\t Validation Loss: 3.2833629801303528\t lr: 0.001\n",
      "Epoch [23/100]\t\n",
      "Epoch [24/100]\t Training Loss: 3.749143854736367\t lr: 0.001\n",
      "Epoch [24/100]\t Validation Loss: 3.282764425760583\t lr: 0.001\n",
      "Epoch [24/100]\t\n",
      "Epoch [25/100]\t Training Loss: 3.7343103952724914\t lr: 0.001\n",
      "Epoch [25/100]\t Validation Loss: 3.274840442440178\t lr: 0.001\n",
      "Epoch [25/100]\t\n",
      "Epoch [26/100]\t Training Loss: 3.716839293384796\t lr: 0.001\n",
      "Epoch [26/100]\t Validation Loss: 3.2558108372024344\t lr: 0.001\n",
      "Epoch [26/100]\t\n",
      "Epoch [27/100]\t Training Loss: 3.7016114054433524\t lr: 0.001\n",
      "Epoch [27/100]\t Validation Loss: 3.2520902337907236\t lr: 0.001\n",
      "Epoch [27/100]\t\n",
      "Epoch [28/100]\t Training Loss: 3.6837309870268684\t lr: 0.001\n",
      "Epoch [28/100]\t Validation Loss: 3.2457692834395395\t lr: 0.001\n",
      "Epoch [28/100]\t\n",
      "Epoch [29/100]\t Training Loss: 3.670307475282713\t lr: 0.001\n",
      "Epoch [29/100]\t Validation Loss: 3.2364087768747836\t lr: 0.001\n",
      "Epoch [29/100]\t\n",
      "Epoch [30/100]\t Training Loss: 3.652425368423657\t lr: 0.001\n",
      "Epoch [30/100]\t Validation Loss: 3.235381491576569\t lr: 0.001\n",
      "Epoch [30/100]\t\n",
      "Epoch [31/100]\t Training Loss: 3.6372084446880213\t lr: 0.001\n",
      "Epoch [31/100]\t Validation Loss: 3.2303658497484427\t lr: 0.001\n",
      "Epoch [31/100]\t\n",
      "Epoch [32/100]\t Training Loss: 3.6211816002340877\t lr: 0.001\n",
      "Epoch [32/100]\t Validation Loss: 3.2151272115828116\t lr: 0.001\n",
      "Epoch [32/100]\t\n",
      "Epoch [33/100]\t Training Loss: 3.6055889050369068\t lr: 0.001\n",
      "Epoch [33/100]\t Validation Loss: 3.2046839738193946\t lr: 0.001\n",
      "Epoch [33/100]\t\n",
      "Epoch [34/100]\t Training Loss: 3.590039568483982\t lr: 0.001\n",
      "Epoch [34/100]\t Validation Loss: 3.198602042620695\t lr: 0.001\n",
      "Epoch [34/100]\t\n",
      "Epoch [35/100]\t Training Loss: 3.575138236555602\t lr: 0.001\n",
      "Epoch [35/100]\t Validation Loss: 3.1945837751219543\t lr: 0.001\n",
      "Epoch [35/100]\t\n",
      "Epoch [36/100]\t Training Loss: 3.5584448639998962\t lr: 0.001\n",
      "Epoch [36/100]\t Validation Loss: 3.194614546208442\t lr: 0.001\n",
      "Epoch [36/100]\t\n",
      "Epoch [37/100]\t Training Loss: 3.5440848276133425\t lr: 0.001\n",
      "Epoch [37/100]\t Validation Loss: 3.2057800172250484\t lr: 0.001\n",
      "Epoch [37/100]\t\n",
      "Epoch [38/100]\t Training Loss: 3.5267761643890223\t lr: 0.001\n",
      "Epoch [38/100]\t Validation Loss: 3.196714564214779\t lr: 0.001\n",
      "Epoch [38/100]\t\n",
      "Epoch [39/100]\t Training Loss: 3.5128762222007106\t lr: 0.001\n",
      "Epoch [39/100]\t Validation Loss: 3.182453695731827\t lr: 0.001\n",
      "Epoch [39/100]\t\n",
      "Epoch [40/100]\t Training Loss: 3.4984856621383704\t lr: 0.001\n",
      "Epoch [40/100]\t Validation Loss: 3.1818101345738277\t lr: 0.001\n",
      "Epoch [40/100]\t\n",
      "Epoch [41/100]\t Training Loss: 3.485105768189101\t lr: 0.001\n",
      "Epoch [41/100]\t Validation Loss: 3.170437438578545\t lr: 0.001\n",
      "Epoch [41/100]\t\n",
      "Epoch [42/100]\t Training Loss: 3.4679531953523837\t lr: 0.001\n",
      "Epoch [42/100]\t Validation Loss: 3.1646438852141174\t lr: 0.001\n",
      "Epoch [42/100]\t\n",
      "Epoch [43/100]\t Training Loss: 3.452919377085498\t lr: 0.001\n",
      "Epoch [43/100]\t Validation Loss: 3.1597334795360323\t lr: 0.001\n",
      "Epoch [43/100]\t\n",
      "Epoch [44/100]\t Training Loss: 3.4361711774030916\t lr: 0.001\n",
      "Epoch [44/100]\t Validation Loss: 3.1668653427800044\t lr: 0.001\n",
      "Epoch [44/100]\t\n",
      "Epoch [45/100]\t Training Loss: 3.42290812319197\t lr: 0.001\n",
      "Epoch [45/100]\t Validation Loss: 3.163219011282619\t lr: 0.001\n",
      "Epoch [45/100]\t\n",
      "Epoch [46/100]\t Training Loss: 3.407008645479636\t lr: 0.001\n",
      "Epoch [46/100]\t Validation Loss: 3.1499199655991568\t lr: 0.001\n",
      "Epoch [46/100]\t\n",
      "Epoch [47/100]\t Training Loss: 3.393336262544403\t lr: 0.001\n",
      "Epoch [47/100]\t Validation Loss: 3.1598875432074824\t lr: 0.001\n",
      "Epoch [47/100]\t\n",
      "Epoch [48/100]\t Training Loss: 3.3786224184743583\t lr: 0.001\n",
      "Epoch [48/100]\t Validation Loss: 3.1420036931581135\t lr: 0.001\n",
      "Epoch [48/100]\t\n",
      "Epoch [49/100]\t Training Loss: 3.362354526129525\t lr: 0.001\n",
      "Epoch [49/100]\t Validation Loss: 3.156451285639896\t lr: 0.001\n",
      "Epoch [49/100]\t\n",
      "Epoch [50/100]\t Training Loss: 3.3482214837427944\t lr: 0.001\n",
      "Epoch [50/100]\t Validation Loss: 3.1514234180691876\t lr: 0.001\n",
      "Epoch [50/100]\t\n",
      "Epoch [51/100]\t Training Loss: 3.333164132159689\t lr: 0.001\n",
      "Epoch [51/100]\t Validation Loss: 3.152054050300695\t lr: 0.001\n",
      "Epoch [51/100]\t\n",
      "Epoch [52/100]\t Training Loss: 3.3194925150907864\t lr: 0.001\n",
      "Epoch [52/100]\t Validation Loss: 3.153983897800687\t lr: 0.001\n",
      "Epoch [52/100]\t\n",
      "Epoch [53/100]\t Training Loss: 3.303348969925395\t lr: 0.001\n",
      "Epoch [53/100]\t Validation Loss: 3.1542037710358826\t lr: 0.001\n",
      "Epoch [53/100]\t\n",
      "Epoch [54/100]\t Training Loss: 3.288179167091389\t lr: 0.001\n",
      "Epoch [54/100]\t Validation Loss: 3.138737938072108\t lr: 0.001\n",
      "Epoch [54/100]\t\n",
      "Epoch [55/100]\t Training Loss: 3.2738556416748126\t lr: 0.001\n",
      "Epoch [55/100]\t Validation Loss: 3.1484739991682993\t lr: 0.001\n",
      "Epoch [55/100]\t\n",
      "Epoch [56/100]\t Training Loss: 3.2613249294593207\t lr: 0.001\n",
      "Epoch [56/100]\t Validation Loss: 3.155300300332564\t lr: 0.001\n",
      "Epoch [56/100]\t\n",
      "Epoch [57/100]\t Training Loss: 3.246646654270494\t lr: 0.001\n",
      "Epoch [57/100]\t Validation Loss: 3.1488040036792997\t lr: 0.001\n",
      "Epoch [57/100]\t\n",
      "Epoch [58/100]\t Training Loss: 3.2312004047891367\t lr: 0.001\n",
      "Epoch [58/100]\t Validation Loss: 3.146873561641838\t lr: 0.001\n",
      "Epoch [58/100]\t\n",
      "Epoch [59/100]\t Training Loss: 3.217392051921171\t lr: 0.001\n",
      "Epoch [59/100]\t Validation Loss: 3.143713359591327\t lr: 0.001\n",
      "Epoch [59/100]\t\n",
      "Epoch [60/100]\t Training Loss: 3.2017815058188672\t lr: 0.001\n",
      "Epoch [60/100]\t Validation Loss: 3.136601417879515\t lr: 0.001\n",
      "Epoch [60/100]\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100]\t Training Loss: 3.184679027103707\t lr: 0.001\n",
      "Epoch [61/100]\t Validation Loss: 3.1444551642936998\t lr: 0.001\n",
      "Epoch [61/100]\t\n",
      "Epoch [62/100]\t Training Loss: 3.1722247923731497\t lr: 0.001\n",
      "Epoch [62/100]\t Validation Loss: 3.149228729779207\t lr: 0.001\n",
      "Epoch [62/100]\t\n",
      "Epoch [63/100]\t Training Loss: 3.1564481386443233\t lr: 0.001\n",
      "Epoch [63/100]\t Validation Loss: 3.1411635785163203\t lr: 0.001\n",
      "Epoch [63/100]\t\n",
      "Epoch [64/100]\t Training Loss: 3.141174600557293\t lr: 0.001\n",
      "Epoch [64/100]\t Validation Loss: 3.144280711306801\t lr: 0.001\n",
      "Epoch [64/100]\t\n",
      "Epoch [65/100]\t Training Loss: 3.126172642573676\t lr: 0.001\n",
      "Epoch [65/100]\t Validation Loss: 3.162195260011697\t lr: 0.001\n",
      "Epoch [65/100]\t\n",
      "Epoch [66/100]\t Training Loss: 3.1116788454372863\t lr: 0.001\n",
      "Epoch [66/100]\t Validation Loss: 3.145811784116528\t lr: 0.001\n",
      "Epoch [66/100]\t\n",
      "Epoch [67/100]\t Training Loss: 3.098067507414562\t lr: 0.001\n",
      "Epoch [67/100]\t Validation Loss: 3.1461163744141785\t lr: 0.001\n",
      "Epoch [67/100]\t\n",
      "Epoch [68/100]\t Training Loss: 3.0826654220785934\t lr: 0.001\n",
      "Epoch [68/100]\t Validation Loss: 3.141711002663721\t lr: 0.001\n",
      "Epoch [68/100]\t\n",
      "Epoch [69/100]\t Training Loss: 3.0681204765349093\t lr: 0.001\n",
      "Epoch [69/100]\t Validation Loss: 3.162806710110435\t lr: 0.001\n",
      "Epoch [69/100]\t\n",
      "Epoch [70/100]\t Training Loss: 3.0528683619730916\t lr: 0.001\n",
      "Epoch [70/100]\t Validation Loss: 3.1604892181444773\t lr: 0.001\n",
      "Epoch [70/100]\t\n",
      "Epoch [71/100]\t Training Loss: 3.03874468925359\t lr: 0.001\n",
      "Epoch [71/100]\t Validation Loss: 3.166491592986674\t lr: 0.001\n",
      "Epoch [71/100]\t\n",
      "Epoch [72/100]\t Training Loss: 3.02697656038777\t lr: 0.001\n",
      "Epoch [72/100]\t Validation Loss: 3.168542855902563\t lr: 0.001\n",
      "Epoch [72/100]\t\n",
      "Epoch [73/100]\t Training Loss: 3.0101166491008478\t lr: 0.001\n",
      "Epoch [73/100]\t Validation Loss: 3.1674980634375465\t lr: 0.001\n",
      "Epoch [73/100]\t\n",
      "Epoch [74/100]\t Training Loss: 2.9972873459691587\t lr: 0.001\n",
      "Epoch [74/100]\t Validation Loss: 3.167165683794625\t lr: 0.001\n",
      "Epoch [74/100]\t\n",
      "Epoch [75/100]\t Training Loss: 2.9846658749348673\t lr: 0.001\n",
      "Epoch [75/100]\t Validation Loss: 3.1782832085331783\t lr: 0.001\n",
      "Epoch [75/100]\t\n",
      "Epoch [76/100]\t Training Loss: 2.968951836266481\t lr: 0.001\n",
      "Epoch [76/100]\t Validation Loss: 3.188764867903311\t lr: 0.001\n",
      "Epoch [76/100]\t\n",
      "Epoch [77/100]\t Training Loss: 2.9541143791754836\t lr: 0.001\n",
      "Epoch [77/100]\t Validation Loss: 3.205813250964201\t lr: 0.001\n",
      "Epoch [77/100]\t\n",
      "Epoch [78/100]\t Training Loss: 2.940001397486538\t lr: 0.001\n",
      "Epoch [78/100]\t Validation Loss: 3.185375270964224\t lr: 0.001\n",
      "Epoch [78/100]\t\n",
      "Epoch [79/100]\t Training Loss: 2.9257004297602816\t lr: 0.001\n",
      "Epoch [79/100]\t Validation Loss: 3.2009765558604952\t lr: 0.001\n",
      "Epoch [79/100]\t\n",
      "Epoch [80/100]\t Training Loss: 2.912101423039156\t lr: 0.001\n",
      "Epoch [80/100]\t Validation Loss: 3.2051533204090745\t lr: 0.001\n",
      "Epoch [80/100]\t\n",
      "Epoch [81/100]\t Training Loss: 2.896160309272044\t lr: 0.001\n",
      "Epoch [81/100]\t Validation Loss: 3.2360467850407466\t lr: 0.001\n",
      "Epoch [81/100]\t\n",
      "Epoch [82/100]\t Training Loss: 2.884959815408263\t lr: 0.001\n",
      "Epoch [82/100]\t Validation Loss: 3.2151247610019733\t lr: 0.001\n",
      "Epoch [82/100]\t\n",
      "Epoch [83/100]\t Training Loss: 2.8690574724046165\t lr: 0.001\n",
      "Epoch [83/100]\t Validation Loss: 3.2253315750556655\t lr: 0.001\n",
      "Epoch [83/100]\t\n",
      "Epoch [84/100]\t Training Loss: 2.8554871070110583\t lr: 0.001\n",
      "Epoch [84/100]\t Validation Loss: 3.2390033202835276\t lr: 0.001\n",
      "Epoch [84/100]\t\n",
      "Epoch [85/100]\t Training Loss: 2.845775526807741\t lr: 0.001\n",
      "Epoch [85/100]\t Validation Loss: 3.242340866523453\t lr: 0.001\n",
      "Epoch [85/100]\t\n",
      "Epoch [86/100]\t Training Loss: 2.8264379769639896\t lr: 0.001\n",
      "Epoch [86/100]\t Validation Loss: 3.2487176007862333\t lr: 0.001\n",
      "Epoch [86/100]\t\n",
      "Epoch [87/100]\t Training Loss: 2.813262718107999\t lr: 0.001\n",
      "Epoch [87/100]\t Validation Loss: 3.261438405966457\t lr: 0.001\n",
      "Epoch [87/100]\t\n",
      "Epoch [88/100]\t Training Loss: 2.8009486686238243\t lr: 0.001\n",
      "Epoch [88/100]\t Validation Loss: 3.2343500897854187\t lr: 0.001\n",
      "Epoch [88/100]\t\n",
      "Epoch [89/100]\t Training Loss: 2.7865606476278866\t lr: 0.001\n",
      "Epoch [89/100]\t Validation Loss: 3.2677376511730727\t lr: 0.001\n",
      "Epoch [89/100]\t\n",
      "Epoch [90/100]\t Training Loss: 2.7746789961519753\t lr: 0.001\n",
      "Epoch [90/100]\t Validation Loss: 3.2756548350370385\t lr: 0.001\n",
      "Epoch [90/100]\t\n",
      "Epoch [91/100]\t Training Loss: 2.7609682138008838\t lr: 0.001\n",
      "Epoch [91/100]\t Validation Loss: 3.280229483978658\t lr: 0.001\n",
      "Epoch [91/100]\t\n",
      "Epoch [92/100]\t Training Loss: 2.7443129394365395\t lr: 0.001\n",
      "Epoch [92/100]\t Validation Loss: 3.2836594762681406\t lr: 0.001\n",
      "Epoch [92/100]\t\n",
      "Epoch [93/100]\t Training Loss: 2.730873846946775\t lr: 0.001\n",
      "Epoch [93/100]\t Validation Loss: 3.294764570043057\t lr: 0.001\n",
      "Epoch [93/100]\t\n",
      "Epoch [94/100]\t Training Loss: 2.719192326831086\t lr: 0.001\n",
      "Epoch [94/100]\t Validation Loss: 3.3066150930863394\t lr: 0.001\n",
      "Epoch [94/100]\t\n",
      "Epoch [95/100]\t Training Loss: 2.707096430346789\t lr: 0.001\n",
      "Epoch [95/100]\t Validation Loss: 3.30968287926686\t lr: 0.001\n",
      "Epoch [95/100]\t\n",
      "Epoch [96/100]\t Training Loss: 2.6914271546141877\t lr: 0.001\n",
      "Epoch [96/100]\t Validation Loss: 3.314769234838365\t lr: 0.001\n",
      "Epoch [96/100]\t\n",
      "Epoch [97/100]\t Training Loss: 2.678776991641735\t lr: 0.001\n",
      "Epoch [97/100]\t Validation Loss: 3.323166898534268\t lr: 0.001\n",
      "Epoch [97/100]\t\n",
      "Epoch [98/100]\t Training Loss: 2.6684054663723997\t lr: 0.001\n",
      "Epoch [98/100]\t Validation Loss: 3.3335684643516057\t lr: 0.001\n",
      "Epoch [98/100]\t\n",
      "Epoch [99/100]\t Training Loss: 2.6529213845577386\t lr: 0.001\n",
      "Epoch [99/100]\t Validation Loss: 3.355159385294854\t lr: 0.001\n",
      "Epoch [99/100]\t\n",
      "Epoch [100/100]\t Training Loss: 2.6401894909646506\t lr: 0.001\n",
      "Epoch [100/100]\t Validation Loss: 3.331891174557843\t lr: 0.001\n",
      "Epoch [99/100]\t Time Taken: 73.83807187477747 minutes\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "net_vmifgsm = copy.deepcopy(net_ori) \n",
    "\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_vmifgsm.parameters(), lr=0.001, momentum=0.9)\n",
    "PATH = './cifar_base_adv_vmifgsm.pth'\n",
    "device = \"cuda\"\n",
    "\n",
    "writer = SummaryWriter(\"Adv_Base_vmifgsm\")\n",
    "vmi_attack = generate_adv(net_vmifgsm, \"vmifgsm\")\n",
    "\n",
    "train(net_vmifgsm, train_loader, train_criterion,val_loader, val_criterion, \n",
    "              optimizer, 100, writer, \n",
    "                 PATH, True, vmi_attack, device=\"cuda\",\n",
    "                  n_steps_show=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e581647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 26 %\n"
     ]
    }
   ],
   "source": [
    "#test clean accuray\n",
    "test(val_loader, net_vmifgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5dfbcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD\n",
      "[Model loaded]\n",
      "Acc: 20.23 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"PGD\")\n",
    "pgd_attack_transfer = generate_adv(net_ori, \"pgd\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "pgd_acc = test_attack(val_loader, pgd_attack_transfer, net_vmifgsm, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6260ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgsm\n",
      "[Model loaded]\n",
      "Acc: 20.03 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"fgsm\")\n",
    "fgsm_attack_transfer = generate_adv(net_ori, \"fgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "fgsm_acc = test_attack(val_loader, fgsm_attack_transfer, net_vmifgsm, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28a0f824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nifgsm\n",
      "[Model loaded]\n",
      "Acc: 20.30 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"nifgsm\")\n",
    "nifgsm_attack_transfer = generate_adv(net_ori, \"nifgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "nifgsm_acc = test_attack(val_loader, nifgsm_attack_transfer, net_vmifgsm, \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc1e266b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmifgsm\n",
      "[Model loaded]\n",
      "Acc: 19.97 %\n",
      "adv model\n"
     ]
    }
   ],
   "source": [
    "print(\"vmifgsm\")\n",
    "vmifgsm_attack_transfer = generate_adv(net_ori, \"vmifgsm\")\n",
    "# pgd_attack_transfer = pgd_attack_transfer.cpu()\n",
    "\n",
    "vmifgsm_acc = test_attack(val_loader, vmifgsm_attack_transfer, net_vmifgsm, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662a6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
